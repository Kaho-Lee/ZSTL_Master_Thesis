{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598132334560",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import timeit\n",
    "from mlmodel import *\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "import utils\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "../ZSTL_Data/CUB_200_2011/CUB_200_2011/label_feature.txt\n"
    }
   ],
   "source": [
    "!ls ../ZSTL_Data/CUB_200_2011/CUB_200_2011/label_feature.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "label         0         1         2         3         4         5  \\\n0        1.0  0.554742  0.449377  0.787002  0.377700  0.439481  0.487352   \n1        1.0  0.518317  0.397836  0.776558  0.384175  0.440413  0.482135   \n2        1.0  0.509601  0.438840  0.742074  0.355505  0.458852  0.520114   \n3        1.0  0.527033  0.440028  0.742868  0.343865  0.448310  0.488656   \n4        1.0  0.552168  0.386958  0.795093  0.359147  0.445325  0.528473   \n...      ...       ...       ...       ...       ...       ...       ...   \n11775  200.0  0.530580  0.413276  0.737307  0.407817  0.466526  0.536493   \n11776  200.0  0.535687  0.457732  0.765419  0.369241  0.504226  0.526427   \n11777  200.0  0.514037  0.482673  0.748582  0.373150  0.482410  0.495119   \n11778  200.0  0.538375  0.463504  0.765660  0.379090  0.448390  0.522997   \n11779  200.0  0.560232  0.476490  0.759214  0.389367  0.477937  0.500162   \n\n              6         7         8  ...      2038      2039      2040  \\\n0      0.417452  0.524080  0.730754  ...  0.513736  0.621625  0.531485   \n1      0.421372  0.500965  0.724335  ...  0.499792  0.604975  0.500643   \n2      0.408811  0.519433  0.753468  ...  0.481505  0.635696  0.497012   \n3      0.423018  0.526254  0.714776  ...  0.507166  0.668522  0.519964   \n4      0.430281  0.505096  0.706038  ...  0.487833  0.651347  0.516070   \n...         ...       ...       ...  ...       ...       ...       ...   \n11775  0.413465  0.487551  0.743418  ...  0.495754  0.639562  0.536414   \n11776  0.421565  0.531367  0.756237  ...  0.514706  0.646719  0.547907   \n11777  0.424842  0.519455  0.750451  ...  0.507967  0.669358  0.533826   \n11778  0.434170  0.503222  0.790027  ...  0.543026  0.707179  0.500062   \n11779  0.409428  0.521907  0.713610  ...  0.523608  0.682616  0.499209   \n\n           2041      2042      2043      2044      2045      2046      2047  \n0      0.514452  0.500363  0.501123  0.479742  0.439953  0.524471  0.418162  \n1      0.449412  0.525468  0.538653  0.494119  0.405931  0.498380  0.382074  \n2      0.518055  0.527479  0.507472  0.566703  0.452037  0.484544  0.389252  \n3      0.491975  0.530252  0.536647  0.515274  0.436954  0.560362  0.393463  \n4      0.488900  0.511301  0.489002  0.432631  0.442812  0.472144  0.385911  \n...         ...       ...       ...       ...       ...       ...       ...  \n11775  0.482167  0.535829  0.472961  0.486075  0.430384  0.519709  0.408091  \n11776  0.475493  0.562208  0.538472  0.524737  0.452918  0.525745  0.382445  \n11777  0.535661  0.531898  0.509304  0.519740  0.436029  0.505107  0.402101  \n11778  0.463361  0.577743  0.487792  0.485213  0.397064  0.519759  0.365018  \n11779  0.500095  0.567060  0.476806  0.471240  0.466036  0.540046  0.443416  \n\n[11780 rows x 2049 columns]\n"
    }
   ],
   "source": [
    "num_task = 200\n",
    "num_data = 40\n",
    "\n",
    "path_feature = '../ZSTL_Data/CUB_200_2011/CUB_200_2011/label_feature.txt'\n",
    "path_attributes = '../ZSTL_Data/CUB_200_2011/CUB_200_2011/attributes/class_attribute_labels_continuous.txt'\n",
    "path_destination = '../ZSTL_Data/CUB_200_2011/CUB_200_2011/splitedTask/'\n",
    "\n",
    "np.random.seed(888)\n",
    "\n",
    "data_label_feature = pd.read_csv(path_feature, sep=\" \", header=None)\n",
    "\n",
    "data_label_feature.columns = ['label'] + [x for x in range(2048)]\n",
    "print(data_label_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(44, 2049)\n(41, 2049)\n(53, 2049)\n(48, 2049)\n(58, 2049)\n(60, 2049)\n(60, 2049)\n(56, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(58, 2049)\n(57, 2049)\n(45, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(56, 2049)\n(59, 2049)\n(52, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(53, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(50, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(57, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(53, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(50, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(49, 2049)\n(60, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(50, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(51, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(56, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(50, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n41\n"
    }
   ],
   "source": [
    "stat = []\n",
    "for r in range(num_task):\n",
    "    df = data_label_feature.loc[data_label_feature['label'].eq(r+1)]\n",
    "    print(df.shape)\n",
    "    stat.append(df.shape[0])\n",
    "print(min(stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_compressed_data(data, num_task, num_data):\n",
    "    # print('row')\n",
    "    data_numpy = data.to_numpy()\n",
    "    data_x = data_numpy[:,1:]\n",
    "    data_y = np.atleast_2d(data_numpy[:,0])\n",
    "    print(data_x.shape)\n",
    "    start = timeit.default_timer()\n",
    "    # pca = PCA(n_components=1024)\n",
    "    # data_x_compress = pca.fit_transform(data_x)\n",
    "    data_x_compress = data_x\n",
    "    print('data_x_compress ',data_x_compress.shape, data_x_compress)\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time:', stop - start)\n",
    "\n",
    "    data_compress_np = np.hstack((data_y.T, data_x_compress))\n",
    "    data_compressed =  pd.DataFrame(data_compress_np,\n",
    "                   columns=['label'] +[i for i in range(data_x_compress.shape[1])])\n",
    "    print(data_compressed)\n",
    "    return data_compressed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 43.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\ntot task  200\n"
    }
   ],
   "source": [
    "def generate_split(splits, num_data):\n",
    "    indx = [ x for x in range(num_data)]\n",
    "    #print(temp)\n",
    "    train_indx = list(np.random.choice(indx, size=splits['train'], replace=False))\n",
    "    temp = [x for x in indx if x not in train_indx]\n",
    "    #print(len(train_indx))\n",
    "    # val_indx = list(np.random.choice(temp, size=55, replace=False))\n",
    "    test_indx = temp\n",
    "    #print(len(test_indx))\n",
    "    return train_indx, test_indx\n",
    "\n",
    "def task_data_split(data, num_task, num_data, splits, random_state=1):\n",
    "    task_train_byID = {}\n",
    "    task_test_byID = {}\n",
    "    task_val_byID = {}\n",
    "  \n",
    "    for i in range(num_task):   \n",
    "        task_data = data.loc[data['label'].eq(i+1)]\n",
    "        \n",
    "        sampled_task_data = task_data.sample(n=num_data, random_state=i)\n",
    "        #print(sampled_task_data.shape, sampled_task_data)\n",
    "        sampled_data = data.sample(n=num_data, random_state=i)\n",
    "\n",
    "        train_indx, test_indx = generate_split(splits, num_data)\n",
    "\n",
    "        task_train_byID[i+1] = np.vstack((sampled_task_data.iloc[train_indx].to_numpy(), \\\n",
    "            sampled_data.iloc[train_indx].to_numpy()))\n",
    "        task_train_byID[i+1] = np.hstack( (task_train_byID[i+1], np.ones((len(train_indx)*2, 1))) )\n",
    "        \n",
    "        temp = task_train_byID[i+1][:, 0]==(i+1)\n",
    "        task_train_byID[i+1][:, 0][temp==True] = 1.\n",
    "        task_train_byID[i+1][:, 0][temp==False] = 0.\n",
    "        print(task_train_byID[i+1].shape, np.sum(task_train_byID[i+1][:,0]))\n",
    "\n",
    "\n",
    "        task_test_byID[i+1] = np.vstack((sampled_task_data.iloc[test_indx].to_numpy(), \\\n",
    "            sampled_data.iloc[test_indx].to_numpy()))\n",
    "        task_test_byID[i+1] = np.hstack( (task_test_byID[i+1], np.ones((len(test_indx)*2, 1))) )\n",
    "\n",
    "        temp = task_test_byID[i+1][:, 0]==(i+1)\n",
    "        task_test_byID[i+1][:, 0][temp==True] = 1.\n",
    "        task_test_byID[i+1][:, 0][temp==False] = 0.\n",
    "        print(task_test_byID[i+1].shape, np.sum(task_test_byID[i+1][:,0]))\n",
    "\n",
    "    print('tot task ', len(list(task_train_byID.keys())))\n",
    "    return task_train_byID, task_test_byID\n",
    "\n",
    "\n",
    "splits = {}\n",
    "splits['train'] = 40\n",
    "splits['test'] = 0\n",
    "task_train_byID, task_test_byID = task_data_split(data_label_feature, num_task, num_data, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test the coef in logistic regression\n",
    "\n",
    "def singleTaskTrain(task_train, task_test):\n",
    "    X = task_train[:, 1:]\n",
    "    y = task_train[:, 0]\n",
    "\n",
    "    clf = LogisticRegression(fit_intercept = False, max_iter=100, C=1.0, random_state=0).fit(X, y)\n",
    "    pred_y = clf.predict(X)\n",
    "\n",
    "    # X_test = task_test[:, 1:]\n",
    "    # y_test = task_test[:, 0]\n",
    "    # pred_y_test = clf.predict(X_test)\n",
    "\n",
    "    param = clf.coef_\n",
    "    #print('pred_y_test ', np.sum(pred_y_test == y_test)/y_test.shape[0], param.shape)\n",
    "\n",
    "    #bias = clf.intercept_\n",
    "    #return np.hstack((param, np.atleast_2d(bias)))\n",
    "    return param, 1\n",
    "\n",
    "weight = singleTaskTrain(task_train_byID[4], task_test_byID[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[torch.Size([1, 2049])]\n{0: [(1, 2049)]}\n"
    }
   ],
   "source": [
    "net = FuncRecursiveNet([\n",
    "    FLinearLayer(1, False)\n",
    "])\n",
    "\n",
    "init_param = net.initialize_weights(utils.toTensor(task_train_byID[1][:, 1:]))\n",
    "p_lst = [p.size() for p in init_param]\n",
    "print(p_lst)\n",
    "shape_record = {}\n",
    "for i , p in enumerate(p_lst):\n",
    "    shape_record[i] = [tuple(p)]\n",
    "print(shape_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n0    0.0 2.9197080292 1.4598540146 0.0 59.854014598...\n1    4.41176470588 4.41176470588 2.94117647059 1.47...\n2    0.0 3.97350993377 3.31125827815 0.0 70.8609271...\n3    0.0 1.4598540146 10.2189781022 0.0 0.0 0.0 8.7...\n4    0.0 0.0 3.22580645161 0.0 1.0752688172 0.0 0.0...\n..                                                 ...\n195  2.7027027027 12.8378378378 0.0 0.0 0.0 0.0 76....\n196  18.3006535948 11.7647058824 0.0 3.26797385621 ...\n197  10.7594936709 36.7088607595 0.0 7.59493670886 ...\n198  0.0 2.06896551724 0.0 2.06896551724 0.0 0.0 88...\n199  0.0 8.28025477707 0.0 0.0 0.0 0.0 82.802547770...\n\n[200 rows x 1 columns]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]\n"
    }
   ],
   "source": [
    "def gen_attr(path_attributes):\n",
    "    data_attributes = pd.read_csv(path_attributes, sep=\"\\n\", header=None)\n",
    "    print(data_attributes)\n",
    "    a = ppp\n",
    "    lst = []\n",
    "    task_attr_byID = {}\n",
    "    i = 1\n",
    "    for r in data_attributes.iterrows():\n",
    "        # print(len(r), )\n",
    "        s = r[1].to_numpy()\n",
    "        # print(len(s[0].split()), s[0].split())\n",
    "        print(len(s[0].split()))\n",
    "        task_attr_byID[i] = np.array([float(a) for a in s[0].split()])\n",
    "        print(task_attr_byID[i].shape)\n",
    "        i += 1\n",
    "    return task_attr_byID\n",
    "\n",
    "def gen_attr_zScore(path_attributes):\n",
    "    data_attributes = pd.read_csv(path_attributes, sep=\"\\n\", header=None)\n",
    "    print(data_attributes)\n",
    "    lst = []\n",
    "    task_attr_byID = {}\n",
    "    i = 1\n",
    "    for r in data_attributes.iterrows():\n",
    "        #print(len(r), )\n",
    "        s = r[1].to_numpy()\n",
    "        # print(len(s[0].split()), s[0].split())\n",
    "        #print(r[0], len(s[0].split()))\n",
    "        lst.append(np.atleast_2d(np.array([float(a) for a in s[0].split()])))\n",
    "        # print(task_attr_byID[i].shape)\n",
    "        # i += 1\n",
    "    attr_mat = np.concatenate(lst, axis=0)\n",
    "    # print('attr_mat ', attr_mat.shape, attr_mat)\n",
    "    # attr_mean = np.mean(attr_mat, axis=0)\n",
    "    # print('attr_mean ', attr_mean.shape, attr_mean)\n",
    "\n",
    "\n",
    "    # attr_sub_mean = attr_mat - attr_mean\n",
    "    # print('sub ', attr_sub_mean.shape, attr_mat - attr_sub_mean)\n",
    "\n",
    "    # attr_std = np.std(attr_mat, axis=0)\n",
    "    # print('attr_std ', attr_std.shape, attr_std)\n",
    "\n",
    "    # attr_z = attr_sub_mean/attr_std\n",
    "    # print('z score', attr_z.shape, attr_sub_mean/attr_z)\n",
    "\n",
    "    # print('ffff ',attr_z)\n",
    "    attr_mat_normalize = attr_mat/100\n",
    "    for t in range(attr_mat.shape[0]):\n",
    "        # task_attr_byID[t+1] = attr_z[t,:]\n",
    "        task_attr_byID[t+1] = attr_mat_normalize[t,:]\n",
    "    \n",
    "    print(list(task_attr_byID.keys()))\n",
    "    return task_attr_byID\n",
    "\n",
    "\n",
    "task_attr_byID = gen_attr_zScore(path_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200\n200\nmean acc  1.0\n"
    }
   ],
   "source": [
    "def ZSTL_train_test_val(num_task, task_train_byID, task_test_byID, task_attr_byID, destination):\n",
    "    task_train_data = {}\n",
    "    task_test_data = {}\n",
    "    task_val_data = {}\n",
    "    acc = 0\n",
    "    for t in range(num_task):\n",
    "        weight, a = singleTaskTrain(task_train_byID[t+1], task_test_byID[t+1])\n",
    "        acc += a\n",
    "        cur_task_train = (task_attr_byID[t+1], weight, task_train_byID[t+1][:,1:], np.atleast_2d(task_train_byID[t+1][:,0]).T)\n",
    "        cur_task_test = (task_attr_byID[t+1], weight, task_test_byID[t+1][:,1:], np.atleast_2d(task_test_byID[t+1][:,0]).T)\n",
    "\n",
    "        task_train_data[t+1] = cur_task_train\n",
    "        task_test_data[t+1] = cur_task_test\n",
    "\n",
    "    print(len(task_train_data))\n",
    "    print(len(task_test_data))\n",
    "    \n",
    "    with open(destination+'task_train_data_normalize.pickle', 'wb') as handle:\n",
    "        pickle.dump(task_train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(destination+'task_test_data_normalize.pickle', 'wb') as handle:\n",
    "        pickle.dump(task_test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('mean acc ', acc/num_task)\n",
    "\n",
    "\n",
    "# a = {'hello': 'world'}\n",
    "\n",
    "# with open('filename.pickle', 'wb') as handle:\n",
    "#     pickle.dump([a], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('filename.pickle', 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "\n",
    "# print(a == b[0])\n",
    "ZSTL_train_test_val(num_task, task_train_byID, task_test_byID, task_attr_byID, path_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}