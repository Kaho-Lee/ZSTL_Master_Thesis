{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597494941238",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import timeit\n",
    "from mlmodel import *\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "import utils\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "../ZSTL_Data/CUB_200_2011/CUB_200_2011/label_feature.txt\n"
    }
   ],
   "source": [
    "!ls ../ZSTL_Data/CUB_200_2011/CUB_200_2011/label_feature.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "label         0         1         2         3         4         5  \\\n0        1.0  0.554742  0.449377  0.787002  0.377700  0.439481  0.487352   \n1        1.0  0.518317  0.397836  0.776558  0.384175  0.440413  0.482135   \n2        1.0  0.509601  0.438840  0.742074  0.355505  0.458852  0.520114   \n3        1.0  0.527033  0.440028  0.742868  0.343865  0.448310  0.488656   \n4        1.0  0.552168  0.386958  0.795093  0.359147  0.445325  0.528473   \n...      ...       ...       ...       ...       ...       ...       ...   \n11775  200.0  0.530580  0.413276  0.737307  0.407817  0.466526  0.536493   \n11776  200.0  0.535687  0.457732  0.765419  0.369241  0.504226  0.526427   \n11777  200.0  0.514037  0.482673  0.748582  0.373150  0.482410  0.495119   \n11778  200.0  0.538375  0.463504  0.765660  0.379090  0.448390  0.522997   \n11779  200.0  0.560232  0.476490  0.759214  0.389367  0.477937  0.500162   \n\n              6         7         8  ...      2038      2039      2040  \\\n0      0.417452  0.524080  0.730754  ...  0.513736  0.621625  0.531485   \n1      0.421372  0.500965  0.724335  ...  0.499792  0.604975  0.500643   \n2      0.408811  0.519433  0.753468  ...  0.481505  0.635696  0.497012   \n3      0.423018  0.526254  0.714776  ...  0.507166  0.668522  0.519964   \n4      0.430281  0.505096  0.706038  ...  0.487833  0.651347  0.516070   \n...         ...       ...       ...  ...       ...       ...       ...   \n11775  0.413465  0.487551  0.743418  ...  0.495754  0.639562  0.536414   \n11776  0.421565  0.531367  0.756237  ...  0.514706  0.646719  0.547907   \n11777  0.424842  0.519455  0.750451  ...  0.507967  0.669358  0.533826   \n11778  0.434170  0.503222  0.790027  ...  0.543026  0.707179  0.500062   \n11779  0.409428  0.521907  0.713610  ...  0.523608  0.682616  0.499209   \n\n           2041      2042      2043      2044      2045      2046      2047  \n0      0.514452  0.500363  0.501123  0.479742  0.439953  0.524471  0.418162  \n1      0.449412  0.525468  0.538653  0.494119  0.405931  0.498380  0.382074  \n2      0.518055  0.527479  0.507472  0.566703  0.452037  0.484544  0.389252  \n3      0.491975  0.530252  0.536647  0.515274  0.436954  0.560362  0.393463  \n4      0.488900  0.511301  0.489002  0.432631  0.442812  0.472144  0.385911  \n...         ...       ...       ...       ...       ...       ...       ...  \n11775  0.482167  0.535829  0.472961  0.486075  0.430384  0.519709  0.408091  \n11776  0.475493  0.562208  0.538472  0.524737  0.452918  0.525745  0.382445  \n11777  0.535661  0.531898  0.509304  0.519740  0.436029  0.505107  0.402101  \n11778  0.463361  0.577743  0.487792  0.485213  0.397064  0.519759  0.365018  \n11779  0.500095  0.567060  0.476806  0.471240  0.466036  0.540046  0.443416  \n\n[11780 rows x 2049 columns]\n"
    }
   ],
   "source": [
    "num_task = 200\n",
    "num_data = 40\n",
    "\n",
    "path_feature = '../ZSTL_Data/CUB_200_2011/CUB_200_2011/label_feature.txt'\n",
    "path_attributes = '../ZSTL_Data/CUB_200_2011/CUB_200_2011/attributes/class_attribute_labels_continuous.txt'\n",
    "path_destination = '../ZSTL_Data/CUB_200_2011/CUB_200_2011/splitedTask/'\n",
    "\n",
    "np.random.seed(888)\n",
    "\n",
    "data_label_feature = pd.read_csv(path_feature, sep=\" \", header=None)\n",
    "\n",
    "data_label_feature.columns = ['label'] + [x for x in range(2048)]\n",
    "print(data_label_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(44, 2049)\n(41, 2049)\n(53, 2049)\n(48, 2049)\n(58, 2049)\n(60, 2049)\n(60, 2049)\n(56, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(58, 2049)\n(57, 2049)\n(45, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(56, 2049)\n(59, 2049)\n(52, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(53, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(50, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(57, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(53, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(50, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(49, 2049)\n(60, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(50, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(51, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(56, 2049)\n(59, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(50, 2049)\n(60, 2049)\n(60, 2049)\n(58, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(59, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n(60, 2049)\n41\n"
    }
   ],
   "source": [
    "stat = []\n",
    "for r in range(num_task):\n",
    "    df = data_label_feature.loc[data_label_feature['label'].eq(r+1)]\n",
    "    print(df.shape)\n",
    "    stat.append(df.shape[0])\n",
    "print(min(stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_compressed_data(data, num_task, num_data):\n",
    "    # print('row')\n",
    "    data_numpy = data.to_numpy()\n",
    "    data_x = data_numpy[:,1:]\n",
    "    data_y = np.atleast_2d(data_numpy[:,0])\n",
    "    print(data_x.shape)\n",
    "    start = timeit.default_timer()\n",
    "    # pca = PCA(n_components=1024)\n",
    "    # data_x_compress = pca.fit_transform(data_x)\n",
    "    data_x_compress = data_x\n",
    "    print('data_x_compress ',data_x_compress.shape, data_x_compress)\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time:', stop - start)\n",
    "\n",
    "    data_compress_np = np.hstack((data_y.T, data_x_compress))\n",
    "    data_compressed =  pd.DataFrame(data_compress_np,\n",
    "                   columns=['label'] +[i for i in range(data_x_compress.shape[1])])\n",
    "    print(data_compressed)\n",
    "    return data_compressed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 42.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 43.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 41.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\n(80, 2050) 40.0\n(0, 2050) 0.0\ntot task  200\n"
    }
   ],
   "source": [
    "def generate_split(splits, num_data):\n",
    "    indx = [ x for x in range(num_data)]\n",
    "    #print(temp)\n",
    "    train_indx = list(np.random.choice(indx, size=splits['train'], replace=False))\n",
    "    temp = [x for x in indx if x not in train_indx]\n",
    "    #print(len(train_indx))\n",
    "    # val_indx = list(np.random.choice(temp, size=55, replace=False))\n",
    "    test_indx = temp\n",
    "    #print(len(test_indx))\n",
    "    return train_indx, test_indx\n",
    "\n",
    "def task_data_split(data, num_task, num_data, splits, random_state=1):\n",
    "    task_train_byID = {}\n",
    "    task_test_byID = {}\n",
    "    task_val_byID = {}\n",
    "  \n",
    "    for i in range(num_task):   \n",
    "        task_data = data.loc[data['label'].eq(i+1)]\n",
    "        \n",
    "        sampled_task_data = task_data.sample(n=num_data, random_state=i)\n",
    "        #print(sampled_task_data.shape, sampled_task_data)\n",
    "        sampled_data = data.sample(n=num_data, random_state=i)\n",
    "\n",
    "        train_indx, test_indx = generate_split(splits, num_data)\n",
    "\n",
    "        task_train_byID[i+1] = np.vstack((sampled_task_data.iloc[train_indx].to_numpy(), \\\n",
    "            sampled_data.iloc[train_indx].to_numpy()))\n",
    "        task_train_byID[i+1] = np.hstack( (task_train_byID[i+1], np.ones((len(train_indx)*2, 1))) )\n",
    "        \n",
    "        temp = task_train_byID[i+1][:, 0]==(i+1)\n",
    "        task_train_byID[i+1][:, 0][temp==True] = 1.\n",
    "        task_train_byID[i+1][:, 0][temp==False] = 0.\n",
    "        print(task_train_byID[i+1].shape, np.sum(task_train_byID[i+1][:,0]))\n",
    "\n",
    "\n",
    "        task_test_byID[i+1] = np.vstack((sampled_task_data.iloc[test_indx].to_numpy(), \\\n",
    "            sampled_data.iloc[test_indx].to_numpy()))\n",
    "        task_test_byID[i+1] = np.hstack( (task_test_byID[i+1], np.ones((len(test_indx)*2, 1))) )\n",
    "\n",
    "        temp = task_test_byID[i+1][:, 0]==(i+1)\n",
    "        task_test_byID[i+1][:, 0][temp==True] = 1.\n",
    "        task_test_byID[i+1][:, 0][temp==False] = 0.\n",
    "        print(task_test_byID[i+1].shape, np.sum(task_test_byID[i+1][:,0]))\n",
    "\n",
    "    print('tot task ', len(list(task_train_byID.keys())))\n",
    "    return task_train_byID, task_test_byID\n",
    "\n",
    "\n",
    "splits = {}\n",
    "splits['train'] = 40\n",
    "splits['test'] = 0\n",
    "task_train_byID, task_test_byID = task_data_split(data_label_feature, num_task, num_data, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test the coef in logistic regression\n",
    "\n",
    "def singleTaskTrain(task_train, task_test):\n",
    "    X = task_train[:, 1:]\n",
    "    y = task_train[:, 0]\n",
    "\n",
    "    clf = LogisticRegression(fit_intercept = False, max_iter=100, C=1.0, random_state=0).fit(X, y)\n",
    "    pred_y = clf.predict(X)\n",
    "\n",
    "    # X_test = task_test[:, 1:]\n",
    "    # y_test = task_test[:, 0]\n",
    "    # pred_y_test = clf.predict(X_test)\n",
    "\n",
    "    param = clf.coef_\n",
    "    #print('pred_y_test ', np.sum(pred_y_test == y_test)/y_test.shape[0], param.shape)\n",
    "\n",
    "    #bias = clf.intercept_\n",
    "    #return np.hstack((param, np.atleast_2d(bias)))\n",
    "    return param, 1\n",
    "\n",
    "weight = singleTaskTrain(task_train_byID[4], task_test_byID[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[torch.Size([1, 2049])]\n{0: [(1, 2049)]}\n"
    }
   ],
   "source": [
    "net = FuncRecursiveNet([\n",
    "    FLinearLayer(1, False)\n",
    "])\n",
    "\n",
    "init_param = net.initialize_weights(utils.toTensor(task_train_byID[1][:, 1:]))\n",
    "p_lst = [p.size() for p in init_param]\n",
    "print(p_lst)\n",
    "shape_record = {}\n",
    "for i , p in enumerate(p_lst):\n",
    "    shape_record[i] = [tuple(p)]\n",
    "print(shape_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n0    0.0 2.9197080292 1.4598540146 0.0 59.854014598...\n1    4.41176470588 4.41176470588 2.94117647059 1.47...\n2    0.0 3.97350993377 3.31125827815 0.0 70.8609271...\n3    0.0 1.4598540146 10.2189781022 0.0 0.0 0.0 8.7...\n4    0.0 0.0 3.22580645161 0.0 1.0752688172 0.0 0.0...\n..                                                 ...\n195  2.7027027027 12.8378378378 0.0 0.0 0.0 0.0 76....\n196  18.3006535948 11.7647058824 0.0 3.26797385621 ...\n197  10.7594936709 36.7088607595 0.0 7.59493670886 ...\n198  0.0 2.06896551724 0.0 2.06896551724 0.0 0.0 88...\n199  0.0 8.28025477707 0.0 0.0 0.0 0.0 82.802547770...\n\n[200 rows x 1 columns]\nattr_mat  (200, 312) [[ 0.          2.91970803  1.45985401 ...  5.35714286 21.42857143\n  19.64285714]\n [ 4.41176471  4.41176471  2.94117647 ...  3.80952381 10.47619048\n   8.57142857]\n [ 0.          3.97350993  3.31125828 ...  0.93457944  7.47663551\n  20.56074766]\n ...\n [10.75949367 36.70886076  0.         ... 44.29530201 18.79194631\n  18.12080537]\n [ 0.          2.06896552  0.         ... 34.48275862 33.10344828\n  20.        ]\n [ 0.          8.28025478  0.         ...  3.05343511 10.6870229\n  29.77099237]]\nattr_mean  (312,) [2.22439166e+00 1.33442906e+01 1.62562359e+00 2.44447481e+00\n 6.65088937e+00 4.01199555e+00 3.98336079e+01 2.74804188e+01\n 2.38430774e+00 5.38323989e+00 2.96536616e+01 2.12468179e+00\n 5.14035428e-01 1.22611182e+00 3.22862610e+01 9.71967892e+00\n 4.18709192e+00 2.01302882e+00 3.64907729e-01 2.45841553e+00\n 4.68930378e+01 2.84280238e+01 2.24144870e+00 1.95260876e+01\n 5.76978635e+00 2.83142072e+01 2.96398818e+00 6.11663931e-01\n 1.33390686e+00 3.34081667e+01 1.06169443e+01 4.62273259e+00\n 2.39725207e+00 4.40925751e-01 2.80338253e+00 4.50678629e+01\n 2.69373039e+01 2.67378140e+00 1.93068789e+01 3.01226552e+00\n 1.19445465e+01 1.83081373e+00 4.21131877e-01 1.54102279e+00\n 2.08370450e+01 1.54941296e+01 2.32907236e+00 1.34008940e+00\n 5.68109572e-01 3.24916734e+00 2.03530042e+01 4.36936484e+01\n 3.91215310e+00 2.00997462e+01 6.08510340e+01 6.60716717e+00\n 1.26441467e+01 1.98976521e+01 5.48677646e+00 2.55097879e+01\n 2.32108444e+00 4.71621604e-01 1.15698694e+00 3.11608845e+01\n 9.16778884e+00 4.53072306e+00 2.17461304e+00 4.29311683e-01\n 1.93260888e+00 3.59414975e+01 1.92508217e+01 2.33396961e+00\n 1.86164259e+01 4.28414056e+00 1.41045081e+01 3.73020327e+01\n 1.02169651e+01 2.28177540e+01 1.12745996e+01 5.14443123e+00\n 2.25773834e+01 1.67972792e+00 4.60873162e-01 9.81103683e-01\n 2.80126949e+01 6.99103474e+00 3.66723695e+00 1.48762694e+00\n 8.07426904e-01 2.15633569e+00 3.78602819e+01 1.81956379e+01\n 2.22236450e+00 1.62303205e+01 3.10383871e+00 9.52650476e+00\n 4.32813025e+00 4.53263476e+00 3.43506473e+00 1.23584822e+01\n 1.62495604e+01 2.80962225e+01 1.19196326e+01 6.87230761e+00\n 1.20504894e+01 3.10425835e+00 1.24898667e+01 1.60721181e+00\n 3.23730751e-01 1.37169893e+00 2.01604294e+01 1.42160667e+01\n 1.99916658e+00 1.13383995e+00 3.59684737e-01 2.88757755e+00\n 2.26854540e+01 3.97064811e+01 4.09236472e+00 1.87715734e+01\n 3.16146993e+00 8.87851319e+00 1.62302061e+00 4.62622603e-01\n 1.32480420e+00 1.71265617e+01 1.14906953e+01 1.42479738e+00\n 9.85548691e-01 4.65116601e-01 2.10228776e+00 2.35137074e+01\n 3.64145139e+01 3.92018343e+00 1.45011565e+01 8.47297871e-01\n 3.16922516e+00 1.13445944e-01 1.16884120e+00 1.82757952e+00\n 2.02558632e+00 1.93931835e-01 8.18023724e-02 3.95033264e-01\n 1.41948300e+00 8.85277075e+01 3.56123613e+00 2.47733695e+00\n 1.51458474e+00 3.55206076e+01 6.35327952e+00 5.88665376e+01\n 5.26674892e+00 1.87522088e+01 2.05993937e+00 5.70477326e-01\n 1.49676486e+00 2.10561415e+01 8.80258059e+00 2.19131923e+00\n 9.54568640e-01 3.85735855e-01 2.55259243e+00 3.45576972e+01\n 1.45453523e+01 4.25988781e+00 1.27297860e+01 4.21993096e+00\n 2.19491403e+01 1.63043194e+00 4.15869627e-01 1.15667736e+00\n 2.71050899e+01 8.19206424e+00 2.99169479e+00 1.38093103e+00\n 4.99145058e-01 1.94623349e+00 4.30178780e+01 2.41153150e+01\n 1.76240170e+00 1.62917978e+01 5.36907913e+00 1.90412146e+01\n 2.23923118e+00 6.45338480e-01 1.40432569e+00 2.63164108e+01\n 8.39550722e+00 3.15701616e+00 1.44172801e+00 6.05861786e-01\n 2.00678614e+00 2.88386331e+01 2.37989320e+01 3.64501383e+00\n 1.67235990e+01 2.47830467e+00 1.05879011e+01 1.58535183e+00\n 3.61168531e-01 1.36529868e+00 1.91740338e+01 1.48790148e+01\n 2.15019407e+00 1.26048631e+00 4.89254633e-01 2.94005886e+00\n 1.72927909e+01 4.25766085e+01 3.58811562e+00 1.88456441e+01\n 4.42366744e+01 3.07103234e+01 8.74327861e+00 1.19861838e+01\n 8.83587113e+00 3.78236952e+00 5.07439317e+01 1.92787289e+00\n 1.97022815e+01 2.38435445e+01 2.49154307e+00 1.61170252e+00\n 1.01591015e+00 6.30197160e+00 4.42805100e-01 6.99871805e+00\n 4.99457358e+00 5.96829699e+00 6.33996835e+00 2.91956418e+00\n 4.48715958e+00 2.63363915e+00 3.61521261e+00 5.01789351e+01\n 4.70683256e+01 7.93894157e+00 1.98226615e+01 2.51700713e+01\n 4.68460104e+01 6.13184640e+00 1.65983222e+01 3.04238210e+01\n 6.51776170e+01 6.59998600e+00 1.12957587e+01 1.69266383e+01\n 6.18117991e+00 2.52721148e+01 1.94409475e+00 8.08054937e-01\n 1.60194450e+00 2.92892698e+01 1.49135600e+01 4.17455004e+00\n 1.92284780e+00 6.20982381e-01 3.63354135e+00 3.55262456e+01\n 2.93117831e+01 3.82518427e+00 1.89034897e+01 1.45360504e+00\n 9.53311068e+00 4.60661957e-01 5.38376886e-01 1.20889320e+00\n 3.05631946e+01 2.07136519e+00 3.93167133e-01 1.58073437e-01\n 2.78930713e+00 9.89092717e+00 3.08515735e+01 4.33428136e+00\n 3.55039633e+00 1.88873471e+01 2.34708730e+00 8.55348320e+00\n 9.79599148e-01 2.49252560e-01 1.40184183e+00 2.73639846e+01\n 5.94970798e+00 5.48090847e-01 2.06811534e-01 7.69259620e-01\n 7.27846136e+00 4.57251655e+01 5.61919282e+00 3.88256145e+00\n 1.48213156e+01 5.59565351e+00 2.00036504e+01 2.20751207e+00\n 5.81792492e-01 1.67366094e+00 2.22955943e+01 7.42493787e+00\n 2.35064044e+00 1.20140646e+00 4.53488578e-01 2.25031672e+00\n 3.48650172e+01 1.27177881e+01 4.87208527e+00 1.28800011e+01\n 3.26561022e+01 8.54550504e+00 2.53930906e+01 3.34053022e+01]\nsub  (200, 312) [[ 2.22439166 13.34429059  1.62562359 ...  8.54550504 25.39309062\n  33.40530218]\n [ 2.22439166 13.34429059  1.62562359 ...  8.54550504 25.39309062\n  33.40530218]\n [ 2.22439166 13.34429059  1.62562359 ...  8.54550504 25.39309062\n  33.40530218]\n ...\n [ 2.22439166 13.34429059  1.62562359 ...  8.54550504 25.39309062\n  33.40530218]\n [ 2.22439166 13.34429059  1.62562359 ...  8.54550504 25.39309062\n  33.40530218]\n [ 2.22439166 13.34429059  1.62562359 ...  8.54550504 25.39309062\n  33.40530218]]\nattr_std  (312,) [ 5.6805492  20.825269    4.31075132 11.81257052 17.88937318 12.40610866\n 28.51239539 30.34732724  9.93974749 15.31078643 25.550724    4.40783282\n  1.28197002  3.74493824 20.79470139 19.03125798  9.44412358  7.2572162\n  0.88564348  6.18877356 26.03564896 25.22629384  8.78060198 19.96022206\n 16.45174989 25.47106797  4.92205568  1.33606316  3.89145362 21.6034129\n 20.33131536 10.25615979  8.11505473  1.27289433  6.47556828 26.58871638\n 24.46998028 10.03096405 20.32963721 10.9333952  15.88781257  4.26153437\n  1.23792868  4.66656295 17.91758812 29.65714144  5.89031992  4.5040945\n  1.62204881  7.57287076 26.34016335 32.36199441 14.19333118 20.29257757\n 28.32359031 12.16223823 16.5051627  16.13524862 15.75036704 24.61223036\n  4.86638747  1.20494638  3.43911286 21.35721562 18.82088798 10.13603786\n  7.31332121  1.03268937  4.58399317 28.39048591 22.37092846 10.04694704\n 19.75675454  8.12061515  8.26947005 16.91267614  8.06866003  9.31936534\n  8.20231033 15.21654264 22.30391933  3.69832582  1.15989886  3.1755615\n 19.14746591 15.07460052  8.58926768  5.33053727  1.86164967  5.12810817\n 26.44663132 21.36333136  9.41660424 17.23351145  6.48566307  8.30664143\n 10.33528527  7.35526169  5.16225617 16.15274947 11.54461269 25.06286371\n 11.61296756 10.22408958 14.32702458 11.42108642 17.14676946  4.12336071\n  1.06805088  3.61997483 17.86267562 28.32844272  5.88855212  4.10426795\n  0.99886418  6.74032184 26.9489156  32.20327237 15.09099034 19.76434658\n 12.17466099 13.04787079  4.25152636  1.18005359  4.47936045 16.37276278\n 26.54686592  4.45599892  3.93597061  1.60356048  4.19126156 30.04648488\n 33.16831381 13.78770178 16.30937232  3.23833522  4.23983573  0.44297775\n  3.02960608  2.11996954  6.23351842  0.61738435  0.33535044  0.95734729\n  5.08532761 15.25734008 12.12451449  9.32323419  2.39021353 30.46737513\n 15.27178124 34.77497281 16.54673775 21.78276144  4.51205871  1.53665362\n  5.09294076 19.02176585 21.32263478  5.49268299  3.72493474  1.62340572\n  5.55075152 30.54438127 23.54298703 15.48935415 13.89942084 12.86634647\n 21.14467013  3.32017685  1.19147417  3.3736246  16.92611351 17.46362541\n  6.94646609  4.34277782  1.21942035  5.53738114 25.78721461 22.28260821\n  7.565038   16.36661282 16.24585047 21.15044667  5.06278682  1.83097306\n  4.77473113 21.16692206 19.87010515  6.69959225  4.97074312  1.67247588\n  4.1253992  28.45594209 26.99305165 13.6308884  17.71237092 10.42537297\n 14.84776654  4.36106792  1.27126096  4.45036472 17.60339273 28.82350221\n  5.889659    4.55415717  1.29421341  6.87906786 25.63861799 32.03429289\n 13.36549695 18.95847705 15.17188517 11.39523049  7.11907708  7.91335027\n 11.40103675  6.30185113 22.78934208  6.62242161 25.85443158 18.11411287\n  5.66117023  2.52637801  4.66965151 17.08254462  1.49309573 14.95337041\n 10.60554347  5.19282608 12.80083225  4.78213577  4.85343101  3.4468638\n  4.04571191 30.9705183  26.06428171 13.36749737 18.48762026 14.08226586\n 22.90296614 10.15105032 13.21154666 16.32984586 25.88309676 13.02402564\n 14.18886754 12.87633545 16.26551477 23.90036262  4.17426483  1.71022359\n  4.22953252 20.39542873 27.57113061  9.23869436  6.49377387  1.50748897\n  6.90233717 27.04098305 26.33254834 13.60850417 18.76174903  3.22135168\n  8.28430003  1.07881144  1.48401503  2.45733232 21.12900519  4.76100573\n  1.01719883  0.61924947  5.51845946 14.8619124  25.15543382  5.8821904\n  8.68395569 18.27422877  4.88855423  9.01948392  1.79663221  0.70143487\n  4.64047229 15.68616354 12.43159383  1.57811065  0.72120303  1.55398156\n 13.24239489 26.78048003  6.9756152   9.77480809 15.80686527 17.35489421\n 22.6794432   5.06449041  1.60476389  5.17409623 19.6345915  18.47307247\n  5.68018336  4.68202451  1.65332027  5.40560836 31.3461834  22.32271173\n 16.91118334 14.39896145 26.85194832 14.80215178 19.21337783 20.23772596]\nz score (200, 312) [[ 5.6805492  20.825269    4.31075132 ... 14.80215178 19.21337783\n  20.23772596]\n [ 5.6805492  20.825269    4.31075132 ... 14.80215178 19.21337783\n  20.23772596]\n [ 5.6805492  20.825269    4.31075132 ... 14.80215178 19.21337783\n  20.23772596]\n ...\n [ 5.6805492  20.825269    4.31075132 ... 14.80215178 19.21337783\n  20.23772596]\n [ 5.6805492  20.825269    4.31075132 ... 14.80215178 19.21337783\n  20.23772596]\n [ 5.6805492  20.825269    4.31075132 ... 14.80215178 19.21337783\n  20.23772596]]\nffff  [[-0.39158039 -0.50057373 -0.03845492 ... -0.21539856 -0.2063416\n  -0.6800391 ]\n [ 0.38506366 -0.42892728  0.30517949 ... -0.31995221 -0.77638093\n  -1.22710791]\n [-0.39158039 -0.44997165  0.39103037 ... -0.51417697 -0.93249897\n  -0.63468369]\n ...\n [ 1.50251353  1.12193365 -0.37710911 ...  2.41517568 -0.34357021\n  -0.75524774]\n [-0.39158039 -0.54142518 -0.37710911 ...  1.75226237  0.40130152\n  -0.66239172]\n [-0.39158039 -0.24316785 -0.37710911 ... -0.37103186 -0.76540772\n  -0.17958094]]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]\n"
    }
   ],
   "source": [
    "def gen_attr(path_attributes):\n",
    "    data_attributes = pd.read_csv(path_attributes, sep=\"\\n\", header=None)\n",
    "    print(data_attributes)\n",
    "    lst = []\n",
    "    task_attr_byID = {}\n",
    "    i = 1\n",
    "    for r in data_attributes.iterrows():\n",
    "        # print(len(r), )\n",
    "        s = r[1].to_numpy()\n",
    "        # print(len(s[0].split()), s[0].split())\n",
    "        print(len(s[0].split()))\n",
    "        task_attr_byID[i] = np.array([float(a) for a in s[0].split()])\n",
    "        print(task_attr_byID[i].shape)\n",
    "        i += 1\n",
    "    return task_attr_byID\n",
    "\n",
    "def gen_attr_zScore(path_attributes):\n",
    "    data_attributes = pd.read_csv(path_attributes, sep=\"\\n\", header=None)\n",
    "    print(data_attributes)\n",
    "    lst = []\n",
    "    task_attr_byID = {}\n",
    "    i = 1\n",
    "    for r in data_attributes.iterrows():\n",
    "        #print(len(r), )\n",
    "        s = r[1].to_numpy()\n",
    "        # print(len(s[0].split()), s[0].split())\n",
    "        #print(r[0], len(s[0].split()))\n",
    "        lst.append(np.atleast_2d(np.array([float(a) for a in s[0].split()])))\n",
    "        # print(task_attr_byID[i].shape)\n",
    "        # i += 1\n",
    "    attr_mat = np.concatenate(lst, axis=0)\n",
    "    print('attr_mat ', attr_mat.shape, attr_mat)\n",
    "    attr_mean = np.mean(attr_mat, axis=0)\n",
    "    print('attr_mean ', attr_mean.shape, attr_mean)\n",
    "\n",
    "\n",
    "    attr_sub_mean = attr_mat - attr_mean\n",
    "    print('sub ', attr_sub_mean.shape, attr_mat - attr_sub_mean)\n",
    "\n",
    "    attr_std = np.std(attr_mat, axis=0)\n",
    "    print('attr_std ', attr_std.shape, attr_std)\n",
    "\n",
    "    attr_z = attr_sub_mean/attr_std\n",
    "    print('z score', attr_z.shape, attr_sub_mean/attr_z)\n",
    "\n",
    "    print('ffff ',attr_z)\n",
    "\n",
    "    for t in range(attr_z.shape[0]):\n",
    "        task_attr_byID[t+1] = attr_z[t,:]\n",
    "    \n",
    "    print(list(task_attr_byID.keys()))\n",
    "    return task_attr_byID\n",
    "\n",
    "task_attr_byID = gen_attr_zScore(path_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200\n200\nmean acc  1.0\n"
    }
   ],
   "source": [
    "def ZSTL_train_test_val(num_task, task_train_byID, task_test_byID, task_attr_byID, destination):\n",
    "    task_train_data = {}\n",
    "    task_test_data = {}\n",
    "    task_val_data = {}\n",
    "    acc = 0\n",
    "    for t in range(num_task):\n",
    "        weight, a = singleTaskTrain(task_train_byID[t+1], task_test_byID[t+1])\n",
    "        acc += a\n",
    "        cur_task_train = (task_attr_byID[t+1], weight, task_train_byID[t+1][:,1:], np.atleast_2d(task_train_byID[t+1][:,0]).T)\n",
    "        cur_task_test = (task_attr_byID[t+1], weight, task_test_byID[t+1][:,1:], np.atleast_2d(task_test_byID[t+1][:,0]).T)\n",
    "\n",
    "        task_train_data[t+1] = cur_task_train\n",
    "        task_test_data[t+1] = cur_task_test\n",
    "\n",
    "    print(len(task_train_data))\n",
    "    print(len(task_test_data))\n",
    "    \n",
    "    with open(destination+'task_train_data_standard.pickle', 'wb') as handle:\n",
    "        pickle.dump(task_train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(destination+'task_test_data_standard.pickle', 'wb') as handle:\n",
    "        pickle.dump(task_test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('mean acc ', acc/num_task)\n",
    "\n",
    "\n",
    "# a = {'hello': 'world'}\n",
    "\n",
    "# with open('filename.pickle', 'wb') as handle:\n",
    "#     pickle.dump([a], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('filename.pickle', 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "\n",
    "# print(a == b[0])\n",
    "ZSTL_train_test_val(num_task, task_train_byID, task_test_byID, task_attr_byID, path_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}