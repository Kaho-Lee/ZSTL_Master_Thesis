{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHOZrBzeGvyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "db8c1c0c-51dd-47ca-d766-ef6814be58fb"
      },
      "source": [
        "!pip install git+https://github.com/prolearner/hypertorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/prolearner/hypertorch\n",
            "  Cloning https://github.com/prolearner/hypertorch to /tmp/pip-req-build-x1nqs1w0\n",
            "  Running command git clone -q https://github.com/prolearner/hypertorch /tmp/pip-req-build-x1nqs1w0\n",
            "Requirement already satisfied (use --upgrade to upgrade): hypergrad==0.1 from git+https://github.com/prolearner/hypertorch in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: hypergrad\n",
            "  Building wheel for hypergrad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hypergrad: filename=hypergrad-0.1-cp36-none-any.whl size=6648 sha256=3afc1b6df94b0f562c402569cc934269c55a50a0fa15e6b20ad04334a036b598\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8zwuyqca/wheels/87/98/9c/443338661f6be492cbc5fb006151190595902bab536abc532c\n",
            "Successfully built hypergrad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfPOWAEQ6F-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import itertools\n",
        "import tqdm\n",
        "import hypergrad as hg\n",
        "from mlmodel import *\n",
        "import utils\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sparsemax import Sparsemax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOtP1IMcm2jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = 40 # feature dim\n",
        "d_a = 12 #dim of attr\n",
        "d_e = 3 #dim of attr embedding\n",
        "num_task = 100 # num of tasks\n",
        "train_task_size = 100 #\n",
        "test_task_size = 100\n",
        "kb_size = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcRF-2zDTsbX",
        "colab_type": "text"
      },
      "source": [
        "#my thought: linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gMAUO-bTnZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_c_byDist(e_item, e_kbb):\n",
        "  c = []\n",
        "  for i in range(e_item.size()[1]):\n",
        "    # print(e_train[:, i].size(), e_train[:, i], e_train[:,i].view(d_e, 1))\n",
        "    # print(e_kb.size(), e_kb)\n",
        "    #print(e_kb - e_train[:,i].view(d_e, 1))\n",
        "    dist = e_kbb - e_item[:,i].view(e_item.size()[0], 1)\n",
        "    dist = torch.norm(dist, p=2, dim=0)\n",
        "    #print(dist.size(), dist, torch.mul(-1, dist))\n",
        "    exp_neg_dist = torch.exp(torch.mul(-1, dist))\n",
        "    #print(exp_neg_dist)\n",
        "    c.append(exp_neg_dist.unsqueeze(0))\n",
        "    \n",
        "  c = torch.cat(c, dim=0)\n",
        "  return c\n",
        "\n",
        "def analytical_soln(w_kbb, e_item, e_kbb):\n",
        "  #find analytical son for one specific novel task\n",
        "  '''\n",
        "  c - row vector \n",
        "  w_kbb \n",
        "  '''\n",
        "  \n",
        "  c = build_c_byDist(e_item, e_kbb)\n",
        "\n",
        "  w = [torch.sum(torch.mul(c[i], w_kbb), dim=1, keepdim=True)/torch.sum(c[i]) for i in range(e_item.size()[1])]\n",
        "  w = torch.cat(w, dim=1)\n",
        "  return w\n",
        "\n",
        "def outer_loss(x_loss, y_loss, w_kbb, e_item, e_kbb):\n",
        "  \n",
        "  w_pred = analytical_soln(w_kbb, e_item, e_kbb)\n",
        "  #print(w_pred.t().size(), x_loss.size())\n",
        "  pred_y = torch.matmul(w_pred.t(), x_loss)\n",
        "  loss = F.mse_loss(pred_y, y_loss)\n",
        "  \n",
        "  return loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VFg0SkQ1Y-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "991c76b1-dac4-45c0-d2bd-88b956637d16"
      },
      "source": [
        "a_kb = torch.randn(d_a, kb_size)\n",
        "a_train = torch.randn(d_a, train_task_size)\n",
        "a_test = torch.randn(d_a, test_task_size)\n",
        "\n",
        "#M = torch.rand(d_e, d_a)\n",
        "M = torch.ones((d_e, d_a))\n",
        "e_kb = torch.matmul(M, a_kb)\n",
        "e_train = torch.matmul(M, a_train)\n",
        "e_test = torch.matmul(M, a_test)\n",
        "\n",
        "w_kb = torch.randn(d, kb_size)\n",
        "#construct matrix c\n",
        "c_train = build_c_byDist(e_train, e_kb)\n",
        "print(c_train.size())\n",
        "c_test = build_c_byDist(e_test, e_kb)\n",
        "print(c_test.size())\n",
        "\n",
        "# print(c_train[0])\n",
        "# print(w_kb.size())\n",
        "# print(torch.mul(c_train[0], w_kb)/w_kb)\n",
        "w_train = [torch.sum(torch.mul(c_train[i], w_kb), dim=1, keepdim=True)/torch.sum(c_train[i]) for i in range(train_task_size)]\n",
        "w_train = torch.cat(w_train, dim=1)\n",
        "print(w_train.size())\n",
        "solved_w = analytical_soln(w_kb, e_train, e_kb)\n",
        "print('cehck analutical soln ', torch.norm(w_train- solved_w))\n",
        "\n",
        "w_test = [torch.sum(torch.mul(c_test[i], w_kb), dim=1, keepdim=True)/torch.sum(c_test[i]) for i in range(test_task_size)]\n",
        "w_test = torch.cat(w_test, dim=1)\n",
        "print(w_test.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 10])\n",
            "torch.Size([100, 10])\n",
            "torch.Size([40, 100])\n",
            "cehck analutical soln  tensor(0.)\n",
            "torch.Size([40, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHbJrl3r-GCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3fd92bf6-31e0-4f22-ff73-9495b23fbfcc"
      },
      "source": [
        "x_train = torch.randn(d, 200)\n",
        "y_train = torch.matmul(w_train.t(), x_train)\n",
        "print(y_train.size())\n",
        "loss = outer_loss(x_train, y_train, w_kb, e_train, e_kb)\n",
        "print('check outer loss ', loss)\n",
        "\n",
        "x_test = torch.randn(d, 200)\n",
        "y_test = torch.matmul(w_test.t(), x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 200])\n",
            "check outer loss  tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjBqt7VN0jvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50ab429d-5f9b-4581-ebb2-0a000bacb696"
      },
      "source": [
        "#lam_learn = [torch.ones_like(M, requires_grad=True)]\n",
        "lam_learn = [torch.randn((d_e, d_a), requires_grad=True)]\n",
        "opt = torch.optim.Adam(lam_learn, lr=1e-2)\n",
        "totIter = 1000\n",
        "\n",
        "\n",
        "e_k = torch.matmul(lam_learn[0], a_kb)\n",
        "e_t = torch.matmul(lam_learn[0], a_train)\n",
        "o_loss = outer_loss(x_train, y_train, w_kb, e_t, e_k)\n",
        "print('outer loss {}'.format(o_loss))\n",
        "\n",
        "for tot in range(totIter):\n",
        "  e_k = torch.matmul(lam_learn[0], a_kb)\n",
        "  e_t = torch.matmul(lam_learn[0], a_train)\n",
        "  o_loss =  outer_loss(x_train, y_train, w_kb, e_t, e_k)\n",
        "\n",
        "  opt.zero_grad()\n",
        "  o_loss.backward()\n",
        "  opt.step()\n",
        "  print('outer loss {}'.format(o_loss))\n",
        "  \n",
        "  #print('lam_learn ', lam_learn)\n",
        "print('lam_learn ', lam_learn)\n",
        "print('M ', M)\n",
        "e_k = torch.matmul(lam_learn[0], a_kb)\n",
        "e_t = torch.matmul(lam_learn[0], a_test)\n",
        "o_loss = outer_loss(x_test, y_test, w_kb, e_t, e_k)\n",
        "print('test set: outer loss {}'.format(o_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "outer loss 32.261959075927734\n",
            "outer loss 32.261959075927734\n",
            "outer loss 31.914905548095703\n",
            "outer loss 31.57183837890625\n",
            "outer loss 31.23643684387207\n",
            "outer loss 30.911802291870117\n",
            "outer loss 30.599634170532227\n",
            "outer loss 30.300189971923828\n",
            "outer loss 30.012868881225586\n",
            "outer loss 29.736881256103516\n",
            "outer loss 29.471261978149414\n",
            "outer loss 29.21319007873535\n",
            "outer loss 28.956090927124023\n",
            "outer loss 28.701675415039062\n",
            "outer loss 28.453065872192383\n",
            "outer loss 28.211074829101562\n",
            "outer loss 27.976152420043945\n",
            "outer loss 27.748674392700195\n",
            "outer loss 27.528963088989258\n",
            "outer loss 27.31707763671875\n",
            "outer loss 27.112611770629883\n",
            "outer loss 26.914562225341797\n",
            "outer loss 26.72142791748047\n",
            "outer loss 26.53152847290039\n",
            "outer loss 26.34331512451172\n",
            "outer loss 26.155559539794922\n",
            "outer loss 25.96741485595703\n",
            "outer loss 25.778409957885742\n",
            "outer loss 25.58842658996582\n",
            "outer loss 25.397693634033203\n",
            "outer loss 25.206722259521484\n",
            "outer loss 25.01622772216797\n",
            "outer loss 24.826967239379883\n",
            "outer loss 24.63958740234375\n",
            "outer loss 24.454416275024414\n",
            "outer loss 24.271263122558594\n",
            "outer loss 24.08950424194336\n",
            "outer loss 23.908567428588867\n",
            "outer loss 23.728403091430664\n",
            "outer loss 23.549335479736328\n",
            "outer loss 23.3716983795166\n",
            "outer loss 23.195796966552734\n",
            "outer loss 23.022138595581055\n",
            "outer loss 22.85163116455078\n",
            "outer loss 22.685802459716797\n",
            "outer loss 22.526721954345703\n",
            "outer loss 22.376708984375\n",
            "outer loss 22.23773956298828\n",
            "outer loss 22.11078643798828\n",
            "outer loss 21.9954833984375\n",
            "outer loss 21.890270233154297\n",
            "outer loss 21.792938232421875\n",
            "outer loss 21.701215744018555\n",
            "outer loss 21.61320686340332\n",
            "outer loss 21.527515411376953\n",
            "outer loss 21.443260192871094\n",
            "outer loss 21.359975814819336\n",
            "outer loss 21.27750587463379\n",
            "outer loss 21.195907592773438\n",
            "outer loss 21.115419387817383\n",
            "outer loss 21.036359786987305\n",
            "outer loss 20.959104537963867\n",
            "outer loss 20.88396644592285\n",
            "outer loss 20.811155319213867\n",
            "outer loss 20.740764617919922\n",
            "outer loss 20.67273712158203\n",
            "outer loss 20.60693359375\n",
            "outer loss 20.54314613342285\n",
            "outer loss 20.481117248535156\n",
            "outer loss 20.420589447021484\n",
            "outer loss 20.361299514770508\n",
            "outer loss 20.303003311157227\n",
            "outer loss 20.24546241760254\n",
            "outer loss 20.188453674316406\n",
            "outer loss 20.13178062438965\n",
            "outer loss 20.07523536682129\n",
            "outer loss 20.01862907409668\n",
            "outer loss 19.961769104003906\n",
            "outer loss 19.904460906982422\n",
            "outer loss 19.846521377563477\n",
            "outer loss 19.78777313232422\n",
            "outer loss 19.728036880493164\n",
            "outer loss 19.667137145996094\n",
            "outer loss 19.604902267456055\n",
            "outer loss 19.54115104675293\n",
            "outer loss 19.47567367553711\n",
            "outer loss 19.408246994018555\n",
            "outer loss 19.338603973388672\n",
            "outer loss 19.26643180847168\n",
            "outer loss 19.19141387939453\n",
            "outer loss 19.113222122192383\n",
            "outer loss 19.031579971313477\n",
            "outer loss 18.946308135986328\n",
            "outer loss 18.85740852355957\n",
            "outer loss 18.765100479125977\n",
            "outer loss 18.66983985900879\n",
            "outer loss 18.572307586669922\n",
            "outer loss 18.473312377929688\n",
            "outer loss 18.373674392700195\n",
            "outer loss 18.274131774902344\n",
            "outer loss 18.17521858215332\n",
            "outer loss 18.07724380493164\n",
            "outer loss 17.980262756347656\n",
            "outer loss 17.884132385253906\n",
            "outer loss 17.78854751586914\n",
            "outer loss 17.693099975585938\n",
            "outer loss 17.597333908081055\n",
            "outer loss 17.500764846801758\n",
            "outer loss 17.40292739868164\n",
            "outer loss 17.303375244140625\n",
            "outer loss 17.20168113708496\n",
            "outer loss 17.097440719604492\n",
            "outer loss 16.99024772644043\n",
            "outer loss 16.87968635559082\n",
            "outer loss 16.76530647277832\n",
            "outer loss 16.646642684936523\n",
            "outer loss 16.523178100585938\n",
            "outer loss 16.394363403320312\n",
            "outer loss 16.259565353393555\n",
            "outer loss 16.1180477142334\n",
            "outer loss 15.968892097473145\n",
            "outer loss 15.810940742492676\n",
            "outer loss 15.642727851867676\n",
            "outer loss 15.462430000305176\n",
            "outer loss 15.26783275604248\n",
            "outer loss 15.056361198425293\n",
            "outer loss 14.825197219848633\n",
            "outer loss 14.57149887084961\n",
            "outer loss 14.292898178100586\n",
            "outer loss 13.988176345825195\n",
            "outer loss 13.65822982788086\n",
            "outer loss 13.306975364685059\n",
            "outer loss 12.941943168640137\n",
            "outer loss 12.573873519897461\n",
            "outer loss 12.215270042419434\n",
            "outer loss 11.878039360046387\n",
            "outer loss 11.570999145507812\n",
            "outer loss 11.298033714294434\n",
            "outer loss 11.057632446289062\n",
            "outer loss 10.844032287597656\n",
            "outer loss 10.649513244628906\n",
            "outer loss 10.466708183288574\n",
            "outer loss 10.290019989013672\n",
            "outer loss 10.115988731384277\n",
            "outer loss 9.942978858947754\n",
            "outer loss 9.77062702178955\n",
            "outer loss 9.599339485168457\n",
            "outer loss 9.429899215698242\n",
            "outer loss 9.26319408416748\n",
            "outer loss 9.100032806396484\n",
            "outer loss 8.941019058227539\n",
            "outer loss 8.786504745483398\n",
            "outer loss 8.636604309082031\n",
            "outer loss 8.491223335266113\n",
            "outer loss 8.350101470947266\n",
            "outer loss 8.212822914123535\n",
            "outer loss 8.078816413879395\n",
            "outer loss 7.947383403778076\n",
            "outer loss 7.817714691162109\n",
            "outer loss 7.688960075378418\n",
            "outer loss 7.560305595397949\n",
            "outer loss 7.431039810180664\n",
            "outer loss 7.300632953643799\n",
            "outer loss 7.168753147125244\n",
            "outer loss 7.0352783203125\n",
            "outer loss 6.9002485275268555\n",
            "outer loss 6.763805389404297\n",
            "outer loss 6.626134395599365\n",
            "outer loss 6.487392425537109\n",
            "outer loss 6.347671985626221\n",
            "outer loss 6.2069902420043945\n",
            "outer loss 6.065286159515381\n",
            "outer loss 5.922451496124268\n",
            "outer loss 5.7783708572387695\n",
            "outer loss 5.6329498291015625\n",
            "outer loss 5.486148357391357\n",
            "outer loss 5.337989330291748\n",
            "outer loss 5.188564300537109\n",
            "outer loss 5.038017749786377\n",
            "outer loss 4.88653564453125\n",
            "outer loss 4.734316825866699\n",
            "outer loss 4.581569671630859\n",
            "outer loss 4.428504943847656\n",
            "outer loss 4.275344371795654\n",
            "outer loss 4.1223368644714355\n",
            "outer loss 3.9697725772857666\n",
            "outer loss 3.8180086612701416\n",
            "outer loss 3.667480945587158\n",
            "outer loss 3.518707513809204\n",
            "outer loss 3.3722949028015137\n",
            "outer loss 3.2289156913757324\n",
            "outer loss 3.089259386062622\n",
            "outer loss 2.9539475440979004\n",
            "outer loss 2.8234846591949463\n",
            "outer loss 2.698258876800537\n",
            "outer loss 2.5785446166992188\n",
            "outer loss 2.4644880294799805\n",
            "outer loss 2.3560922145843506\n",
            "outer loss 2.253225088119507\n",
            "outer loss 2.155639886856079\n",
            "outer loss 2.063000440597534\n",
            "outer loss 1.9749218225479126\n",
            "outer loss 1.8909997940063477\n",
            "outer loss 1.8108394145965576\n",
            "outer loss 1.7340703010559082\n",
            "outer loss 1.6603630781173706\n",
            "outer loss 1.5894356966018677\n",
            "outer loss 1.5210554599761963\n",
            "outer loss 1.4550400972366333\n",
            "outer loss 1.3912551403045654\n",
            "outer loss 1.3296068906784058\n",
            "outer loss 1.2700388431549072\n",
            "outer loss 1.2125205993652344\n",
            "outer loss 1.1570409536361694\n",
            "outer loss 1.1036008596420288\n",
            "outer loss 1.05220627784729\n",
            "outer loss 1.0028636455535889\n",
            "outer loss 0.9555770754814148\n",
            "outer loss 0.9103440642356873\n",
            "outer loss 0.8671561479568481\n",
            "outer loss 0.8259965777397156\n",
            "outer loss 0.7868421077728271\n",
            "outer loss 0.7496617436408997\n",
            "outer loss 0.7144033908843994\n",
            "outer loss 0.6809897422790527\n",
            "outer loss 0.6493330597877502\n",
            "outer loss 0.6193416714668274\n",
            "outer loss 0.5909238457679749\n",
            "outer loss 0.563987672328949\n",
            "outer loss 0.5384456515312195\n",
            "outer loss 0.5142139792442322\n",
            "outer loss 0.49121561646461487\n",
            "outer loss 0.469378799200058\n",
            "outer loss 0.44863858819007874\n",
            "outer loss 0.4289337396621704\n",
            "outer loss 0.41020771861076355\n",
            "outer loss 0.3924073874950409\n",
            "outer loss 0.37548238039016724\n",
            "outer loss 0.3593851923942566\n",
            "outer loss 0.34407103061676025\n",
            "outer loss 0.3294979929924011\n",
            "outer loss 0.31562694907188416\n",
            "outer loss 0.30242103338241577\n",
            "outer loss 0.2898463308811188\n",
            "outer loss 0.27787071466445923\n",
            "outer loss 0.26646387577056885\n",
            "outer loss 0.2555970847606659\n",
            "outer loss 0.2452428638935089\n",
            "outer loss 0.23537497222423553\n",
            "outer loss 0.22596845030784607\n",
            "outer loss 0.21699891984462738\n",
            "outer loss 0.20844370126724243\n",
            "outer loss 0.20028109848499298\n",
            "outer loss 0.1924906075000763\n",
            "outer loss 0.18505322933197021\n",
            "outer loss 0.1779506653547287\n",
            "outer loss 0.17116563022136688\n",
            "outer loss 0.16468243300914764\n",
            "outer loss 0.15848584473133087\n",
            "outer loss 0.15256193280220032\n",
            "outer loss 0.14689715206623077\n",
            "outer loss 0.14147911965847015\n",
            "outer loss 0.13629566133022308\n",
            "outer loss 0.1313357651233673\n",
            "outer loss 0.12658867239952087\n",
            "outer loss 0.12204403430223465\n",
            "outer loss 0.11769217252731323\n",
            "outer loss 0.11352388560771942\n",
            "outer loss 0.10953028500080109\n",
            "outer loss 0.10570298880338669\n",
            "outer loss 0.10203392058610916\n",
            "outer loss 0.09851547330617905\n",
            "outer loss 0.0951405018568039\n",
            "outer loss 0.09190209954977036\n",
            "outer loss 0.08879376947879791\n",
            "outer loss 0.08580946922302246\n",
            "outer loss 0.0829433798789978\n",
            "outer loss 0.0801900252699852\n",
            "outer loss 0.07754432410001755\n",
            "outer loss 0.0750013217329979\n",
            "outer loss 0.07255647331476212\n",
            "outer loss 0.07020539790391922\n",
            "outer loss 0.06794403493404388\n",
            "outer loss 0.06576840579509735\n",
            "outer loss 0.0636749118566513\n",
            "outer loss 0.061659764498472214\n",
            "outer loss 0.05971989035606384\n",
            "outer loss 0.057851873338222504\n",
            "outer loss 0.056052759289741516\n",
            "outer loss 0.054319605231285095\n",
            "outer loss 0.052649617195129395\n",
            "outer loss 0.051040150225162506\n",
            "outer loss 0.049488700926303864\n",
            "outer loss 0.047992806881666183\n",
            "outer loss 0.046550195664167404\n",
            "outer loss 0.04515870660543442\n",
            "outer loss 0.043816279619932175\n",
            "outer loss 0.04252078756690025\n",
            "outer loss 0.041270509362220764\n",
            "outer loss 0.040063582360744476\n",
            "outer loss 0.03889830783009529\n",
            "outer loss 0.0377730056643486\n",
            "outer loss 0.03668613359332085\n",
            "outer loss 0.03563620150089264\n",
            "outer loss 0.034621722996234894\n",
            "outer loss 0.033641450107097626\n",
            "outer loss 0.032693956047296524\n",
            "outer loss 0.03177803382277489\n",
            "outer loss 0.03089248575270176\n",
            "outer loss 0.030036194249987602\n",
            "outer loss 0.029208019375801086\n",
            "outer loss 0.02840685099363327\n",
            "outer loss 0.027631811797618866\n",
            "outer loss 0.026881838217377663\n",
            "outer loss 0.026156041771173477\n",
            "outer loss 0.02545354887843132\n",
            "outer loss 0.02477346733212471\n",
            "outer loss 0.024114975705742836\n",
            "outer loss 0.02347728982567787\n",
            "outer loss 0.022859716787934303\n",
            "outer loss 0.022261478006839752\n",
            "outer loss 0.021681876853108406\n",
            "outer loss 0.02112029492855072\n",
            "outer loss 0.02057604119181633\n",
            "outer loss 0.020048553124070168\n",
            "outer loss 0.01953721046447754\n",
            "outer loss 0.01904146373271942\n",
            "outer loss 0.018560752272605896\n",
            "outer loss 0.01809459924697876\n",
            "outer loss 0.017642438411712646\n",
            "outer loss 0.01720384880900383\n",
            "outer loss 0.016778314486145973\n",
            "outer loss 0.016365451738238335\n",
            "outer loss 0.01596476510167122\n",
            "outer loss 0.015575877390801907\n",
            "outer loss 0.015198392793536186\n",
            "outer loss 0.014831933192908764\n",
            "outer loss 0.01447610929608345\n",
            "outer loss 0.014130623079836369\n",
            "outer loss 0.013795066624879837\n",
            "outer loss 0.013469127006828785\n",
            "outer loss 0.01315255556255579\n",
            "outer loss 0.012844950892031193\n",
            "outer loss 0.01254609227180481\n",
            "outer loss 0.01225566677749157\n",
            "outer loss 0.01197339128702879\n",
            "outer loss 0.011699031107127666\n",
            "outer loss 0.011432343162596226\n",
            "outer loss 0.011173026636242867\n",
            "outer loss 0.010920907370746136\n",
            "outer loss 0.010675745084881783\n",
            "outer loss 0.01043730042874813\n",
            "outer loss 0.010205388069152832\n",
            "outer loss 0.00997976865619421\n",
            "outer loss 0.009760282933712006\n",
            "outer loss 0.009546727873384953\n",
            "outer loss 0.009338944219052792\n",
            "outer loss 0.009136724285781384\n",
            "outer loss 0.008939887396991253\n",
            "outer loss 0.008748281747102737\n",
            "outer loss 0.008561770431697369\n",
            "outer loss 0.008380204439163208\n",
            "outer loss 0.008203388191759586\n",
            "outer loss 0.008031242527067661\n",
            "outer loss 0.00786357931792736\n",
            "outer loss 0.007700260262936354\n",
            "outer loss 0.0075411926954984665\n",
            "outer loss 0.007386230397969484\n",
            "outer loss 0.007235270459204912\n",
            "outer loss 0.007088162936270237\n",
            "outer loss 0.006944809574633837\n",
            "outer loss 0.006805116310715675\n",
            "outer loss 0.006668969057500362\n",
            "outer loss 0.006536257918924093\n",
            "outer loss 0.006406893953680992\n",
            "outer loss 0.006280784495174885\n",
            "outer loss 0.006157823838293552\n",
            "outer loss 0.006037934683263302\n",
            "outer loss 0.005921007599681616\n",
            "outer loss 0.005806972738355398\n",
            "outer loss 0.005695763509720564\n",
            "outer loss 0.00558727839961648\n",
            "outer loss 0.005481453612446785\n",
            "outer loss 0.0053782230243086815\n",
            "outer loss 0.00527750700712204\n",
            "outer loss 0.005179202184081078\n",
            "outer loss 0.005083292257040739\n",
            "outer loss 0.004989692475646734\n",
            "outer loss 0.004898342303931713\n",
            "outer loss 0.0048091779462993145\n",
            "outer loss 0.004722129087895155\n",
            "outer loss 0.004637165926396847\n",
            "outer loss 0.004554209299385548\n",
            "outer loss 0.004473197273910046\n",
            "outer loss 0.00439409539103508\n",
            "outer loss 0.004316863138228655\n",
            "outer loss 0.004241418559104204\n",
            "outer loss 0.0041677383705973625\n",
            "outer loss 0.00409576203674078\n",
            "outer loss 0.004025465343147516\n",
            "outer loss 0.003956770524382591\n",
            "outer loss 0.0038896563928574324\n",
            "outer loss 0.003824092447757721\n",
            "outer loss 0.003760023508220911\n",
            "outer loss 0.003697414416819811\n",
            "outer loss 0.0036362060345709324\n",
            "outer loss 0.0035764030180871487\n",
            "outer loss 0.0035179206170141697\n",
            "outer loss 0.0034607788547873497\n",
            "outer loss 0.003404914867132902\n",
            "outer loss 0.003350290236994624\n",
            "outer loss 0.0032968693412840366\n",
            "outer loss 0.0032446368131786585\n",
            "outer loss 0.0031935619190335274\n",
            "outer loss 0.0031436062417924404\n",
            "outer loss 0.0030947572086006403\n",
            "outer loss 0.0030469587072730064\n",
            "outer loss 0.003000215394422412\n",
            "outer loss 0.0029544751159846783\n",
            "outer loss 0.0029097390361130238\n",
            "outer loss 0.002865951508283615\n",
            "outer loss 0.002823110204190016\n",
            "outer loss 0.0027811985928565264\n",
            "outer loss 0.0027401652187108994\n",
            "outer loss 0.002700011245906353\n",
            "outer loss 0.002660701284185052\n",
            "outer loss 0.002622233470901847\n",
            "outer loss 0.002584566595032811\n",
            "outer loss 0.002547689713537693\n",
            "outer loss 0.0025115925818681717\n",
            "outer loss 0.0024762407410889864\n",
            "outer loss 0.0024416137021034956\n",
            "outer loss 0.002407708205282688\n",
            "outer loss 0.002374506089836359\n",
            "outer loss 0.0023419735953211784\n",
            "outer loss 0.002310109091922641\n",
            "outer loss 0.002278906526044011\n",
            "outer loss 0.002248321892693639\n",
            "outer loss 0.002218359149992466\n",
            "outer loss 0.0021890033967792988\n",
            "outer loss 0.0021602364722639322\n",
            "outer loss 0.0021320385858416557\n",
            "outer loss 0.002104412764310837\n",
            "outer loss 0.002077317563816905\n",
            "outer loss 0.0020507716108113527\n",
            "outer loss 0.002024744637310505\n",
            "outer loss 0.0019992240704596043\n",
            "outer loss 0.001974207116290927\n",
            "outer loss 0.001949666766449809\n",
            "outer loss 0.0019256044179201126\n",
            "outer loss 0.0019020084291696548\n",
            "outer loss 0.0018788721645250916\n",
            "outer loss 0.0018561636097729206\n",
            "outer loss 0.0018338909139856696\n",
            "outer loss 0.00181204779073596\n",
            "outer loss 0.0017906120046973228\n",
            "outer loss 0.0017695764545351267\n",
            "outer loss 0.0017489424208179116\n",
            "outer loss 0.0017286910442635417\n",
            "outer loss 0.0017088111490011215\n",
            "outer loss 0.0016892990097403526\n",
            "outer loss 0.001670147175900638\n",
            "outer loss 0.0016513443551957607\n",
            "outer loss 0.0016328905476257205\n",
            "outer loss 0.0016147556016221642\n",
            "outer loss 0.0015969643136486411\n",
            "outer loss 0.001579481759108603\n",
            "outer loss 0.0015623144572600722\n",
            "outer loss 0.0015454563545063138\n",
            "outer loss 0.0015288939466699958\n",
            "outer loss 0.001512623392045498\n",
            "outer loss 0.0014966465532779694\n",
            "outer loss 0.0014809282729402184\n",
            "outer loss 0.0014655053382739425\n",
            "outer loss 0.0014503349084407091\n",
            "outer loss 0.0014354217564687133\n",
            "outer loss 0.0014207713538780808\n",
            "outer loss 0.0014063777634873986\n",
            "outer loss 0.0013922147918492556\n",
            "outer loss 0.0013782937312498689\n",
            "outer loss 0.001364618306979537\n",
            "outer loss 0.001351156155578792\n",
            "outer loss 0.001337927533313632\n",
            "outer loss 0.001324912765994668\n",
            "outer loss 0.0013121049851179123\n",
            "outer loss 0.0012995083816349506\n",
            "outer loss 0.0012871265644207597\n",
            "outer loss 0.0012749307788908482\n",
            "outer loss 0.0012629427947103977\n",
            "outer loss 0.0012511409586295485\n",
            "outer loss 0.0012395229423418641\n",
            "outer loss 0.001228094450198114\n",
            "outer loss 0.0012168397661298513\n",
            "outer loss 0.0012057668063789606\n",
            "outer loss 0.0011948613682761788\n",
            "outer loss 0.0011841235682368279\n",
            "outer loss 0.0011735507287085056\n",
            "outer loss 0.0011631464585661888\n",
            "outer loss 0.0011528929462656379\n",
            "outer loss 0.0011427924036979675\n",
            "outer loss 0.001132845412939787\n",
            "outer loss 0.0011230544187128544\n",
            "outer loss 0.001113398582674563\n",
            "outer loss 0.001103883725591004\n",
            "outer loss 0.0010945055400952697\n",
            "outer loss 0.0010852745035663247\n",
            "outer loss 0.0010761654702946544\n",
            "outer loss 0.0010671955533325672\n",
            "outer loss 0.0010583592811599374\n",
            "outer loss 0.0010496354661881924\n",
            "outer loss 0.0010410372633486986\n",
            "outer loss 0.0010325562907382846\n",
            "outer loss 0.0010242011630907655\n",
            "outer loss 0.001015954650938511\n",
            "outer loss 0.0010078235063701868\n",
            "outer loss 0.0009998028399422765\n",
            "outer loss 0.000991896027699113\n",
            "outer loss 0.0009840941056609154\n",
            "outer loss 0.0009763938724063337\n",
            "outer loss 0.0009687914862297475\n",
            "outer loss 0.0009612910216674209\n",
            "outer loss 0.0009538901504129171\n",
            "outer loss 0.0009465915500186384\n",
            "outer loss 0.0009393827640451491\n",
            "outer loss 0.0009322617552243173\n",
            "outer loss 0.0009252367308363318\n",
            "outer loss 0.0009182983194477856\n",
            "outer loss 0.0009114507702179253\n",
            "outer loss 0.0009046825580298901\n",
            "outer loss 0.0008980000275187194\n",
            "outer loss 0.0008914062636904418\n",
            "outer loss 0.0008848847355693579\n",
            "outer loss 0.0008784515666775405\n",
            "outer loss 0.0008720915648154914\n",
            "outer loss 0.0008658043807372451\n",
            "outer loss 0.0008595920517109334\n",
            "outer loss 0.0008534545195288956\n",
            "outer loss 0.0008473893394693732\n",
            "outer loss 0.0008413990726694465\n",
            "outer loss 0.000835470505990088\n",
            "outer loss 0.0008296035812236369\n",
            "outer loss 0.0008238143636845052\n",
            "outer loss 0.0008180927252396941\n",
            "outer loss 0.0008124373271130025\n",
            "outer loss 0.0008068348979577422\n",
            "outer loss 0.000801299640443176\n",
            "outer loss 0.0007958236965350807\n",
            "outer loss 0.0007904040976427495\n",
            "outer loss 0.0007850561523810029\n",
            "outer loss 0.0007797552971169353\n",
            "outer loss 0.0007745091570541263\n",
            "outer loss 0.0007693258230574429\n",
            "outer loss 0.0007641944685019553\n",
            "outer loss 0.0007591103785671294\n",
            "outer loss 0.0007540859514847398\n",
            "outer loss 0.0007491075666621327\n",
            "outer loss 0.000744185468647629\n",
            "outer loss 0.0007393157575279474\n",
            "outer loss 0.0007344953482970595\n",
            "outer loss 0.0007297139381989837\n",
            "outer loss 0.0007249849149957299\n",
            "outer loss 0.0007203016430139542\n",
            "outer loss 0.0007156694191507995\n",
            "outer loss 0.000711074797436595\n",
            "outer loss 0.0007065277313813567\n",
            "outer loss 0.0007020243792794645\n",
            "outer loss 0.0006975650903768837\n",
            "outer loss 0.0006931463722139597\n",
            "outer loss 0.0006887721247039735\n",
            "outer loss 0.0006844339659437537\n",
            "outer loss 0.0006801400450058281\n",
            "outer loss 0.0006758790113963187\n",
            "outer loss 0.0006716643110848963\n",
            "outer loss 0.000667485932353884\n",
            "outer loss 0.0006633415468968451\n",
            "outer loss 0.0006592408753931522\n",
            "outer loss 0.0006551729165948927\n",
            "outer loss 0.000651139416731894\n",
            "outer loss 0.0006471442757174373\n",
            "outer loss 0.0006431848742067814\n",
            "outer loss 0.0006392530631273985\n",
            "outer loss 0.0006353554199449718\n",
            "outer loss 0.0006315021310001612\n",
            "outer loss 0.0006276706117205322\n",
            "outer loss 0.0006238774512894452\n",
            "outer loss 0.0006201114738360047\n",
            "outer loss 0.000616380013525486\n",
            "outer loss 0.0006126770749688148\n",
            "outer loss 0.0006090031820349395\n",
            "outer loss 0.0006053593824617565\n",
            "outer loss 0.0006017511477693915\n",
            "outer loss 0.0005981645663268864\n",
            "outer loss 0.0005946125602349639\n",
            "outer loss 0.0005910832551307976\n",
            "outer loss 0.0005875880597159266\n",
            "outer loss 0.0005841081729158759\n",
            "outer loss 0.000580671476200223\n",
            "outer loss 0.0005772552685812116\n",
            "outer loss 0.0005738663603551686\n",
            "outer loss 0.0005705005605705082\n",
            "outer loss 0.0005671581020578742\n",
            "outer loss 0.0005638466100208461\n",
            "outer loss 0.000560558051802218\n",
            "outer loss 0.0005572966183535755\n",
            "outer loss 0.0005540606216527522\n",
            "outer loss 0.0005508427275344729\n",
            "outer loss 0.0005476539372466505\n",
            "outer loss 0.000544483307749033\n",
            "outer loss 0.0005413379985839128\n",
            "outer loss 0.0005382213857956231\n",
            "outer loss 0.0005351220024749637\n",
            "outer loss 0.0005320455529727042\n",
            "outer loss 0.0005289950640872121\n",
            "outer loss 0.000525963376276195\n",
            "outer loss 0.0005229524103924632\n",
            "outer loss 0.000519960536621511\n",
            "outer loss 0.0005169936339370906\n",
            "outer loss 0.0005140468128956854\n",
            "outer loss 0.0005111154750920832\n",
            "outer loss 0.0005082107963971794\n",
            "outer loss 0.0005053326603956521\n",
            "outer loss 0.000502467795740813\n",
            "outer loss 0.0004996165516786277\n",
            "outer loss 0.0004967937711626291\n",
            "outer loss 0.0004939871723763645\n",
            "outer loss 0.0004911997239105403\n",
            "outer loss 0.0004884343361482024\n",
            "outer loss 0.00048568358761258423\n",
            "outer loss 0.0004829526296816766\n",
            "outer loss 0.0004802439652848989\n",
            "outer loss 0.00047755060950294137\n",
            "outer loss 0.00047487462870776653\n",
            "outer loss 0.0004722166049759835\n",
            "outer loss 0.0004695808747783303\n",
            "outer loss 0.0004669579502660781\n",
            "outer loss 0.0004643521679099649\n",
            "outer loss 0.00046176568139344454\n",
            "outer loss 0.0004591961915139109\n",
            "outer loss 0.0004566454445011914\n",
            "outer loss 0.0004541127709671855\n",
            "outer loss 0.00045159555156715214\n",
            "outer loss 0.00044909247662872076\n",
            "outer loss 0.0004466063401196152\n",
            "outer loss 0.0004441368510015309\n",
            "outer loss 0.00044169093598611653\n",
            "outer loss 0.0004392504633869976\n",
            "outer loss 0.0004368362424429506\n",
            "outer loss 0.00043442720198072493\n",
            "outer loss 0.0004320426960475743\n",
            "outer loss 0.0004296670376788825\n",
            "outer loss 0.00042731023859232664\n",
            "outer loss 0.00042497439426369965\n",
            "outer loss 0.00042264838702976704\n",
            "outer loss 0.0004203316639177501\n",
            "outer loss 0.0004180423857178539\n",
            "outer loss 0.00041576195508241653\n",
            "outer loss 0.00041349921957589686\n",
            "outer loss 0.0004112464375793934\n",
            "outer loss 0.00040901245665736496\n",
            "outer loss 0.00040679454104974866\n",
            "outer loss 0.0004045864916406572\n",
            "outer loss 0.00040239933878183365\n",
            "outer loss 0.0004002185305580497\n",
            "outer loss 0.0003980605979450047\n",
            "outer loss 0.0003959065652452409\n",
            "outer loss 0.0003937752335332334\n",
            "outer loss 0.00039165568887256086\n",
            "outer loss 0.0003895515401381999\n",
            "outer loss 0.00038745871279388666\n",
            "outer loss 0.00038537863292731345\n",
            "outer loss 0.00038331857649609447\n",
            "outer loss 0.00038126736762933433\n",
            "outer loss 0.0003792302159126848\n",
            "outer loss 0.0003772083146031946\n",
            "outer loss 0.00037519578472711146\n",
            "outer loss 0.00037320071714930236\n",
            "outer loss 0.0003712190955411643\n",
            "outer loss 0.00036924073356203735\n",
            "outer loss 0.000367287895642221\n",
            "outer loss 0.00036534032551571727\n",
            "outer loss 0.00036341100349090993\n",
            "outer loss 0.00036149105289950967\n",
            "outer loss 0.00035958667285740376\n",
            "outer loss 0.00035769573878496885\n",
            "outer loss 0.00035581435076892376\n",
            "outer loss 0.0003539478639140725\n",
            "outer loss 0.00035209828638471663\n",
            "outer loss 0.00035024943645112216\n",
            "outer loss 0.00034841953311115503\n",
            "outer loss 0.0003465966146904975\n",
            "outer loss 0.00034479316673241556\n",
            "outer loss 0.0003430002834647894\n",
            "outer loss 0.0003412156365811825\n",
            "outer loss 0.0003394470550119877\n",
            "outer loss 0.00033768959110602736\n",
            "outer loss 0.0003359400434419513\n",
            "outer loss 0.0003342054842505604\n",
            "outer loss 0.00033248323597945273\n",
            "outer loss 0.00033076759427785873\n",
            "outer loss 0.0003290683380328119\n",
            "outer loss 0.00032738008303567767\n",
            "outer loss 0.00032570032635703683\n",
            "outer loss 0.00032403392833657563\n",
            "outer loss 0.00032238155836239457\n",
            "outer loss 0.0003207380650565028\n",
            "outer loss 0.0003191016730852425\n",
            "outer loss 0.0003174781450070441\n",
            "outer loss 0.0003158660256303847\n",
            "outer loss 0.0003142615605611354\n",
            "outer loss 0.0003126732481177896\n",
            "outer loss 0.00031109206611290574\n",
            "outer loss 0.0003095237188972533\n",
            "outer loss 0.0003079652087762952\n",
            "outer loss 0.00030641412013210356\n",
            "outer loss 0.0003048762446269393\n",
            "outer loss 0.0003033476823475212\n",
            "outer loss 0.000301827909424901\n",
            "outer loss 0.000300322164548561\n",
            "outer loss 0.00029881991213187575\n",
            "outer loss 0.00029733357951045036\n",
            "outer loss 0.0002958542900159955\n",
            "outer loss 0.00029438614728860557\n",
            "outer loss 0.0002929252805188298\n",
            "outer loss 0.00029147567693144083\n",
            "outer loss 0.00029003547388128936\n",
            "outer loss 0.00028859873418696225\n",
            "outer loss 0.0002871803299058229\n",
            "outer loss 0.0002857664949260652\n",
            "outer loss 0.0002843636611942202\n",
            "outer loss 0.0002829708973877132\n",
            "outer loss 0.00028158636996522546\n",
            "outer loss 0.00028021205798722804\n",
            "outer loss 0.0002788417332340032\n",
            "outer loss 0.00027748747379519045\n",
            "outer loss 0.0002761339128483087\n",
            "outer loss 0.000274793419521302\n",
            "outer loss 0.00027346189017407596\n",
            "outer loss 0.0002721378696151078\n",
            "outer loss 0.0002708204847294837\n",
            "outer loss 0.00026951631298288703\n",
            "outer loss 0.00026821918436326087\n",
            "outer loss 0.0002669248206075281\n",
            "outer loss 0.00026564623112790287\n",
            "outer loss 0.0002643686893861741\n",
            "outer loss 0.0002631060779094696\n",
            "outer loss 0.0002618462312966585\n",
            "outer loss 0.0002605956688057631\n",
            "outer loss 0.0002593550307210535\n",
            "outer loss 0.0002581203880254179\n",
            "outer loss 0.00025689133326523006\n",
            "outer loss 0.000255675200605765\n",
            "outer loss 0.0002544617163948715\n",
            "outer loss 0.0002532546641305089\n",
            "outer loss 0.00025206428836099803\n",
            "outer loss 0.0002508732723072171\n",
            "outer loss 0.0002496921515557915\n",
            "outer loss 0.00024851656053215265\n",
            "outer loss 0.00024734888575039804\n",
            "outer loss 0.0002461874973960221\n",
            "outer loss 0.0002450378378853202\n",
            "outer loss 0.0002438892552163452\n",
            "outer loss 0.00024274959287140518\n",
            "outer loss 0.00024162149929907173\n",
            "outer loss 0.0002404923434369266\n",
            "outer loss 0.00023937717196531594\n",
            "outer loss 0.00023826732649467885\n",
            "outer loss 0.00023716164287179708\n",
            "outer loss 0.00023606423928868026\n",
            "outer loss 0.00023497286019846797\n",
            "outer loss 0.00023388686531689018\n",
            "outer loss 0.00023281016910914332\n",
            "outer loss 0.00023173613590188324\n",
            "outer loss 0.00023067049914970994\n",
            "outer loss 0.00022961184731684625\n",
            "outer loss 0.00022855904535390437\n",
            "outer loss 0.0002275139995617792\n",
            "outer loss 0.00022646799334324896\n",
            "outer loss 0.0002254395221825689\n",
            "outer loss 0.00022440750035457313\n",
            "outer loss 0.0002233886916656047\n",
            "outer loss 0.00022237253142520785\n",
            "outer loss 0.00022136085317470133\n",
            "outer loss 0.0002203570620622486\n",
            "outer loss 0.00021935529366601259\n",
            "outer loss 0.00021836278028786182\n",
            "outer loss 0.00021737632050644606\n",
            "outer loss 0.00021639619080815464\n",
            "outer loss 0.00021541838941629976\n",
            "outer loss 0.00021445004676934332\n",
            "outer loss 0.0002134872629540041\n",
            "outer loss 0.00021252722945064306\n",
            "outer loss 0.00021157274022698402\n",
            "outer loss 0.0002106231258949265\n",
            "outer loss 0.00020968145690858364\n",
            "outer loss 0.00020874397887382656\n",
            "outer loss 0.00020781219063792378\n",
            "outer loss 0.00020688360382337123\n",
            "outer loss 0.0002059622056549415\n",
            "outer loss 0.0002050471812253818\n",
            "outer loss 0.0002041346306214109\n",
            "outer loss 0.00020323226635809988\n",
            "outer loss 0.0002023251581704244\n",
            "outer loss 0.00020143082656431943\n",
            "outer loss 0.00020054106425959617\n",
            "outer loss 0.00019965361570939422\n",
            "outer loss 0.000198770547285676\n",
            "outer loss 0.0001978963555302471\n",
            "outer loss 0.0001970255107153207\n",
            "outer loss 0.00019615761993918568\n",
            "outer loss 0.0001952948368852958\n",
            "outer loss 0.00019443874771241099\n",
            "outer loss 0.00019358473946340382\n",
            "outer loss 0.00019273856014478952\n",
            "outer loss 0.00019189169688615948\n",
            "outer loss 0.0001910560531541705\n",
            "outer loss 0.00019022203923668712\n",
            "outer loss 0.00018939137225970626\n",
            "outer loss 0.00018856528913602233\n",
            "outer loss 0.00018774316413328052\n",
            "outer loss 0.00018693118181545287\n",
            "outer loss 0.00018611617269925773\n",
            "outer loss 0.00018530889065004885\n",
            "outer loss 0.0001845034712459892\n",
            "outer loss 0.00018370577890891582\n",
            "outer loss 0.00018291364540345967\n",
            "outer loss 0.00018211864517070353\n",
            "outer loss 0.0001813327253330499\n",
            "outer loss 0.00018055357213597745\n",
            "outer loss 0.00017977351672016084\n",
            "outer loss 0.00017900043167173862\n",
            "outer loss 0.0001782275503501296\n",
            "outer loss 0.00017746105731930584\n",
            "outer loss 0.00017670156375970691\n",
            "outer loss 0.00017594483506400138\n",
            "outer loss 0.00017518653476145118\n",
            "outer loss 0.0001744396868161857\n",
            "outer loss 0.00017369564739055932\n",
            "outer loss 0.00017294973076786846\n",
            "outer loss 0.00017221159941982478\n",
            "outer loss 0.00017147435573861003\n",
            "outer loss 0.0001707469782559201\n",
            "outer loss 0.0001700204302323982\n",
            "outer loss 0.00016929650155361742\n",
            "outer loss 0.00016857744776643813\n",
            "outer loss 0.0001678588887443766\n",
            "outer loss 0.0001671484933467582\n",
            "outer loss 0.00016643966955598444\n",
            "outer loss 0.00016573548782616854\n",
            "outer loss 0.00016503494407515973\n",
            "outer loss 0.00016433668497484177\n",
            "outer loss 0.00016364175826311111\n",
            "outer loss 0.00016294862143695354\n",
            "outer loss 0.00016226238221861422\n",
            "outer loss 0.00016157919890247285\n",
            "outer loss 0.00016089959535747766\n",
            "outer loss 0.00016022341151256114\n",
            "outer loss 0.00015954677655827254\n",
            "outer loss 0.00015887753397691995\n",
            "outer loss 0.00015821299166418612\n",
            "outer loss 0.00015754729975014925\n",
            "outer loss 0.00015688742860220373\n",
            "outer loss 0.00015622882347088307\n",
            "outer loss 0.00015557646111119539\n",
            "outer loss 0.00015492549573536962\n",
            "outer loss 0.00015427677135448903\n",
            "outer loss 0.00015363661805167794\n",
            "outer loss 0.00015299148799385875\n",
            "outer loss 0.00015235680621117353\n",
            "outer loss 0.00015172209532465786\n",
            "outer loss 0.00015109147352632135\n",
            "outer loss 0.0001504632964497432\n",
            "outer loss 0.00014983763685449958\n",
            "outer loss 0.00014921551337465644\n",
            "outer loss 0.00014859586372040212\n",
            "outer loss 0.00014798202028032392\n",
            "outer loss 0.000147369340993464\n",
            "outer loss 0.00014675746206194162\n",
            "outer loss 0.00014614943938795477\n",
            "outer loss 0.00014554748486261815\n",
            "outer loss 0.00014494426432065666\n",
            "outer loss 0.00014434657350648195\n",
            "outer loss 0.00014375259343069047\n",
            "outer loss 0.0001431574928574264\n",
            "outer loss 0.0001425719674443826\n",
            "outer loss 0.0001419840264134109\n",
            "outer loss 0.00014140129496809095\n",
            "outer loss 0.00014081949484534562\n",
            "outer loss 0.00014024111442267895\n",
            "outer loss 0.00013966609549243003\n",
            "outer loss 0.00013909144036006182\n",
            "outer loss 0.00013852228585164994\n",
            "outer loss 0.00013795563427265733\n",
            "outer loss 0.00013739008863922209\n",
            "outer loss 0.00013682953431271017\n",
            "outer loss 0.00013626759755425155\n",
            "outer loss 0.00013571474119089544\n",
            "outer loss 0.00013515957107301801\n",
            "outer loss 0.00013461013440974057\n",
            "outer loss 0.00013405887875705957\n",
            "outer loss 0.00013351239613257349\n",
            "outer loss 0.00013296848919708282\n",
            "outer loss 0.00013242957356851548\n",
            "outer loss 0.00013189302990213037\n",
            "outer loss 0.0001313540997216478\n",
            "outer loss 0.00013081944780424237\n",
            "outer loss 0.00013028940884396434\n",
            "outer loss 0.00012976271682418883\n",
            "outer loss 0.00012923985195811838\n",
            "outer loss 0.00012871362559963018\n",
            "outer loss 0.00012819327821489424\n",
            "outer loss 0.00012767525913659483\n",
            "outer loss 0.00012716128549072891\n",
            "outer loss 0.00012664493988268077\n",
            "outer loss 0.00012613374565262347\n",
            "outer loss 0.00012562493793666363\n",
            "outer loss 0.0001251186040462926\n",
            "outer loss 0.00012461353617254645\n",
            "outer loss 0.00012411184434313327\n",
            "outer loss 0.00012361320841591805\n",
            "outer loss 0.00012311678437981755\n",
            "outer loss 0.00012261905067134649\n",
            "outer loss 0.0001221295678988099\n",
            "outer loss 0.00012163855717517436\n",
            "outer loss 0.00012115142453694716\n",
            "outer loss 0.0001206648230436258\n",
            "outer loss 0.0001201814302476123\n",
            "outer loss 0.00011970162449870259\n",
            "outer loss 0.0001192211129819043\n",
            "outer loss 0.00011874539632117376\n",
            "outer loss 0.00011826773697976023\n",
            "outer loss 0.00011779711348935962\n",
            "outer loss 0.00011732686834875494\n",
            "outer loss 0.00011685967183439061\n",
            "outer loss 0.00011639177682809532\n",
            "outer loss 0.00011592768714763224\n",
            "outer loss 0.00011546738096512854\n",
            "outer loss 0.00011500636173877865\n",
            "outer loss 0.00011455098137957975\n",
            "outer loss 0.0001140964959631674\n",
            "outer loss 0.00011363910743966699\n",
            "outer loss 0.00011318821634631604\n",
            "outer loss 0.00011274121061433107\n",
            "outer loss 0.00011229430674575269\n",
            "outer loss 0.00011184722097823396\n",
            "outer loss 0.00011140541755594313\n",
            "outer loss 0.00011096416710643098\n",
            "outer loss 0.0001105227493098937\n",
            "outer loss 0.00011008709407178685\n",
            "outer loss 0.00010965232650050893\n",
            "outer loss 0.00010921854118350893\n",
            "outer loss 0.00010878650937229395\n",
            "outer loss 0.00010835687135113403\n",
            "outer loss 0.00010793140245368704\n",
            "outer loss 0.00010750627552624792\n",
            "outer loss 0.00010708183981478214\n",
            "outer loss 0.0001066609111148864\n",
            "outer loss 0.00010624025162542239\n",
            "outer loss 0.00010582168033579364\n",
            "outer loss 0.00010540612856857479\n",
            "outer loss 0.00010499492782400921\n",
            "outer loss 0.0001045797107508406\n",
            "outer loss 0.00010417097655590624\n",
            "outer loss 0.00010376171121606603\n",
            "outer loss 0.00010335368278902024\n",
            "outer loss 0.00010294950334355235\n",
            "outer loss 0.00010254628432448953\n",
            "outer loss 0.00010214406211161986\n",
            "outer loss 0.00010174587805522606\n",
            "outer loss 0.00010134706826647744\n",
            "outer loss 0.0001009502520901151\n",
            "outer loss 0.00010055508755613118\n",
            "outer loss 0.00010016418673330918\n",
            "outer loss 9.977357694879174e-05\n",
            "outer loss 9.938314906321466e-05\n",
            "outer loss 9.899747237795964e-05\n",
            "outer loss 9.861058060778305e-05\n",
            "outer loss 9.822927677305415e-05\n",
            "outer loss 9.784684516489506e-05\n",
            "outer loss 9.746696014190093e-05\n",
            "outer loss 9.708862489787862e-05\n",
            "outer loss 9.671145380707458e-05\n",
            "outer loss 9.633570880396292e-05\n",
            "outer loss 9.596212476026267e-05\n",
            "outer loss 9.558960300637409e-05\n",
            "outer loss 9.521838364889845e-05\n",
            "outer loss 9.484837210038677e-05\n",
            "outer loss 9.448203491047025e-05\n",
            "outer loss 9.411759674549103e-05\n",
            "outer loss 9.375372610520571e-05\n",
            "outer loss 9.338931704405695e-05\n",
            "outer loss 9.303033584728837e-05\n",
            "outer loss 9.267288987757638e-05\n",
            "outer loss 9.231468720827252e-05\n",
            "outer loss 9.19581507332623e-05\n",
            "outer loss 9.160320769296959e-05\n",
            "outer loss 9.125012729782611e-05\n",
            "outer loss 9.089995000977069e-05\n",
            "outer loss 9.05493288883008e-05\n",
            "outer loss 9.020113066071644e-05\n",
            "outer loss 8.985359454527497e-05\n",
            "outer loss 8.950799383455887e-05\n",
            "lam_learn  [tensor([[-0.8939, -0.9255, -0.8898, -0.9017, -0.9118, -0.9197, -0.9408, -0.8840,\n",
            "         -0.9376, -0.9231, -0.9403, -0.9314],\n",
            "        [-0.8918, -0.8824, -0.9053, -0.8837, -0.8527, -0.8560, -0.8873, -0.8997,\n",
            "         -0.8965, -0.9028, -0.8892, -0.8769],\n",
            "        [ 1.1994,  1.1831,  1.1929,  1.1992,  1.2146,  1.2059,  1.1673,  1.2018,\n",
            "          1.1608,  1.1692,  1.1667,  1.1818]], requires_grad=True)]\n",
            "M  tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "test set: outer loss 0.00182823755312711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t5HFSzPa9Ww",
        "colab_type": "text"
      },
      "source": [
        "#my thought: non-linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "042bnIT11IX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# non_lin_transform_toA = FuncRecursiveNet([\n",
        "#         FLinearLayer(10, False), \n",
        "#         FActivation(F.relu),                                 \n",
        "#         FLinearLayer(3, True),\n",
        "#     ])\n",
        "\n",
        "\n",
        "non_lin_toEm = FuncRecursiveNet([\n",
        "        FLinearLayer(d_e*3+1, True),\n",
        "        FActivation(F.relu),\n",
        "        FLinearLayer(d_e*2+1, True), \n",
        "        FActivation(F.relu),                                 \n",
        "        FLinearLayer(d_e, True),\n",
        "    ])\n",
        "\n",
        "non_lin_toEm_learn = FuncRecursiveNet([\n",
        "        FLinearLayer(d_e*3, True),\n",
        "        FActivation(F.relu),\n",
        "        FLinearLayer(d_e*2, True), \n",
        "        FActivation(F.relu),                                 \n",
        "        FLinearLayer(d_e, True),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N15w8VrGEyAX",
        "colab_type": "text"
      },
      "source": [
        "#Attention Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgawahTHgT3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analytical_soln_atten(w_kbb, e_item, e_kbb, w_atten):\n",
        "  #find analytical son for one specific novel task\n",
        "  '''\n",
        "  c - row vector \n",
        "  w_kbb \n",
        "  '''\n",
        "  affinity = build_c_byAtten(e_item, e_kbb, w_atten)\n",
        "\n",
        "  softmax = nn.Softmax(dim=1)\n",
        "  c_newnew = softmax(affinity)\n",
        "  \n",
        "  w = torch.matmul(c_newnew, w_kbb.t())\n",
        "\n",
        "  return w.t()\n",
        "\n",
        "def outer_loss_atten(x_loss, y_loss, w_kbb, e_item, e_kbb, w_atten):\n",
        "  \n",
        "  w_pred = analytical_soln_atten(w_kbb, e_item, e_kbb, w_atten)\n",
        "  \n",
        "  #print(w_pred.t().size(), x_loss.size())\n",
        "  pred_y = torch.matmul(w_pred.t(), x_loss)\n",
        "  loss = F.mse_loss(pred_y, y_loss)\n",
        "  \n",
        "  return loss\n",
        "\n",
        "def build_c_byAtten(e_item, e_kbb, w_atten):\n",
        "  dim = e_item.size()[0]\n",
        "  affinity = torch.matmul(e_item.t(), w_atten)\n",
        "  affinity = torch.matmul(affinity, e_kbb)\n",
        "  \n",
        "  #torch.sqrt(torch.tensor(e_item.size()[0], dtype=float))\n",
        "\n",
        "  return affinity\n",
        "  \n",
        "def outer_loss_class_atten(x_loss, y_loss, w_kbb, e_item, e_kbb, hp, regu_coef=0.01):\n",
        "  \n",
        "  w_pred = analytical_soln_atten(w_kbb, e_item, e_kbb, hp)\n",
        "  #print(w_pred.t().size(), x_loss.size())\n",
        "  logit = torch.matmul(w_pred.t(), x_loss)\n",
        "  pred_y = torch.sigmoid(logit)\n",
        "  # loss = F.mse_loss(pred_y, y_loss)\n",
        "  try:\n",
        "    loss = F.binary_cross_entropy(pred_y, y_loss)\n",
        "  except:\n",
        "    print('e_kbb ', e_kbb)\n",
        "    print('e_item ', e_item)\n",
        "    print('w_pred ', w_pred)\n",
        "    # print('pred_y ', pred_y)\n",
        "    print('c ', build_c_byAtten(e_item, e_kbb))\n",
        "\n",
        "  # l2loss = torch.tensor(0., dtype=float, requires_grad=True)\n",
        "  # for p in hp:\n",
        "  #   l2loss = l2loss + torch.norm(p, p=2)\n",
        "  return loss \n",
        "\n",
        "def getPred(x_loss, w_pred):\n",
        "  \n",
        "  logit = torch.matmul(w_pred.t(), x_loss)\n",
        "  pred = torch.sigmoid(logit)\n",
        "  pred[pred>=0.5] = torch.ones_like(pred[pred>=0.5])\n",
        "  pred[pred<0.5] = torch.zeros_like(pred[pred<0.5])\n",
        "\n",
        "  return pred\n",
        "\n",
        "def acc_binary_atten(x_loss, y_loss, w_kbb, e_item, e_kbb, w_atten):\n",
        "  w_pred = analytical_soln_atten(w_kbb, e_item, e_kbb, w_atten)\n",
        "  #print(w_pred.t().size(), x_loss.size())\n",
        "  pred = getPred(x_loss, w_pred)\n",
        "  compare = (pred == y_loss)\n",
        "  #print('compare ', compare.size(), compare)\n",
        "  compare = torch.sum(compare, dim=1)\n",
        "  #print('compare ', compare.size(), compare)\n",
        "  #print(y_loss.size()[1])\n",
        "  acc = torch.mean(compare.float())\n",
        "  #print('acc ', acc)\n",
        "  mean_acc = acc/y_loss.size()[1]\n",
        "  #print(mean_acc)\n",
        "  \n",
        "  return mean_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHh60mLiPvfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "4421a304-007f-4a6c-e900-003ae14b78b6"
      },
      "source": [
        "'''\n",
        "Sparsemax utility\n",
        "'''\n",
        "sparsemax = Sparsemax(dim=1)\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "logits = torch.randn(2, 5)\n",
        "print(\"\\nLogits\")\n",
        "print(logits)\n",
        "\n",
        "softmax_probs = softmax(logits)\n",
        "print(\"\\nSoftmax probabilities\")\n",
        "print(softmax_probs)\n",
        "\n",
        "sparsemax_probs = sparsemax(logits)\n",
        "print(\"\\nSparsemax probabilities\")\n",
        "print(sparsemax_probs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Logits\n",
            "tensor([[ 1.0910,  1.0434,  0.8470, -0.4349,  0.8394],\n",
            "        [-0.4407, -1.0627, -1.3407, -0.8610,  0.8660]])\n",
            "\n",
            "Softmax probabilities\n",
            "tensor([[0.2680, 0.2555, 0.2099, 0.0583, 0.2083],\n",
            "        [0.1589, 0.0853, 0.0646, 0.1044, 0.5869]])\n",
            "\n",
            "Sparsemax probabilities\n",
            "tensor([[0.3859, 0.3382, 0.1418, 0.0000, 0.1342],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qNoeg6v6f1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "44c63e64-02f6-48c5-8c9a-4626745ab07f"
      },
      "source": [
        "'''\n",
        "KL divergence utility\n",
        "'''\n",
        "# this is the same example in wiki\n",
        "P = torch.Tensor([0.36, 0.48, 0.16])\n",
        "Q = torch.Tensor([0.333, 0.333, 0.333])\n",
        "\n",
        "print('mannually ',(P * (P / Q).log()).sum())\n",
        "# tensor(0.0863), 10.2 µs ± 508\n",
        "\n",
        "print('torch ',F.kl_div(Q.log(), P, None, None, 'sum'))\n",
        "# tensor(0.0863), 14.1 µs ± 408 ns\n",
        "\n",
        "P1 = torch.Tensor([0.32, 0.6, 0.08])\n",
        "Q1 = torch.Tensor([0.3, 0.5, 0.2])\n",
        "print('mannually ',(P1 * (P1 / Q1).log()).sum())\n",
        "# tensor(0.0567), 10.2 µs ± 508\n",
        "\n",
        "print('torch ',F.kl_div(Q1.log(), P1, None, None, 'sum'))\n",
        "# tensor(0.0567), 14.1 µs ± 408 ns\n",
        "print(0.0567+0.0863)\n",
        "P_all = torch.cat([torch.unsqueeze(P, dim=0), torch.unsqueeze(P1, dim=0)], dim=0)\n",
        "print(P_all)\n",
        "Q_all = torch.cat([torch.unsqueeze(Q, dim=0), torch.unsqueeze(Q1, dim=0)], dim=0)\n",
        "print(Q_all)\n",
        "print('torch ',F.kl_div(Q_all.log(), P_all, None, None, 'batchmean'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mannually  tensor(0.0863)\n",
            "torch  tensor(0.0863)\n",
            "mannually  tensor(0.0567)\n",
            "torch  tensor(0.0567)\n",
            "0.14300000000000002\n",
            "tensor([[0.3600, 0.4800, 0.1600],\n",
            "        [0.3200, 0.6000, 0.0800]])\n",
            "tensor([[0.3330, 0.3330, 0.3330],\n",
            "        [0.3000, 0.5000, 0.2000]])\n",
            "torch  tensor(0.0715)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLkgRDhMNats",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention_alignment(weight_train, weight_kb, attr_train, attr_kb, w_atten, x_loss):\n",
        "  cal_affinity = lambda a, b: torch.exp(torch.matmul(a, b)/torch.sqrt(torch.tensor(b.size()[0], dtype=float)))\n",
        "  cal_atten = lambda a : a/torch.sum(a, dim=1, keepdim=True)\n",
        "  #threshold = torch.tensor(1e-, dtype=float)\n",
        "  #hp = [torch.eye(dm, requires_grad=False)]\n",
        "  #attr_kb[attr_kb < 1e-8] = torch.randn_like(attr_kb[attr_kb < 1e-8])\n",
        "\n",
        "  attr_kb_opt = [attr_kb.clone().detach().requires_grad_(True)]\n",
        "  #attr_kb_opt = [torch.randn(attr_kb.size(), requires_grad=True)]\n",
        "  opt = torch.optim.Adam(attr_kb_opt, lr=1e-4)\n",
        "\n",
        "  totIter = 200\n",
        "\n",
        "  y_kb_pred = getPred(x_loss,  weight_kb)\n",
        "  #print('y_kb_pred shape', y_kb_pred.size())\n",
        "  #affinity_w_kb = cal_affinity(weight_kb.t(), weight_kb)\n",
        "  #print('affinity_w_kb ', affinity_w_kb.size(), torch.sum(affinity_w_kb, dim=1))\n",
        "  affinity_w_kb = cal_affinity(y_kb_pred, y_kb_pred.t())\n",
        "  \n",
        "  w_kb_atten = cal_atten(affinity_w_kb)\n",
        "  #print('w_kb_atten ', w_kb_atten.size(), torch.sum(w_kb_atten, dim=1))\n",
        "\n",
        "  y_train_pred = getPred(x_loss,  weight_train)\n",
        "  #print('y_train_pred shape', y_train_pred.size())\n",
        "  #affinity_w_train_kb = cal_affinity(weight_train.t(), weight_kb)\n",
        "  #print('affinity_w_train_kb ', affinity_w_train_kb.size(), torch.sum(affinity_w_train_kb, dim=1))\n",
        "  affinity_w_train_kb = cal_affinity(y_train_pred, y_kb_pred.t())\n",
        "  \n",
        "  w_train_kb_atten = cal_atten(affinity_w_train_kb)\n",
        "  #print('w_train_kb_atten ', w_train_kb_atten.size(), torch.sum(w_train_kb_atten, dim=1))\n",
        "  \n",
        "  for t in range(totIter):\n",
        "    #affinity_attr_kb = torch.exp(torch.matmul(torch.matmul(attr_kb_opt[0].t(), w_atten), attr_kb_opt[0]))\n",
        "    affinity_attr_kb = cal_affinity(attr_kb_opt[0].t(), attr_kb_opt[0])\n",
        "    attr_kb_atten = cal_atten(affinity_attr_kb)\n",
        "\n",
        "    #affinity_attr_train_kb = torch.exp(torch.matmul(torch.matmul(attr_train.t(), w_atten), attr_kb_opt[0]))\n",
        "    affinity_attr_train_kb = cal_affinity(attr_train.t(), attr_kb_opt[0])\n",
        "    attr_train_kb_atten = cal_atten(affinity_attr_train_kb)\n",
        "\n",
        "    # kl_loss_kb =  F.kl_div(attr_kb_atten.log(), w_kb_atten, None, None, 'batchmean')\n",
        "    # kl_loss_train_kb = F.kl_div(attr_train_kb_atten.log(), w_train_kb_atten, None, None, 'batchmean')\n",
        "    # kl_loss = kl_loss_kb + kl_loss_train_kb + 0.00001*torch.pow(torch.norm(attr_kb_opt[0]), 2)\n",
        "    # if kl_loss <= threshold:\n",
        "    #   break\n",
        "    # print('{}/{}: kl loss is {}'.format(t, totIter, kl_loss))\n",
        "    loss = nn.MSELoss()\n",
        "    mse_loss_kb =  loss(attr_kb_atten, w_kb_atten)\n",
        "    mse_loss_train_kb = loss(attr_train_kb_atten, w_train_kb_atten)\n",
        "    mse_loss = mse_loss_kb + mse_loss_train_kb + 0.00001*torch.pow(torch.norm(attr_kb_opt[0]), 2)\n",
        "    \n",
        "    #print('{}/{}: mse loss is {}'.format(t, totIter, mse_loss))\n",
        "\n",
        "    opt.zero_grad()\n",
        "    mse_loss.backward()\n",
        "    opt.step()\n",
        "  #a = pppp\n",
        "  return attr_kb_opt[0].clone().detach().requires_grad_(False), mse_loss\n",
        "\n",
        "def sigmoid(theta):\n",
        "  theta[theta < -100] = -100\n",
        "  return 1/(1+np.exp(-theta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ7tsep7V0hk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "344a0f4d-4bec-4674-857e-72a9b67ae860"
      },
      "source": [
        "d = 8\n",
        "dm = 5\n",
        "T = 100\n",
        "n = 10\n",
        "dict_k = 6\n",
        "\n",
        "np.random.seed(666)\n",
        "L_gt = np.random.randn(d,dict_k)\n",
        "\n",
        "D_gt = np.random.randn(dm,dict_k)\n",
        "\n",
        "s_train = np.random.uniform(0, 2, size=(dict_k, T))\n",
        "s_train[s_train>=1] = 0\n",
        "#print(s_train)\n",
        "\n",
        "s_val = np.random.uniform(0, 2, size=(dict_k, 10))\n",
        "s_val[s_val>=1] = 0\n",
        "#print(s_val)\n",
        "s_test = np.random.uniform(0, 2, size=(dict_k, T))\n",
        "s_test[s_test>=1] = 0\n",
        "#print(s_test)\n",
        "\n",
        "w_train = L_gt @ s_train\n",
        "w_kb = L_gt @ s_val\n",
        "w_test = L_gt @ s_test\n",
        "\n",
        "a_train = D_gt @ s_train\n",
        "a_kb = D_gt @ s_val\n",
        "a_test = D_gt @ s_test\n",
        "\n",
        "#ones = np.atleast_2d(np.ones(10))\n",
        "\n",
        "ones = np.atleast_2d(np.ones(n))\n",
        "\n",
        "x_train = np.random.normal(0, 1.0, size=(d-1,n))\n",
        "x_train = np.vstack((ones, x_train))\n",
        "\n",
        "# x_val = np.random.normal(0, 1.0, size=(d-1,n))\n",
        "# x_val = np.vstack((ones, x_val))\n",
        "x_test = np.random.normal(0, 1.0, size=(d-1,n))\n",
        "x_test = np.vstack((ones, x_test))\n",
        "\n",
        "y_train = sigmoid(w_train.T @ x_train)\n",
        "y_train[y_train >=0.5] = 1\n",
        "y_train[y_train <0.5] = 0\n",
        "print('y_train ', y_train.shape, np.sum(y_train, axis=1))\n",
        "\n",
        "# y_val = sigmoid(w_val.T @ x_val)\n",
        "# y_val[y_val >= 0.5] = 1\n",
        "# y_val[y_val < 0.5] = 0\n",
        "# print('y_val ', y_val.shape, np.sum(y_val, axis=1))\n",
        "\n",
        "y_test = sigmoid(w_test.T @x_test)\n",
        "y_test[y_test >=0.5] = 1\n",
        "y_test[y_test <0.5] = 0\n",
        "print('y_test ', y_test.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train  (100, 10) [5. 7. 4. 7. 7. 3. 6. 8. 8. 5. 8. 5. 7. 4. 3. 2. 8. 7. 8. 7. 7. 7. 4. 4.\n",
            " 6. 7. 7. 7. 8. 7. 4. 3. 8. 8. 7. 8. 2. 7. 7. 6. 7. 8. 8. 4. 6. 3. 3. 5.\n",
            " 8. 7. 7. 8. 7. 5. 8. 5. 8. 8. 5. 7. 5. 8. 3. 4. 9. 8. 8. 4. 6. 8. 6. 5.\n",
            " 3. 7. 9. 8. 7. 7. 5. 5. 7. 8. 7. 8. 7. 7. 2. 6. 6. 4. 6. 7. 7. 7. 7. 5.\n",
            " 8. 3. 8. 6.]\n",
            "y_test  (100, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNwuZ9Pnse73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w_train = utils.toTensor(w_train)\n",
        "w_test = utils.toTensor(w_test)\n",
        "a_train = utils.toTensor(a_train)\n",
        "a_test = utils.toTensor(a_test)\n",
        "x_train = utils.toTensor(x_train)\n",
        "x_test = utils.toTensor(x_test)\n",
        "y_train = utils.toTensor(y_train)\n",
        "y_test = utils.toTensor(y_test)\n",
        "w_kb = utils.toTensor(w_kb)\n",
        "a_kb = utils.toTensor(a_kb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld-FnJ-q8njl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "133c4b40-bd7c-43df-e5a5-fe4393ded409"
      },
      "source": [
        "hp = [torch.eye(dm, requires_grad=True), w_kb.clone().detach().requires_grad_(True)]\n",
        "opt_hp = torch.optim.Adam(hp, lr=1e-3)\n",
        "a_kb_opt = a_kb.clone().detach().requires_grad_(False)\n",
        "\n",
        "totIter = 2000\n",
        "\n",
        "init_o_loss = outer_loss_class_atten(x_train, y_train, hp[1], a_train, a_kb_opt, hp[0])\n",
        "init_acc = acc_binary_atten(x_test, y_test, hp[1], a_test, a_kb_opt, hp[0])\n",
        "print('init outer loss {}; init acc {}'.format(init_o_loss, init_acc))\n",
        "l_lst = []\n",
        "acc_lst = []\n",
        "for tot in range(totIter):\n",
        " \n",
        "  o_loss =  outer_loss_class_atten(x_train, y_train, hp[1], a_train, a_kb_opt, hp[0])\n",
        "\n",
        "  #x_loss, y_loss, w_kbb, e_item, e_kbb, w_atten\n",
        "  batch_acc = acc_binary_atten(x_test, y_test, hp[1], a_test, a_kb_opt, hp[0])\n",
        "\n",
        "  o_loss = o_loss + 0.00001*torch.pow(torch.norm(hp[1]), 2)\n",
        "\n",
        "  opt_hp.zero_grad()\n",
        "  o_loss.backward()\n",
        "  opt_hp.step()\n",
        "\n",
        "\n",
        "  a_kb_opt, kl_loss = attention_alignment(w_train, hp[1].clone().detach(), a_train, a_kb_opt, hp[0], x_train)\n",
        "  #a = pppp\n",
        "\n",
        "  l_lst.append(utils.toNumpy(o_loss))\n",
        "  acc_lst.append(utils.toNumpy(batch_acc))\n",
        "  \n",
        "  if (tot+1) % 10 == 0:\n",
        "    print('{}/{} o_loss {}; mean test acc {} with mse loss in atten align {}'.format(tot+1, totIter, o_loss, batch_acc, kl_loss))\n",
        "    #print('{}/{} o_loss {}; mean test acc {} '.format(tot+1, totIter, o_loss, batch_acc))\n",
        "\n",
        "  #print('hp ', hp)\n",
        "\n",
        "\n",
        "o_loss = outer_loss_class_atten(x_train, y_train, hp[1], a_train, a_kb_opt, hp[0])\n",
        "batch_acc = acc_binary_atten(x_test, y_test, hp[1], a_test, a_kb_opt, hp[0])\n",
        "print('test set: outer loss {}; batch acc {}'.format(o_loss, batch_acc))\n",
        "\n",
        "\n",
        "plt.plot(l_lst, label='cross entropy loss in training data')\n",
        "plt.plot(acc_lst, label='batch accuracy in testing data')\n",
        "plt.xlabel('Iteration')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init outer loss 0.5547641515731812; init acc 0.7829999923706055\n",
            "10/2000 o_loss 0.5468142628669739; mean test acc 0.7860000133514404 with mse loss in atten align 0.01586596481502056\n",
            "20/2000 o_loss 0.5418986082077026; mean test acc 0.7760000228881836 with mse loss in atten align 0.009174272418022156\n",
            "30/2000 o_loss 0.5433687567710876; mean test acc 0.7860000133514404 with mse loss in atten align 0.005260645877569914\n",
            "40/2000 o_loss 0.5481353998184204; mean test acc 0.7860000133514404 with mse loss in atten align 0.0032132454216480255\n",
            "50/2000 o_loss 0.5464982986450195; mean test acc 0.784000039100647 with mse loss in atten align 0.0021767611615359783\n",
            "60/2000 o_loss 0.542650043964386; mean test acc 0.777999997138977 with mse loss in atten align 0.0015560525935143232\n",
            "70/2000 o_loss 0.5356536507606506; mean test acc 0.7749999761581421 with mse loss in atten align 0.0012120233150199056\n",
            "80/2000 o_loss 0.5315757989883423; mean test acc 0.765999972820282 with mse loss in atten align 0.001039243070408702\n",
            "90/2000 o_loss 0.532558023929596; mean test acc 0.765999972820282 with mse loss in atten align 0.0009879448916763067\n",
            "100/2000 o_loss 0.5290358662605286; mean test acc 0.765999972820282 with mse loss in atten align 0.0009746004361659288\n",
            "110/2000 o_loss 0.5263444781303406; mean test acc 0.7670000195503235 with mse loss in atten align 0.0009706131531856954\n",
            "120/2000 o_loss 0.5242813229560852; mean test acc 0.7689999938011169 with mse loss in atten align 0.000969514949247241\n",
            "130/2000 o_loss 0.5202101469039917; mean test acc 0.7719999551773071 with mse loss in atten align 0.0009884684113785625\n",
            "140/2000 o_loss 0.5160772800445557; mean test acc 0.7789999842643738 with mse loss in atten align 0.0009679679060354829\n",
            "150/2000 o_loss 0.5120285153388977; mean test acc 0.7829999923706055 with mse loss in atten align 0.0009668458951637149\n",
            "160/2000 o_loss 0.5083210468292236; mean test acc 0.7889999747276306 with mse loss in atten align 0.0009668459533713758\n",
            "170/2000 o_loss 0.504740297794342; mean test acc 0.7929999828338623 with mse loss in atten align 0.0009668459533713758\n",
            "180/2000 o_loss 0.5012794733047485; mean test acc 0.7940000295639038 with mse loss in atten align 0.0009668459533713758\n",
            "190/2000 o_loss 0.4979318380355835; mean test acc 0.7940000295639038 with mse loss in atten align 0.0009668459533713758\n",
            "200/2000 o_loss 0.494692862033844; mean test acc 0.796999990940094 with mse loss in atten align 0.0009668459533713758\n",
            "210/2000 o_loss 0.49155980348587036; mean test acc 0.800000011920929 with mse loss in atten align 0.0009668458951637149\n",
            "220/2000 o_loss 0.48853015899658203; mean test acc 0.8040000200271606 with mse loss in atten align 0.0009668458951637149\n",
            "230/2000 o_loss 0.4856019914150238; mean test acc 0.8060000538825989 with mse loss in atten align 0.0009668459533713758\n",
            "240/2000 o_loss 0.4827730357646942; mean test acc 0.8050000071525574 with mse loss in atten align 0.0009668458951637149\n",
            "250/2000 o_loss 0.4800410568714142; mean test acc 0.8069999814033508 with mse loss in atten align 0.0009668458951637149\n",
            "260/2000 o_loss 0.477402925491333; mean test acc 0.8079999685287476 with mse loss in atten align 0.0009668459533713758\n",
            "270/2000 o_loss 0.4746626317501068; mean test acc 0.8109999895095825 with mse loss in atten align 0.0010150561574846506\n",
            "280/2000 o_loss 0.47596749663352966; mean test acc 0.8130000233650208 with mse loss in atten align 0.000962423044256866\n",
            "290/2000 o_loss 0.4729432165622711; mean test acc 0.8119999766349792 with mse loss in atten align 0.001081777736544609\n",
            "300/2000 o_loss 0.4707968831062317; mean test acc 0.8170000314712524 with mse loss in atten align 0.0011214378755539656\n",
            "310/2000 o_loss 0.4682843089103699; mean test acc 0.8199999928474426 with mse loss in atten align 0.0011857075151056051\n",
            "320/2000 o_loss 0.46661409735679626; mean test acc 0.8209999799728394 with mse loss in atten align 0.0011534170480445027\n",
            "330/2000 o_loss 0.4649849832057953; mean test acc 0.8229999542236328 with mse loss in atten align 0.0011502443812787533\n",
            "340/2000 o_loss 0.46291714906692505; mean test acc 0.8229999542236328 with mse loss in atten align 0.0011502443812787533\n",
            "350/2000 o_loss 0.46087488532066345; mean test acc 0.8209999799728394 with mse loss in atten align 0.0011502443812787533\n",
            "360/2000 o_loss 0.45943060517311096; mean test acc 0.8159999847412109 with mse loss in atten align 0.0012193871662020683\n",
            "370/2000 o_loss 0.45822539925575256; mean test acc 0.8159999847412109 with mse loss in atten align 0.0012090965174138546\n",
            "380/2000 o_loss 0.4559071660041809; mean test acc 0.8140000104904175 with mse loss in atten align 0.00120762106962502\n",
            "390/2000 o_loss 0.4538308382034302; mean test acc 0.8130000233650208 with mse loss in atten align 0.00120762106962502\n",
            "400/2000 o_loss 0.4519670307636261; mean test acc 0.8140000104904175 with mse loss in atten align 0.00120762106962502\n",
            "410/2000 o_loss 0.45025116205215454; mean test acc 0.8140000104904175 with mse loss in atten align 0.00120762106962502\n",
            "420/2000 o_loss 0.44862857460975647; mean test acc 0.8159999847412109 with mse loss in atten align 0.00120762106962502\n",
            "430/2000 o_loss 0.44706740975379944; mean test acc 0.8159999847412109 with mse loss in atten align 0.00120762106962502\n",
            "440/2000 o_loss 0.44555145502090454; mean test acc 0.8159999847412109 with mse loss in atten align 0.00120762106962502\n",
            "450/2000 o_loss 0.4440728724002838; mean test acc 0.8140000104904175 with mse loss in atten align 0.0012076211860403419\n",
            "460/2000 o_loss 0.4426271915435791; mean test acc 0.8140000104904175 with mse loss in atten align 0.00120762106962502\n",
            "470/2000 o_loss 0.44121116399765015; mean test acc 0.8149999380111694 with mse loss in atten align 0.00120762106962502\n",
            "480/2000 o_loss 0.43982231616973877; mean test acc 0.8159999847412109 with mse loss in atten align 0.0012076211860403419\n",
            "490/2000 o_loss 0.43805640935897827; mean test acc 0.8159999847412109 with mse loss in atten align 0.001303887227550149\n",
            "500/2000 o_loss 0.43707922101020813; mean test acc 0.8189999461174011 with mse loss in atten align 0.001277464092709124\n",
            "510/2000 o_loss 0.4362325966358185; mean test acc 0.8199999928474426 with mse loss in atten align 0.0012764486018568277\n",
            "520/2000 o_loss 0.4343852996826172; mean test acc 0.8209999799728394 with mse loss in atten align 0.0012764486018568277\n",
            "530/2000 o_loss 0.433562308549881; mean test acc 0.8220000267028809 with mse loss in atten align 0.0013968960847705603\n",
            "540/2000 o_loss 0.43046364188194275; mean test acc 0.8220000267028809 with mse loss in atten align 0.0013863845961168408\n",
            "550/2000 o_loss 0.4281776547431946; mean test acc 0.8199999928474426 with mse loss in atten align 0.0013859543250873685\n",
            "560/2000 o_loss 0.4266497790813446; mean test acc 0.8209999799728394 with mse loss in atten align 0.0013859545579180121\n",
            "570/2000 o_loss 0.4252546727657318; mean test acc 0.824999988079071 with mse loss in atten align 0.0013859543250873685\n",
            "580/2000 o_loss 0.42391103506088257; mean test acc 0.8240000009536743 with mse loss in atten align 0.0013859543250873685\n",
            "590/2000 o_loss 0.4244349002838135; mean test acc 0.824999988079071 with mse loss in atten align 0.0014323407085612416\n",
            "600/2000 o_loss 0.4234476089477539; mean test acc 0.8260000348091125 with mse loss in atten align 0.0014201643643900752\n",
            "610/2000 o_loss 0.42222487926483154; mean test acc 0.8289999961853027 with mse loss in atten align 0.0014170199865475297\n",
            "620/2000 o_loss 0.42068642377853394; mean test acc 0.8260000348091125 with mse loss in atten align 0.0014170199865475297\n",
            "630/2000 o_loss 0.41927143931388855; mean test acc 0.8279999494552612 with mse loss in atten align 0.0014170198701322079\n",
            "640/2000 o_loss 0.41791680455207825; mean test acc 0.8270000219345093 with mse loss in atten align 0.0014170199865475297\n",
            "650/2000 o_loss 0.41658398509025574; mean test acc 0.8270000219345093 with mse loss in atten align 0.0014170201029628515\n",
            "660/2000 o_loss 0.415261447429657; mean test acc 0.8270000219345093 with mse loss in atten align 0.0014170199865475297\n",
            "670/2000 o_loss 0.41394615173339844; mean test acc 0.8260000348091125 with mse loss in atten align 0.0014170201029628515\n",
            "680/2000 o_loss 0.41263747215270996; mean test acc 0.8260000348091125 with mse loss in atten align 0.0014170199865475297\n",
            "690/2000 o_loss 0.41133445501327515; mean test acc 0.8260000348091125 with mse loss in atten align 0.0014170201029628515\n",
            "700/2000 o_loss 0.4100359380245209; mean test acc 0.8279999494552612 with mse loss in atten align 0.0014170198701322079\n",
            "710/2000 o_loss 0.40874195098876953; mean test acc 0.8270000219345093 with mse loss in atten align 0.0014170199865475297\n",
            "720/2000 o_loss 0.40745213627815247; mean test acc 0.8260000348091125 with mse loss in atten align 0.0014170201029628515\n",
            "730/2000 o_loss 0.40616682171821594; mean test acc 0.8260000348091125 with mse loss in atten align 0.0014170199865475297\n",
            "740/2000 o_loss 0.404885858297348; mean test acc 0.824999988079071 with mse loss in atten align 0.0014170201029628515\n",
            "750/2000 o_loss 0.4036094844341278; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170198701322079\n",
            "760/2000 o_loss 0.4023376405239105; mean test acc 0.8220000267028809 with mse loss in atten align 0.0014170199865475297\n",
            "770/2000 o_loss 0.4010707139968872; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014170199865475297\n",
            "780/2000 o_loss 0.3998086750507355; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170201029628515\n",
            "790/2000 o_loss 0.3985515534877777; mean test acc 0.824999988079071 with mse loss in atten align 0.0014170199865475297\n",
            "800/2000 o_loss 0.3972996473312378; mean test acc 0.8220000267028809 with mse loss in atten align 0.0014170198701322079\n",
            "810/2000 o_loss 0.39605289697647095; mean test acc 0.8220000267028809 with mse loss in atten align 0.0014170198701322079\n",
            "820/2000 o_loss 0.39481163024902344; mean test acc 0.8209999799728394 with mse loss in atten align 0.0014170199865475297\n",
            "830/2000 o_loss 0.39357560873031616; mean test acc 0.8209999799728394 with mse loss in atten align 0.0014170201029628515\n",
            "840/2000 o_loss 0.3923451900482178; mean test acc 0.8209999799728394 with mse loss in atten align 0.0014170201029628515\n",
            "850/2000 o_loss 0.3911203145980835; mean test acc 0.8220000267028809 with mse loss in atten align 0.0014170201029628515\n",
            "860/2000 o_loss 0.3899010121822357; mean test acc 0.8220000267028809 with mse loss in atten align 0.0014170199865475297\n",
            "870/2000 o_loss 0.38868746161460876; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014170201029628515\n",
            "880/2000 o_loss 0.3874795734882355; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014170199865475297\n",
            "890/2000 o_loss 0.38627752661705017; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014170199865475297\n",
            "900/2000 o_loss 0.38508114218711853; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014170199865475297\n",
            "910/2000 o_loss 0.3838905394077301; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170199865475297\n",
            "920/2000 o_loss 0.3827057182788849; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014170199865475297\n",
            "930/2000 o_loss 0.38152676820755005; mean test acc 0.8220000267028809 with mse loss in atten align 0.0014170201029628515\n",
            "940/2000 o_loss 0.38035356998443604; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170201029628515\n",
            "950/2000 o_loss 0.37918612360954285; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170198701322079\n",
            "960/2000 o_loss 0.3780246078968048; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170201029628515\n",
            "970/2000 o_loss 0.37686872482299805; mean test acc 0.824999988079071 with mse loss in atten align 0.0014170199865475297\n",
            "980/2000 o_loss 0.3757186233997345; mean test acc 0.824999988079071 with mse loss in atten align 0.0014170198701322079\n",
            "990/2000 o_loss 0.3745742440223694; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170199865475297\n",
            "1000/2000 o_loss 0.37343549728393555; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014170201029628515\n",
            "1010/2000 o_loss 0.3723025619983673; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170199865475297\n",
            "1020/2000 o_loss 0.37117519974708557; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170201029628515\n",
            "1030/2000 o_loss 0.37005355954170227; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014170199865475297\n",
            "1040/2000 o_loss 0.37072327733039856; mean test acc 0.8229999542236328 with mse loss in atten align 0.001506167114712298\n",
            "1050/2000 o_loss 0.3712012469768524; mean test acc 0.8260000348091125 with mse loss in atten align 0.001489307964220643\n",
            "1060/2000 o_loss 0.37041664123535156; mean test acc 0.8300000429153442 with mse loss in atten align 0.0014889100566506386\n",
            "1070/2000 o_loss 0.3687390685081482; mean test acc 0.8270000219345093 with mse loss in atten align 0.0014889101730659604\n",
            "1080/2000 o_loss 0.36727508902549744; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014889100566506386\n",
            "1090/2000 o_loss 0.3659466803073883; mean test acc 0.824999988079071 with mse loss in atten align 0.0014889100566506386\n",
            "1100/2000 o_loss 0.36468809843063354; mean test acc 0.8240000009536743 with mse loss in atten align 0.0014889100566506386\n",
            "1110/2000 o_loss 0.36347490549087524; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014889100566506386\n",
            "1120/2000 o_loss 0.362291544675827; mean test acc 0.8220000267028809 with mse loss in atten align 0.0014889100566506386\n",
            "1130/2000 o_loss 0.36112961173057556; mean test acc 0.8229999542236328 with mse loss in atten align 0.0014889100566506386\n",
            "1140/2000 o_loss 0.35998430848121643; mean test acc 0.8260000348091125 with mse loss in atten align 0.0014889100566506386\n",
            "1150/2000 o_loss 0.3588527739048004; mean test acc 0.824999988079071 with mse loss in atten align 0.0014889101730659604\n",
            "1160/2000 o_loss 0.35773301124572754; mean test acc 0.8270000219345093 with mse loss in atten align 0.0014889101730659604\n",
            "1170/2000 o_loss 0.3566243350505829; mean test acc 0.8279999494552612 with mse loss in atten align 0.0014889101730659604\n",
            "1180/2000 o_loss 0.35552552342414856; mean test acc 0.8279999494552612 with mse loss in atten align 0.0014889100566506386\n",
            "1190/2000 o_loss 0.35566219687461853; mean test acc 0.8319999575614929 with mse loss in atten align 0.0015549617819488049\n",
            "1200/2000 o_loss 0.3556176722049713; mean test acc 0.8330000042915344 with mse loss in atten align 0.0015378609532490373\n",
            "1210/2000 o_loss 0.3540405035018921; mean test acc 0.8370000123977661 with mse loss in atten align 0.0015288330614566803\n",
            "1220/2000 o_loss 0.3522181808948517; mean test acc 0.8390000462532043 with mse loss in atten align 0.0016296999529004097\n",
            "1230/2000 o_loss 0.34891122579574585; mean test acc 0.8339999914169312 with mse loss in atten align 0.001596270129084587\n",
            "1240/2000 o_loss 0.34712278842926025; mean test acc 0.8379999995231628 with mse loss in atten align 0.0015946883941069245\n",
            "1250/2000 o_loss 0.34537917375564575; mean test acc 0.8319999575614929 with mse loss in atten align 0.001650395104661584\n",
            "1260/2000 o_loss 0.34461766481399536; mean test acc 0.8279999494552612 with mse loss in atten align 0.0016367218922823668\n",
            "1270/2000 o_loss 0.3437293469905853; mean test acc 0.8270000219345093 with mse loss in atten align 0.0016363998875021935\n",
            "1280/2000 o_loss 0.34251999855041504; mean test acc 0.8270000219345093 with mse loss in atten align 0.0016363998875021935\n",
            "1290/2000 o_loss 0.3413475751876831; mean test acc 0.8270000219345093 with mse loss in atten align 0.0016364000039175153\n",
            "1300/2000 o_loss 0.34020572900772095; mean test acc 0.8279999494552612 with mse loss in atten align 0.0016363998875021935\n",
            "1310/2000 o_loss 0.3390893042087555; mean test acc 0.8289999961853027 with mse loss in atten align 0.0016364000039175153\n",
            "1320/2000 o_loss 0.33799418807029724; mean test acc 0.8289999961853027 with mse loss in atten align 0.0016363998875021935\n",
            "1330/2000 o_loss 0.3369174003601074; mean test acc 0.8289999961853027 with mse loss in atten align 0.001636400236748159\n",
            "1340/2000 o_loss 0.3358565866947174; mean test acc 0.8289999961853027 with mse loss in atten align 0.0016363998875021935\n",
            "1350/2000 o_loss 0.3373013734817505; mean test acc 0.824999988079071 with mse loss in atten align 0.0018208411056548357\n",
            "1360/2000 o_loss 0.33874428272247314; mean test acc 0.8279999494552612 with mse loss in atten align 0.001761064981110394\n",
            "1370/2000 o_loss 0.339884877204895; mean test acc 0.824999988079071 with mse loss in atten align 0.0017563876463100314\n",
            "1380/2000 o_loss 0.34142187237739563; mean test acc 0.8229999542236328 with mse loss in atten align 0.0017557325772941113\n",
            "1390/2000 o_loss 0.33964771032333374; mean test acc 0.8229999542236328 with mse loss in atten align 0.0017557324608787894\n",
            "1400/2000 o_loss 0.33815237879753113; mean test acc 0.8229999542236328 with mse loss in atten align 0.0017557324608787894\n",
            "1410/2000 o_loss 0.3368307054042816; mean test acc 0.8229999542236328 with mse loss in atten align 0.0017557325772941113\n",
            "1420/2000 o_loss 0.3356063663959503; mean test acc 0.824999988079071 with mse loss in atten align 0.0017557325772941113\n",
            "1430/2000 o_loss 0.3344397246837616; mean test acc 0.8270000219345093 with mse loss in atten align 0.0017557325772941113\n",
            "1440/2000 o_loss 0.3333118259906769; mean test acc 0.8270000219345093 with mse loss in atten align 0.0017557325772941113\n",
            "1450/2000 o_loss 0.33221328258514404; mean test acc 0.8289999961853027 with mse loss in atten align 0.0017557324608787894\n",
            "1460/2000 o_loss 0.33113762736320496; mean test acc 0.8289999961853027 with mse loss in atten align 0.0017557323444634676\n",
            "1470/2000 o_loss 0.3300805687904358; mean test acc 0.8279999494552612 with mse loss in atten align 0.0017557325772941113\n",
            "1480/2000 o_loss 0.32903939485549927; mean test acc 0.8279999494552612 with mse loss in atten align 0.0017557322280481458\n",
            "1490/2000 o_loss 0.3280123472213745; mean test acc 0.8270000219345093 with mse loss in atten align 0.0017557324608787894\n",
            "1500/2000 o_loss 0.3269982933998108; mean test acc 0.8279999494552612 with mse loss in atten align 0.0017557324608787894\n",
            "1510/2000 o_loss 0.32599616050720215; mean test acc 0.8279999494552612 with mse loss in atten align 0.0017557324608787894\n",
            "1520/2000 o_loss 0.3250054121017456; mean test acc 0.8279999494552612 with mse loss in atten align 0.0017557322280481458\n",
            "1530/2000 o_loss 0.3240254521369934; mean test acc 0.8279999494552612 with mse loss in atten align 0.0017557325772941113\n",
            "1540/2000 o_loss 0.32305583357810974; mean test acc 0.8270000219345093 with mse loss in atten align 0.0017557324608787894\n",
            "1550/2000 o_loss 0.32209622859954834; mean test acc 0.8270000219345093 with mse loss in atten align 0.0017557325772941113\n",
            "1560/2000 o_loss 0.3211462199687958; mean test acc 0.8270000219345093 with mse loss in atten align 0.0017557325772941113\n",
            "1570/2000 o_loss 0.32020559906959534; mean test acc 0.8289999961853027 with mse loss in atten align 0.0017557324608787894\n",
            "1580/2000 o_loss 0.31927400827407837; mean test acc 0.831000030040741 with mse loss in atten align 0.0017557324608787894\n",
            "1590/2000 o_loss 0.31835129857063293; mean test acc 0.8330000042915344 with mse loss in atten align 0.0017557325772941113\n",
            "1600/2000 o_loss 0.3174372911453247; mean test acc 0.8350000381469727 with mse loss in atten align 0.0017557324608787894\n",
            "1610/2000 o_loss 0.3165316879749298; mean test acc 0.8339999914169312 with mse loss in atten align 0.0017557325772941113\n",
            "1620/2000 o_loss 0.31563448905944824; mean test acc 0.8359999656677246 with mse loss in atten align 0.0017557324608787894\n",
            "1630/2000 o_loss 0.3147452771663666; mean test acc 0.8359999656677246 with mse loss in atten align 0.0017557325772941113\n",
            "1640/2000 o_loss 0.313864141702652; mean test acc 0.8359999656677246 with mse loss in atten align 0.0017557325772941113\n",
            "1650/2000 o_loss 0.31299078464508057; mean test acc 0.8350000381469727 with mse loss in atten align 0.0017557324608787894\n",
            "1660/2000 o_loss 0.31212517619132996; mean test acc 0.8359999656677246 with mse loss in atten align 0.0017557324608787894\n",
            "1670/2000 o_loss 0.3127062916755676; mean test acc 0.8370000123977661 with mse loss in atten align 0.0018232048023492098\n",
            "1680/2000 o_loss 0.3102727234363556; mean test acc 0.8390000462532043 with mse loss in atten align 0.0018185913795605302\n",
            "1690/2000 o_loss 0.30903348326683044; mean test acc 0.8330000042915344 with mse loss in atten align 0.001818581484258175\n",
            "1700/2000 o_loss 0.3079579770565033; mean test acc 0.8289999961853027 with mse loss in atten align 0.001818581484258175\n",
            "1710/2000 o_loss 0.30697596073150635; mean test acc 0.831000030040741 with mse loss in atten align 0.001818581484258175\n",
            "1720/2000 o_loss 0.30610689520835876; mean test acc 0.8319999575614929 with mse loss in atten align 0.001818581484258175\n",
            "1730/2000 o_loss 0.3052334785461426; mean test acc 0.831000030040741 with mse loss in atten align 0.001818581484258175\n",
            "1740/2000 o_loss 0.30439719557762146; mean test acc 0.831000030040741 with mse loss in atten align 0.0018185816006734967\n",
            "1750/2000 o_loss 0.3080024719238281; mean test acc 0.8359999656677246 with mse loss in atten align 0.0017445716075599194\n",
            "1760/2000 o_loss 0.3055175244808197; mean test acc 0.8339999914169312 with mse loss in atten align 0.0017367147374898195\n",
            "1770/2000 o_loss 0.3050008714199066; mean test acc 0.8300000429153442 with mse loss in atten align 0.0017364013474434614\n",
            "1780/2000 o_loss 0.3036434054374695; mean test acc 0.8359999656677246 with mse loss in atten align 0.0017364014638587832\n",
            "1790/2000 o_loss 0.30273503065109253; mean test acc 0.8359999656677246 with mse loss in atten align 0.0017364014638587832\n",
            "1800/2000 o_loss 0.3018079400062561; mean test acc 0.8330000042915344 with mse loss in atten align 0.0017364014638587832\n",
            "1810/2000 o_loss 0.30098599195480347; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364014638587832\n",
            "1820/2000 o_loss 0.30018681287765503; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364014638587832\n",
            "1830/2000 o_loss 0.2994207739830017; mean test acc 0.831000030040741 with mse loss in atten align 0.0017364013474434614\n",
            "1840/2000 o_loss 0.29867348074913025; mean test acc 0.831000030040741 with mse loss in atten align 0.0017364013474434614\n",
            "1850/2000 o_loss 0.29794275760650635; mean test acc 0.831000030040741 with mse loss in atten align 0.0017364014638587832\n",
            "1860/2000 o_loss 0.2972260117530823; mean test acc 0.831000030040741 with mse loss in atten align 0.0017364014638587832\n",
            "1870/2000 o_loss 0.29652139544487; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364013474434614\n",
            "1880/2000 o_loss 0.29582762718200684; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364013474434614\n",
            "1890/2000 o_loss 0.29514384269714355; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364014638587832\n",
            "1900/2000 o_loss 0.29446932673454285; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364014638587832\n",
            "1910/2000 o_loss 0.2938033640384674; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364013474434614\n",
            "1920/2000 o_loss 0.29314541816711426; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364014638587832\n",
            "1930/2000 o_loss 0.2924951910972595; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364013474434614\n",
            "1940/2000 o_loss 0.2918524742126465; mean test acc 0.831000030040741 with mse loss in atten align 0.0017364014638587832\n",
            "1950/2000 o_loss 0.29121679067611694; mean test acc 0.831000030040741 with mse loss in atten align 0.0017364013474434614\n",
            "1960/2000 o_loss 0.29058781266212463; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364014638587832\n",
            "1970/2000 o_loss 0.2899654507637024; mean test acc 0.8319999575614929 with mse loss in atten align 0.0017364014638587832\n",
            "1980/2000 o_loss 0.28934943675994873; mean test acc 0.8330000042915344 with mse loss in atten align 0.0017364014638587832\n",
            "1990/2000 o_loss 0.2887396216392517; mean test acc 0.8330000042915344 with mse loss in atten align 0.0017364014638587832\n",
            "2000/2000 o_loss 0.2893117070198059; mean test acc 0.843999981880188 with mse loss in atten align 0.0016950775170698762\n",
            "test set: outer loss 0.2872883677482605; batch acc 0.8420000076293945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVhU59n48e/DIigiKJsLKqioqCwquMclamJios3Wmpg0Jk2M2Zu8WWzTNv6S9m3Tpm8a2+xtzL7aLKaaZjEatySCxn0FVAQVAdn35f79cYZxQEBQYGC8P9c1FzNnnjnnPoeZe555zjn3MSKCUkqpjs/N2QEopZRqGZrQlVLKRWhCV0opF6EJXSmlXIQmdKWUchEezlpwYGCghIWFOWvxSinVIW3ZsiVLRILqe85pCT0sLIzExERnLV4ppTokY8yRhp7TIRellHIRmtCVUspFaEJXSikXoQldKaVchCZ0pZRyEZrQlVLKRWhCV0opF6EJXSml2kp1FXzxGKRvbZXZa0JXqr2proaqSmdH0fGJQEWJs6Oo7cgm+O4fkLm/VWavCV2p9qT4FPx1MDwZADs+sKYd+Q5yDjs1rHbhmz/AEj/Y/ErT2n9yJ/x5oLVN24u9K6y/A6a2yuydduq/UspB8hooOG4loRof3Q5e3eDdn1mP794MQUOcE19rqK6ClDXQtSec3Asf3wG9Y8G/3+k2vr2g4ATs/uj0tFUPQfxtYEzD8y4rhO3vWvePboYhs2Dnclj5PzBwGgy9AqKubZ31asypQxA8HLr1apXZG2ddgi4uLk60lotSQGk+/Klv09ou2gg9RzR/GSKQmwrd+zf/tS3tyHeQ8Ars+veZz3n6gF8fkGrITjrz+f4T4chG6DcBZj4BoXFWYj+xC6QKesVY7VK+hTfmnH7dLf+FZbNqz2vwLBhxLfiFgps79IoFj06QsQeMm/XleXwbVJRCUSb41FsPq7aQYeDmCSd2WNs8+yAc2wbY8mziq1bst37epE1VH2PMFhGJq/c5TejqrIpPwZbXYPQC6NLD2dE4V/5x60Pp5gHj7wavruc3v+oq+OQu2PHe6WmPZcDR7+GNudbjy/4MG5+F/HTr8ZK8+udVWQZ5aeDhDWX5tZ9LWg1fPgbz/w0RM5oX4/7/QtpmGHixlVAb6xk3pigLPlwAh9dbj32CIWwSdOttPR55IwRHnm6ffwwKT0K3PvDZ/VYve8hs+MuA2vP19oNSh23i1xfyjtYfw+RHYOhseO+G09vTUefuUJJje2CwJ+Lz5RNkjeeXF0LgYLgn4ZxnpQldnbuyQvjbiNNv8pAo6+/AqdbPZHcv6DfWmubeyeohlRfDpmehOMf68Ef/FIZfbfV6vHzPPwk6Q3ay1cM98AX88II1zb8fXPUS9J/Q+GtFrMQk1VByyvrpf/Ar67nMvVDtsAP0jvXQK/rMeVSWw+9tPURPHxj1c+u1Ay+GYz9CVhJk7GzaujyeC1Xl4OHVcLx7V1jj9sd3wK7ltZ/vM9qKJ/cI+DfQ43f3gGFzrf85WMn5hxet+3594cpnYdD0psVbV8Ye60s1wTaWHjDIiillrfX+6mt7P4bGw6AZ1vsX4NI/wvi7Ts+n5n8KkPT16fd4WYHVY/fsYr1/B14Mh9bBoJnQyafhuNITrSEVAN+eEHbR6fj8bb/Avn/RWu/AiHNbdzSht081RzG4t/FujNxUa0yxe5j1c7VGRak1hlmcDUMut5JWzZu9pfUbbyWC8Itg2E+sD05VOSSvtj70ItabftID1hEfUgXunvXPq6rC+jl84L/W/W69rQ9e6ibIPWr1pIdcBp26wsndVoKyv7bcSlw1n4HIOVZvL/IKOLkP8lKt4ZCyfPjyN7WX23sUHLMdenb1K3Bip5VYwydD1xBrLLhnNJTmwktTrORX14BpVpJP/Q7mvQMRMxvfbrs/tnq49fHqZvV2U7+zEtPoW2DAFOu51B9Ofwk5uv0b8Pa3jryIuMRKZF8/DqnfQ3HW6XbGHa5+2fqVcGJH7XkYNxh8We1peUfPbFdjxv+DSb9sfD2b6uBXViK+9I/g1sjxHSU51heqi+x/0ITeXhxaZ43XJa+BlQ+ent5nNISMgB/fggd2t84Ok9J8+OLX8OObp6dFXgmx860ksP19KDxR/2u9/a0dcilrYfClVi+lqpxaP0kz98HuT6x19PK1ktO4u63e555PrJ+bBSesn9vpW6GyCYeTdQ+Dwkyr7aQHoE+clQB3vGeNmeYcOr9tUqNLgPVF1lQ/ecEaf/1oISR91bTXBEVaX2An91pDNYNmNPwldTaHN9p6+zlW79GY073J+tQM6/SNt3YKnk3nHtavjsuesoYgPLytHitYHZGqMuvXWLXti7bmOUcVpdYXsSN3r7bvwLggTejtwb6V1rjd2Vz8W5j8UMssszQfXpxUf++wPj5BcMc6+PRuSN9iHVo14X4IHd0y8dSorrKS+7o/W2PzYPVWB0w53WP/6DZrzLW6ErIOND6/4GHWF2JoHFSWWvM3xup1pm+BwgzY86m1s+rSP9QeJvDwBp8AKMq2vjj2/sfa6XZ4AwybY/U+e0ZZPf3lt1i9/Qn3Wq+tqrB++mcnWV/UMddb004lW+t1ch8g1hEZcbee+9hzS6oosdY56Wt423aUx8iboIdtXLpnlPVl0x5iVfXShO5sFaXwVJiVMPrEWT/Dr33VGm+uKLHG8gBenGj9HTbXSkAjroUR18DQy63pVRXw7VNW0jrwpdWz7x0LYxZaPbbdH1u9Ns/OVo9530o4lXI6juifWT87x90Fm1+2fkr/8BLELYCB062k5BPYllvG+pmff8zaUdRQEinKsr4AkldbPemon1pDK24e4N2tbeN1JYc3QEmuNcSkOgxN6G2tugpWPQxFJ60jD8bfYx1CNe4umPXHhl/399H1H6rl7W8dj5tp6/HV5dXNGgKpLK093bOLddTAL76q/2exUqrDaSyh64BWa9jxAST+6/Tjg19af0cvaPx1t35h7YArzrF2wgUNgS3LrJ5rYIR16xoCl//FGov1CbTa2U9vFmvoomuINfzg27M11k4p1U5pD/18Ja+B/Z9bh3ZFXgkFGXDAdtLAog3W8b9fP249/m1W83eEVVVavWsd01RK0QI9dGPMLOBZwB34p4j8qc7z/YDXAX9bm8Uisuq8om7PRKwjQz5aWPsEhpodfD0GwkUPWjuYekbBiKut41PP5agGPSpAKdVEZ80Wxhh34DlgJpAGJBhjVojIHodmvwE+EJEXjDHDgFVAWCvE63xVFfD8uNpj3V17wvTfWcMcnXysnYuO/PvVrk+hlFKtoCndvzFAkoikABhj3gPmAo4JXYCaww38gGMtGWS7kbQa3roG+47Jix6CKY9a9R+UUsrJmpLQ+wCOhRHSgLF12iwBvjTG3Av4APUWizDGLAQWAvTr10F6rJVl1inSRzfDW1db00YvsM5O69TFqaEppZSjlhqgvR54TUT+aowZD7xpjBkhItWOjUTkZeBlsHaKttCyW8epFHjz6jPPRvzZ23rcrlKqXWpKQk8HHGt7htqmOfoFMAtARL4zxngDgcDJlgiyzX3/Ivz30drTPDrD/A+t07eVUqodakpCTwAijDHhWIl8HlD3HPZUYDrwmjEmEvAGMlsyUDsR2PCMdYhgfRXL8tKscpvncphfdRW8eJFVxAnghg+s08eLs62aFnpyjlKqHTtrQheRSmPMPcAXWIckvioiu40xTwCJIrIC+B/gFWPMA1h7DBdIax3gnvwNrP5/1u3qf8Lwq6zLdTk613oomfusZB4wyOqN19S3aOvT4ZVS6hw0aQzddkz5qjrTfudwfw8wsWVDa4BjbZKPbqu/TOc3T1q99Njrmz7fvf+B9+db92/8qH1c2UUppZqh410kumuwVTJ03jvW401Lrb+LNlhXchl4sfX4k0XWpa6aorzodDKf9KAmc6VUh9TxEvqwuXDTx9ZlpDxthw32m2CdkQlW77omqS+bZZ0IdDY7bVdkGXUzzHi85WNWSqk20PESuqMhtiulTPv16WnGwE9etK4cA/CvSxpP6hl74LP7rPtXPts6cSqlVBvo2An9Jy/CI4fOPJTQNwRu+tS6kMGxrXD0h/pfX1UBr9h687HztQCWUqpD69gJ3aNTw1ehd3OD29dY93e8b11kokZRtnUdxf/tbV10YuL98JPnWz9epZRqRa5dys8nAPz6wdY3rFvvkdZl2U4ln24TPc+6cK1SSnVwHbuH3hTXLbOuNwnW2Z6OF32Y+QRc/ZIOtSilXIJr99DBKml750ZnR6GUUq3O9XvoSil1gdCErpRSLkITulJKuQhN6Eop5SI0oSullIvQhK6UUi5CE7pSSrkITehKKeUiNKErpZSL0ISulFIuQhO6Ukq5CE3oSinlIjShK6WUi9CErpRSLkITulJKuQhN6Eop5SI0oSullIvQhK6UUi6iSQndGDPLGLPfGJNkjFlcz/PPGGO22W4HjDG5LR+qUkqpxpz1mqLGGHfgOWAmkAYkGGNWiMiemjYi8oBD+3uBka0Qq1JKqUY0pYc+BkgSkRQRKQfeA+Y20v564N2WCE4ppVTTNSWh9wGOOjxOs007gzGmPxAOfNPA8wuNMYnGmMTMzMzmxqqUUqoRLb1TdB6wXESq6ntSRF4WkTgRiQsKCmrhRSul1IWtKQk9Hejr8DjUNq0+89DhFqWUcoqmJPQEIMIYE26M6YSVtFfUbWSMGQp0B75r2RCVUko1xVkTuohUAvcAXwB7gQ9EZLcx5gljzByHpvOA90REWidUpZRSjTnrYYsAIrIKWFVn2u/qPF7ScmEppZRqLj1TVCmlXIQmdKWUchGa0JVSykVoQldKKRehCV0ppVyEJnSllHIRmtCVUspFaEJXSikXoQldKaVchCZ0pZRyEZrQlVLKRWhCV0opF6EJXSmlXIQmdKWUchGa0JVSykVoQldKKRehCV0ppVyEJnSllHIRmtCVUspFaEJXSikXoQldKaVchCZ0pZRyEZrQlVLKRWhCV0opF6EJXSmlXIQmdKWUchEezg5AXRgqKipIS0ujtLTU2aEo1SF4e3sTGhqKp6dnk1/TpIRujJkFPAu4A/8UkT/V0+anwBJAgO0ickOTo1AuLy0tDV9fX8LCwjDGODscpdo1ESE7O5u0tDTCw8Ob/LqzJnRjjDvwHDATSAMSjDErRGSPQ5sI4FfARBHJMcYEN3sNlEsrLS3VZK5UExljCAgIIDMzs1mva8oY+hggSURSRKQceA+YW6fN7cBzIpIDICInmxWFuiBoMleq6c7l89KUhN4HOOrwOM02zdFgYLAxZqMx5nvbEE19AS40xiQaYxKb+82j1IXgk08+Yc+ePWdv2IKWLFnC008/3SrznjBhQrPav/baaxw7dqzZy3nxxRd54403Gm2TmJjIfffd1+x5N0XXrl0bfT43N5fnn3++VZbtqKWOcvEAIoCpwPXAK8YY/7qNRORlEYkTkbigoKAWWrRSLaOystLZITSa0NtDfM21adOmZrVvLKFXVVU1+LpFixbx85//vNF5x8XFsXTp0mbF01LaU0JPB/o6PA61TXOUBqwQkQoROQQcwErwSrUbb7zxBtHR0cTExHDTTTcBsGDBAhYtWsTYsWN55JFH2LZtG+PGjSM6OpqrrrqKnJwcAJYuXcqwYcOIjo5m3rx5AHz77bfExsYSGxvLyJEjKSgoOGOZb731FmPGjCE2NpY77rjDnpS6du3KY489RkxMDOPGjSMjI4NNmzaxYsUKHn74YWJjY0lOTmbq1Kn88pe/JC4ujmeffZbVq1czcuRIoqKiuPXWWykrKwMgLCyMRx55hKioKMaMGUNSUhIFBQWEh4dTUVEBQH5+fq3H9Wnp9a/pua5du5apU6dy7bXXMnToUObPn4+I1Gq7fPlyEhMTmT9/PrGxsZSUlBAWFsajjz7KqFGj+PDDD3nllVeIj48nJiaGa665huLiYqD2r4ypU6fy6KOPMmbMGAYPHsz69evtMVxxxRX29rfeeitTp05lwIABtRL9k08+yZAhQ5g0aRLXX399vb9eDh06xPjx44mKiuI3v/mNfXphYSHTp09n1KhRREVF8emnnwKwePFikpOTiY2N5eGHH26w3XkTkUZvWL3vFCAc6ARsB4bXaTMLeN12PxBriCagsfmOHj1a1IVjz5499vtLVuySn764qUVvS1bsanT5u3btkoiICMnMzBQRkezsbBERufnmm2X27NlSWVkpIiJRUVGydu1aERH57W9/K/fff7+IiPTq1UtKS0tFRCQnJ0dERK644grZsGGDiIgUFBRIRUXFGet8xRVXSHl5uYiI3HnnnfL666+LiAggK1asEBGRhx9+WJ588kl7PB9++KF9HlOmTJE777xTRERKSkokNDRU9u/fLyIiN910kzzzzDMiItK/f3/5/e9/LyIir7/+usyePVtERBYsWCAff/yxiIi89NJL8uCDD56xbR5//HH5y1/+0uLrLyLi4+MjIiJr1qyRbt26ydGjR6WqqkrGjRsn69evP6P9lClTJCEhwf64f//+8tRTT9kfZ2Vl2e8/9thjsnTp0jPWYcqUKfb1XLlypUyfPt0eQ812efzxx2X8+PFSWloqmZmZ0qNHDykvL5fNmzdLTEyMlJSUSH5+vgwaNMg+X0dXXnml/X/5j3/8w76eFRUVkpeXJyIimZmZMnDgQKmurpZDhw7J8OHD7a9vqF1djp+bGkCiNJBXz9pDF5FK4B7gC2Av8IGI7DbGPGGMmWNr9gWQbYzZA6wBHhaR7Jb5ylHq/H3zzTdcd911BAYGAtCjRw/7c9dddx3u7u7k5eWRm5vLlClTALj55ptZt24dANHR0cyfP5+33noLDw/r4LCJEyfy4IMPsnTpUnJzc+3Ta6xevZotW7YQHx9PbGwsq1evJiUlBYBOnTrZe4ujR4/m8OHDDcb+s5/9DID9+/cTHh7O4MGDz4gP4Prrr7f//e677wC47bbbWLZsGQDLli3jlltuaXA5Lb3+dY0ZM4bQ0FDc3NyIjY1tdJ3rW3+AXbt2cdFFFxEVFcXbb7/N7t27633N1VdfDTS+bWfPno2XlxeBgYEEBweTkZHBxo0bmTt3Lt7e3vj6+nLllVfW+9qNGzfat3fNrz2wOsi//vWviY6OZsaMGaSnp5ORkXHG65varrmadBy6iKwCVtWZ9juH+wI8aLsp1ajHrxzu7BBq8fHxOWublStXsm7dOj777DP+8Ic/sHPnThYvXszs2bNZtWoVEydO5IsvvmDo0KH214gIN998M3/84x/PmJ+np6f9KAZ3d/dGx8ebEh/UPiqi5v7EiRM5fPgwa9eupaqqihEjRjRpXnWdy/rX5eXlZb9/tnV25Lj+CxYs4JNPPiEmJobXXnuNtWvXNrqsxpZzrvHUqO8olLfffpvMzEy2bNmCp6cnYWFh9Z5M19R2zaWn/qsLwsUXX8yHH35Idrb1w/HUqVNntPHz86N79+72Mdc333yTKVOmUF1dzdGjR5k2bRpPPfUUeXl5FBYWkpycTFRUFI8++ijx8fHs27ev1vymT5/O8uXLOXnypH2ZR44caTROX1/feseiAYYMGcLhw4dJSkqqFV+N999/3/53/Pjx9uk///nPueGGGxrtnbfG+p+LxtYfoKCggF69elFRUcHbb7993sura+LEiXz22WeUlpZSWFjIf/7znwbbvffeewC14sjLyyM4OBhPT0/WrFlj/3/XXa+G2p0vPfVfXRCGDx/OY489xpQpU3B3d2fkyJG89tprZ7R7/fXXWbRoEcXFxQwYMIBly5ZRVVXFjTfeSF5eHiLCfffdh7+/P7/97W9Zs2YNbm5uDB8+nMsuu6zWvIYNG8bvf/97LrnkEqqrq/H09OS5556jf//+DcY5b948br/9dpYuXcry5ctrPeft7c2yZcu47rrrqKysJD4+nkWLFtmfz8nJITo6Gi8vL95991379Pnz5/Ob3/zGPkTQmJZc/3NRs5O6c+fO9mEjR08++SRjx44lKCiIsWPHNpr8z0V8fDxz5swhOjqakJAQoqKi8PPzO6Pds88+yw033MBTTz3F3LmnT8uZP38+V155JVFRUcTFxdl/sQQEBDBx4kRGjBjBZZddxqOPPlpvu/NlpM6e5rYSFxcniYmJTlm2ant79+4lMjLS2WG4rLCwMBITE+37CBwtX76cTz/9lDfffNMJkXU8hYWFdO3aleLiYiZPnszLL7/MqFGjnBJLfZ8bY8wWEYmrr7320JVyYffeey+ff/45q1atOntjBcDChQvZs2cPpaWl3HzzzU5L5udCE7pSLqChIzn+/ve/t20gLuCdd95xdgjnTHeKKqWUi9CErpRSLkITulJKuQhN6Eop5SI6XEL/bPsxrntxE5VV1c4ORXUghw8fbvZZkk0p5fraa69xzz33nE9oHUJTytM6Onz48HntXPzf//3fWo+bW4a3qaZOncrZDp/+29/+Zi8C1t51uITu5eFGwuEcPt91wtmhKBd3rrW5nam1Suw2pTyto5ZO6M0tw9uSNKG3ohmRIQwI9OHldSm1ym/mlVTw8Y9pVGjPXTWgsrKS+fPnExkZybXXXmv/kD7xxBPEx8czYsQIFi5ciIjUW8o1ISGBCRMmEBMTw5gxY+xnKR47doxZs2YRERHBI488Uu+y61sGQFJSEjNmzCAmJoZRo0aRnJwMwFNPPUVUVBQxMTEsXrwYqN2bzMrKIiwsDLC+eObMmcPFF1/M9OnTGy3NWreEcFNL7DalPK2jxYsXs379emJjY3nmmWeoqqri4YcfJj4+nujoaF566SUAjh8/zuTJk4mNjWXEiBGsX7+exYsXU1JSQmxsLPPnzweaVoZ31apVDB06lNGjR3PffffZi585KikpYd68eURGRnLVVVdRUlJif+7OO+8kLi6O4cOH8/jjjwNW2eBjx44xbdo0pk2b1mC7dqOhMoytfTuf8rlvf39E+j/6H1l/wCqFWl1dLbcu2yz9H/2PXP7sOvkgIVUqq84sRamcp1YZ0FWPirx6ecveVj3a6PIPHTokgL3c6y233GIvi1pTSldE5MYbb7SXtXUs5VpWVibh4eGyefNmERHJy8uTiooKWbZsmYSHh0tubq6UlJRIv379JDU19YzlN7SMMWPGyEcffSQiVnncoqIiWbVqlYwfP16KiopqvdYxnszMTOnfv7+IiCxbtkz69Oljb9dQadaGSgg3t8RuQ+VpHTmWqq2Zb02J4NLSUhk9erSkpKTI008/bS/7W1lZKfn5+SJyuuxujbOV4a0pLZySkiIiIvPmzau1/Bp//etf5ZZbbhERke3bt4u7u7t9m9Zsj8rKSpkyZYps375dRKwSvjXbrLF2raHFy+e2R1eP6kMf/84s+Ww3J/JKuffdH1m97ySj+vlTUFrJw8t38Ncv9zs7TNXO9O3bl4kTJwJw4403smHDBgDWrFnD2LFjiYqK4ptvvqm3JOv+/fvp1asX8fHxAHTr1s1eLnb69On4+fnh7e3NsGHD6i20VN8yCgoKSE9P56qrrgKsWi1dunTh66+/5pZbbqFLly5A7VK/DZk5c6a9nTRQmrWhEsLNKbFboynlaR19+eWXvPHGG8TGxjJ27Fiys7M5ePAg8fHxLFu2jCVLlrBz5058fX3POq/6yvDu27ePAQMGEB4eDtBg3Zp169Zx4403AlZJ4OjoaPtzH3zwAaNGjWLkyJHs3r27wStHNbWdM3TIM0W9Pd35w1UjuOW1BMb9cTUeboaHLhnMXVMHYQw8snwHL3ybzMRBgUwcdGZtC+Vkl/3JKYutW+7UGENpaSl33XUXiYmJ9O3blyVLljS7jOnZyrC2xDIAPDw8qK6uts/TkWOJ2eaWZj2XErtNKU/rSET4+9//zqWXXnrGc+vWrWPlypUsWLCABx988Kxj9edb9rY+hw4d4umnnyYhIYHu3buzYMGCerdZU9s5S4fsoQNMHRLMO7eN447JA/jorgncc3EEbm4GYwyPzxnOwKCuPPThdkorGr4OobqwpKam2iv4vfPOO0yaNMn+YQwMDKSwsLBWhUPHkqdDhgzh+PHjJCQkAFYZ16YmkoaW4evrS2hoKJ988gkAZWVlFBcXM3PmTJYtW2Yf468p9RsWFsaWLVsAzqjE6Kih0qyNlRBuaondpqpbLvbSSy/lhRdesI/NHzhwgKKiIo4cOUJISAi33347t912G1u3bgWsevGNXSqvriFDhpCSkmL/tVBTSriuyZMn23fW7tq1ix07dgDWvgMfHx/8/PzIyMjg888/r3ddGmvXHnTYhA4wfmAAv7o8kujQ2tej7urlwZIrh3M8r5T3E446KTrV3gwZMoTnnnuOyMhIcnJyuPPOO/H39+f2229nxIgRXHrppfYhFThdyjU2Npaqqiref/997r33XmJiYpg5c2aTe2aNLePNN99k6dKlREdHM2HCBE6cOMGsWbOYM2cOcXFxxMbG2ndGPvTQQ7zwwguMHDmSrKysBpc3f/58EhMTiYqK4o033rCXZnUsIRwTE8ODDz5Y6zU5OTlNKrHbFNHR0bi7uxMTE8MzzzzDbbfdxrBhwxg1ahQjRozgjjvuoLKykrVr1xITE8PIkSN5//33uf/++wGrQFbNVZKaonPnzjz//PPMmjWL0aNH4+vrW2/Z2zvvvJPCwkIiIyP53e9+x+jRowHsMQwdOpQbbrjBPjRXE8usWbOYNm1ao+3aA5ctnysi/Oyl70nJKuLeiwexKTmLS4b15JrRoa22TNUwLZ/bvrlCid2asrciwt13301ERAQPPPCAs8M6L1o+18YYw6OXDeWGV77n8RW76ebtwRe7M9iUnM2TPxlOl04uu+pKNYurlNh95ZVXeP311ykvL2fkyJHccccdzg6pzblsD71GbnE5+SWV9Pb35u/fJLH0m4MMDOrKX66NZmS/7q2+fGXRHrpSzdfcHnqHHkNvCv8unegX0AUPdzcemDmYt38xloLSCq56fhMPfbidzIIyZ4eolFItwuUTel0TBgWy+n+mcseUAXy6LZ25/9hAWk7HOK23o3PWr0GlOqJz+bxccAkdrKNgfnVZJB/fNakg3XcAABkjSURBVJHCskpu/OcPpGZrUm9N3t7eZGdna1JXqglEhOzsbLy9vZv1OpcfQz+brak5LHh1Mx7ubrz5izEM733moU7q/FVUVJCWltauTsJQqj3z9vYmNDQUT0/PWtMbG0O/4BM6wKGsIua/8j2FZZW88YuxxPb1P/uLlFLKCS7onaJNER7ow/t3jMe/Syfmvfwd/9113NkhKaVUs2lCt+nbowv/vnMCkb26seitrTy/NknHe5VSHUqTEroxZpYxZr8xJskYs7ie5xcYYzKNMdtst9taPtTWF+Trxbu3j2NOTG/+/N/9/M+H2ymr1FowSqmO4aynSxpj3IHngJlAGpBgjFkhInVrRr4vIh3+Wlzenu48Oy+WgUFdeebrAxzOKuK5+aPo5dfZ2aEppVSjmtJDHwMkiUiKiJQD7wFzWzcs5zLGcP+MCJ67YRT7TxRwxdINbExquBiSUkq1B01J6H0Ax5KFabZpdV1jjNlhjFlujOlb34yMMQuNMYnGmMTMzMxzCLdtzY7uxaf3TKKHTydu+tcP/OObg1RX67i6Uqp9aqmdop8BYSISDXwFvF5fIxF5WUTiRCQuKCiohRbdugYFd+WTuydyRXRvnv7yALe+nqDlApRS7VJTEno64NjjDrVNsxORbBGpyXL/BEa3THjtg4+XB8/Oi+XJucP5LjmbWX9bx1d7MpwdllJK1dKUhJ4ARBhjwo0xnYB5wArHBsaYXg4P5wB7Wy7E9sEYw03jw/js3kmEdPPm9jcS+dVHOykuP//LXymlVEs4a0IXkUrgHuALrET9gYjsNsY8YYyZY2t2nzFmtzFmO3AfsKC1Ana2wSG+fHz3BO6YMoD3ElK5/Nn1fJ+S7eywlFJKT/0/H98lZ/PIv7dz9FQJ14/py+LLIvHr7Hn2Fyql1DnSU/9byfiBAXzxy8ksnDyA9xOOMuP/vuXzncf1DFOllFNoD72F7EzL49F/72DP8XxmDgthwsAA0nNKmBQRyNQhwc4OTynlIrTaYhupqKrmXxsO8cxXByirrLZPvzyqJw/OHMygYF8nRqeUcgWa0NtYanYxe0/kM3VIEM98dZAXv00G4LdXDOP6MX31AtVKqXOmCd3JdqXncdfbW0k9VUy/Hl247aJw5sT0xr9LJ2eHppTqYDShtwMiwtoDmfy/Fbs5nF1MJw83rojqxcIpAxjas5uzw1NKdRCa0NuZvcfzeeeHVD7amkZReRXThwazaOpA4sN6ODs0pVQ7pwm9ncotLuf1TUd4bdMhcoorGNrTlyE9fbkoIohrRvXBGOPsEJVS7Ywm9HaupLyKDxKP8uGWo2QWlJGRX8bY8B78csZgxg3ooYldKWWnCb0Dqa4W3tmcyt++PkhWYRlx/buzaMpApg0Nxt1NE7tSFzpN6B1QaYXVa39xbTLH8krp49+ZefF9+Vl8X4K7eTs7PKWUk2hC78Aqqqr5ek8Gb/+QyoakLNzdDJMGBXLVyD5cMjxEj2lX6gKjCd1FHMoq4sPEo3y67RjpuSV09nTn0uEhzB3Zh4sGBeLhrqV5lHJ1mtBdTHW1sCU1h49/TGfljuPklVQQ4NOJ2dG9mBvbm1H9uuuOVKVclCZ0F1ZeWc3a/Sf5dPsxvt6TQVllNaHdOzMnpjdzY/swpKfWj1HKlWhCv0AUllXy5e4TfLrtGBuSsqiqFuL6d6ennzd9e3Th2tGhDAj0QQTc9IgZpTokTegXoKzCMl7dcIgXvk0m2NeLU0XlVFQJPXw64d/ZkxduHK29d6U6IE3oF7DSiiq8Pd3JLCjjvc2prNx5nH0nCujs6c6vZ0cyf0w/7a0r1YFoQle1ZOSX8tCH21l/MIuYvv78fFx/Lhkegq+3Xj5PqfZOE7o6g4jw8Y/p/N9XB0jLKcHLw43pkcHMHBbClMHBnCoqx9fbgxA9iUmpdqWxhK5npVygjDFcPSqUq0b2YWtqLp9uS2fVzhOs2nmiVruwgC5Mjwxh1oiejO7XXYdnlGrHtIeu7KqrhV3H8lh/MAu/zp4Ul1fyXXI2G5OyKa+qJsjXi0uGWcl93IAAPPVEJqXanA65qPNSUFrBmv2ZfLHrBGv2n6S4vAq/zp7MiAxh5rAQLooIxMdLf+wp1RY0oasWU1pRxboDmfx39wm+3pNBfmklnTzcmDAwgBmRIUyPDKaXX2dnh6mUy9KErlpFRVU1CYdP8fWek3y9N4PUU8UAjOjTjRmRIcyIDGF4725ahkCpFqQJXbU6ESHpZCFf77WS+9bUHESgl5830yODmREZwviBAXh5uDs7VKU6tPNO6MaYWcCzgDvwTxH5UwPtrgGWA/Ei0mi21oTu2rIKy1izz0ru6w5kUVJRRZdO7kyOCGLGsBCmDQkioKuXs8NUqsM5r4RujHEHDgAzgTQgAbheRPbUaecLrAQ6AfdoQlc1Siuq+C4lm6/3ZLB670lO5JdiDIzu151pQ4OZNiSYyF6+OjSjVBOcb0IfDywRkUttj38FICJ/rNPub8BXwMPAQ5rQVX1EhN3H8vlqTwar92WwKz0fsIZmpg4JZtqQICYO0qNmlGrI+Z5Y1Ac46vA4DRhbZwGjgL4istIY83AjgSwEFgL069evCYtWrsYYw4g+fozo48cDMwdzMr+UtfszWbP/JJ9tP8a7m1Pp5O7G2AE9mDokmIuHBhMe6OPssJXqEM67G2SMcQP+D1hwtrYi8jLwMlg99PNdtur4grt589P4vvw0vi/lldUkHjnFmn0nWbM/kyf/s4cn/7OHsIAu9qGZsQN66I5VpRrQlISeDvR1eBxqm1bDFxgBrLWNgfYEVhhj5pxt2EUpR9bx7IFMGBjIY7MhNbuYNftPsmb/Sd75IZVlGw/TpZM7EwYGcvHQYKYOCaK3vx7zrlSNpoyhe2DtFJ2OlcgTgBtEZHcD7deiY+iqhZWUV/FdShbf7DvJmn2ZpOeWADC0py9ThgQxJSKI0WHdtfeuXN55jaGLSKUx5h7gC6zDFl8Vkd3GmCeARBFZ0bLhKnWmzp3cuXhoCBcPDUFEOHiykDX7TvLNvpP8a/0hXvo2hc6e7owb0IPJg4OYPDiIAYE+euSMuqDoiUWqwyssq+T75GzWHcxk3YFMDmdbZ6z28e9sJfeIQCYMCsSvs9Z7Vx2fnimqLiip2cV8ezCT9Qcy2ZScTWFZJe5uhti+/kyOCGLy4ECiQ/1x11LAqgPShK4uWBVV1fyYmsu6A5msO5jJzvQ8RMCvsyeTBgUyeXAgF0XozlXVcWhCV8rmVFE5G5KyrAR/IJOTBWUADAj0YeKgQCYOCmT8gAD8uujwjGqfNKErVQ8RYX9GARsOZrExKYsfDp2iuLwKNwNRof5MGhTAxEGBjOrXHbC+DNJzS6isEiJ7+eLfpZOT10BdiDShK9UE5ZXVbDuay8YkK8H/eDSXqmqhk7sb5VXVtdp2cndj5rAQro0LZXJEkI7HqzajCV2pc1BQWsHmQ6fYkJRFSmYRU4cEERbgg6e7G2v2n+TjH9M5VVROgE8nLh4azAzb1Zu6dNI6NKr1aEJXqhWUV1bzzb4MPt91gm/2naSgtBIvDzfGDwyw7XANIiK4qx4Lr1qUJnSlWllFVTUJh07x5Z4M1h3MJCWzCICQbl5MHBTI5AirimSQr9aAV+fnfKstKqXOwtPdjQmDrBOYANJzS9hwMJP1B7NYs+8kH221yh9F9urGpEEBTBgUyJiwHlomWLUo7aEr1cqqq60a8OsOZrLhYBZbjuRQXlWNp7thZN/uDO3ly+5j+bgbw7iBAVw3OpS+Pbo4O2zVTumQi1LtSEl5FYlHTrExKZtNyVnsPpaPASqrT38WJw8OYvGsoQzr3c15gap2SRO6Uu1YVbXg7maoqhYy8kv5IPEor6xLIdDXiyfmjmDSoEA9LFLZaUJXqoNZs/8kd7y5hfLKanr7eXPN6FCuGRVK/4AuetTMBU4TulIdUEFpBesOZPF+4lHWH8xExKogGdvPn2G9ujE2vAcxff3xdHdzdqiqDelRLkp1QL7ensyO7sXs6F4czyth5Y7jbDmSw/ajuazccRyArl4ejBsQwEURgUyKCGxSDfhTReV8tv0YqaeKuSK6F7F9/bXX7yK0h65UB5RTVM53KdmsP5jFhqRMjp6yruDU28+bSRGBjB8YwNjwAHsVyS1HTvHptmMkHs5h34l8qgU83Q0VVcLQnr5cGdObmcNC9ESoDkCHXJRycUeyi6zkfjCL71KyySupAKBfjy5E9fFj5c7jeLobxoT3IK5/D2aN6Elo986s2H6MDxPT2HY0197+4qHBTBgYwLiBAXTz1qqT7Y0mdKUuINXVwt4T+XyfcorvU7LZfOgU7m6GFfdMJLR7/ce3Z+SXsnrvSb7ac4LvUrIprai2V52cONCqOjm6f3e8PfWarc6mCV2pC1h1tVAtgkcTd56WVVbxY2oum5Ky2JiczTaHqpMxff2ID+tBfHgPRvfvrj14J9CErpQ6Z4VllWw+lM13ydlsPpzDrvQ8qqoFNwNDe3ZjTHgPxoT3ID6sh9aqaQOa0JVSLaa4vJIfU3PZfOgUCYdPsTU1h9IKq158eKAP8WHdGdWvOyP7dSciuCtuelJUi9LDFpVSLaZLJw/75frAqjS5Kz2PhMOn2Hwohy/3ZPBBYhoAvl4exPT1Z2Q/6xbbtzs9fPRKT61Fe+hKqRYlIhzOLubH1By2pubwY2ou+04UUGWrVRMe6MNIe5LvzpCevnpyVDPokItSyqmKyyvZmZbH1tRcW6LPJavQukC3t6cbI3r7MaKPH1F9/IgK9WNAoE+Td+JeaDShK6XaFREhPbeEram5bD2Sw870PPYcy6ekogqwkvywXt2I6mMl+mG9u7HlSA6DgrsyYWCgk6N3Lk3oSql2r6paSMksZGd6HjvT89iVnsfuY/kUl1fZ2xgDS64cztzY3vh3uTDH4jWhK6U6pKpq4VBWEbuP5RHU1Yvn1yazISkLgP4BXYgO9Scm1I/oUH9G9Olmv0B3anYxv/10FwsnD2DCwACXKmdw3gndGDMLeBZwB/4pIn+q8/wi4G6gCigEForInsbmqQldKdVcVdXC5kOn+PFoDjuO5rEjLZdjeaUAuBmICPYlKtSPHWm5HMgoBMDX24MAn07cPCGMiyICGRDYsQ+lPK+EboxxBw4AM4E0IAG43jFhG2O6iUi+7f4c4C4RmdXYfDWhK6VaQmZBGTvSctmeZiX4HWl5nCoq56KIQMIDfaiyXQKwpl6Nr5eHtQM21NoJGx3qR78eHafO/Pkehz4GSBKRFNvM3gPmAvaEXpPMbXwA54zjKKUuOEG+XkyPDGF6ZAhg7XDNLCgjyNfLnqRFhOTMIn5MzWHb0Vx2pefx2sbDlFdZJ0R19fIgIqQrQ0J8GRziy5CevkSEdCWoq1eHSfTQtITeBzjq8DgNGFu3kTHmbuBBoBNwcX0zMsYsBBYC9OvXr7mxKqXUWRljCO7mfca0QcFdGRTclevi+gJQXlnNgYwCdqTlse9EPvtPFPDF7hO8l3A63XXv4slgW5IfFNyVAUE+DAjqSq9u3u1y2KbFzhQVkeeA54wxNwC/AW6up83LwMtgDbm01LKVUqq5Onm4McJ2WGQNESGrsJwDGQX22/4TBXzyYzoFZZX2dp093QkP9GFAkA8Dg7ra/4YH+uDj5bwT8Juy5HSgr8PjUNu0hrwHvHA+QSmllDMYYwjy9SLI18te2gBOD+MkZxaRnFlISmYRKVmFbE/LZeXO4zjuiuzl52315AOtBB8e6ENYoA+h3TtTXFZFem4Jkb18W2UopykJPQGIMMaEYyXyecANjg2MMREictD2cDZwEKWUchE1wzjB3bwZPzCg1nOlFVUcyS4mJbPQnuyTs4rO6NW7uxl7+YPfzI7ktosGtHicZ03oIlJpjLkH+ALrsMVXRWS3MeYJIFFEVgD3GGNmABVADvUMtyillCvy9nRnSE9rR6ojESG7qJzDWUUcyiricHYR2YXl9PLrzOVRvVolFj2xSCmlOpDGDlvU6jdKKeUiNKErpZSL0ISulFIuQhO6Ukq5CE3oSinlIjShK6WUi9CErpRSLkITulJKuQinnVhkjMkEjpzjywOBrBYMp6VoXM3TXuOC9hubxtU8rhhXfxEJqu8JpyX082GMSWzoTCln0riap73GBe03No2reS60uHTIRSmlXIQmdKWUchEdNaG/7OwAGqBxNU97jQvab2waV/NcUHF1yDF0pZRSZ+qoPXSllFJ1aEJXSikX0eESujFmljFmvzEmyRizuI2X3dcYs8YYs8cYs9sYc79t+hJjTLoxZpvtdrnDa35li3W/MebSVoztsDFmp235ibZpPYwxXxljDtr+drdNN8aYpba4dhhjRrVSTEMctsk2Y0y+MeaXzthexphXjTEnjTG7HKY1e/sYY262tT9ojDnvK3M1ENdfjDH7bMv+2Bjjb5seZowpcdhuLzq8ZrTt/59ki/28LljZQFzN/r+19Oe1gbjed4jpsDFmm216W26vhnJD277HRKTD3LAugZcMDAA6AduBYW24/F7AKNt9X+AAMAxYAjxUT/ththi9gHBb7O6tFNthILDOtD8Di233FwNP2e5fDnwOGGAc8EMb/e9OAP2dsb2AycAoYNe5bh+gB5Bi+9vddr97K8R1CeBhu/+UQ1xhju3qzGezLVZji/2yVoirWf+31vi81hdXnef/CvzOCdurodzQpu+xjtZDHwMkiUiKiJQD7wFz22rhInJcRLba7hcAe4E+jbxkLvCeiJSJyCEgCWsd2spc4HXb/deBnzhMf0Ms3wP+xpjWucjhadOBZBFp7OzgVtteIrIOOFXP8pqzfS4FvhKRUyKSA3wFzGrpuETkSxGpubrw90BoY/OwxdZNRL4XKyu84bAuLRZXIxr6v7X457WxuGy97J8C7zY2j1baXg3lhjZ9j3W0hN4HOOrwOI3GE2qrMcaEASOBH2yT7rH9dHq15mcVbRuvAF8aY7YYYxbapoWIyHHb/RNAiBPiqjGP2h80Z28vaP72ccZ2uxWrJ1cj3BjzozHmW2PMRbZpfWyxtEVczfm/tfX2ugjIEJGDDtPafHvVyQ1t+h7raAm9XTDGdAX+DfxSRPKBF4CBQCxwHOtnX1ubJCKjgMuAu40xkx2ftPVEnHKMqjGmEzAH+NA2qT1sr1qcuX0aYox5DKgE3rZNOg70E5GRwIPAO8aYbm0YUrv7v9VxPbU7DW2+verJDXZt8R7raAk9Hejr8DjUNq3NGGM8sf5hb4vIRwAikiEiVSJSDbzC6WGCNotXRNJtf08CH9tiyKgZSrH9PdnWcdlcBmwVkQxbjE7fXjbN3T5tFp8xZgFwBTDflgiwDWlk2+5vwRqfHmyLwXFYplXiOof/W1tuLw/gauB9h3jbdHvVlxto4/dYR0voCUCEMSbc1uubB6xoq4Xbxuj+BewVkf9zmO44/nwVULMHfgUwzxjjZYwJByKwdsa0dFw+xhjfmvtYO9V22ZZfs5f8ZuBTh7h+btvTPg7Ic/hZ2Bpq9Zycvb0cNHf7fAFcYozpbhtuuMQ2rUUZY2YBjwBzRKTYYXqQMcbddn8A1vZJscWWb4wZZ3uP/txhXVoyrub+39ry8zoD2Cci9qGUttxeDeUG2vo9dj57dp1xw9o7fADr2/axNl72JKyfTDuAbbbb5cCbwE7b9BVAL4fXPGaLdT/nuSe9kbgGYB1BsB3YXbNdgABgNXAQ+BroYZtugOdsce0E4lpxm/kA2YCfw7Q2315YXyjHgQqscclfnMv2wRrTTrLdbmmluJKwxlFr3mMv2tpeY/v/bgO2Alc6zCcOK8EmA//AdhZ4C8fV7P9bS39e64vLNv01YFGdtm25vRrKDW36HtNT/5VSykV0tCEXpZRSDdCErpRSLkITulJKuQhN6Eop5SI0oSullIvQhK46PGNMoe1vmDHmhhae96/rPN7UkvNXqiVpQleuJAxoVkK3nWHYmFoJXUQmNDMmpdqMJnTlSv4EXGSs2tcPGGPcjVVbPMFWUOoOAGPMVGPMemPMCmCPbdontsJmu2uKmxlj/gR0ts3vbdu0ml8DxjbvXcaqq/0zh3mvNcYsN1ZN87dtZxEq1erO1jtRqiNZjFWv+woAW2LOE5F4Y4wXsNEY86Wt7ShghFjlXgFuFZFTxpjOQIIx5t8istgYc4+IxNazrKuxilTFAIG216yzPTcSGA4cAzYCE4ENLb+6StWmPXTlyi7BqpexDauUaQBWPQ+AzQ7JHOA+Y8x2rPrjfR3aNWQS8K5YxaoygG+BeId5p4lVxGob1lCQUq1Oe+jKlRngXhGpVdzIGDMVKKrzeAYwXkSKjTFrAe/zWG6Zw/0q9HOm2oj20JUrKcC6/FeNL4A7bWVNMcYMtlWjrMsPyLEl86FYlwSrUVHz+jrWAz+zjdMHYV0arTUrQyp1VtpzUK5kB1BlGzp5DXgWa7hjq23HZCb1X2rsv8AiY8xerGqB3zs89zKwwxizVUTmO0z/GBiPVeFSgEdE5ITtC0Epp9Bqi0op5SJ0yEUppVyEJnSllHIRmtCVUspFaEJXSikXoQldKaVchCZ0pZRyEZrQlVLKRfx/+aMlaF2FvNkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwowKmbvgnlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "a850e939-779f-431f-f81a-122276136460"
      },
      "source": [
        "w_eye = torch.eye(a_kb.size()[0])\n",
        "\n",
        "a_kb_affinity = build_c_byAtten(a_kb_opt, a_kb_opt, w_eye)\n",
        "#print(torch.exp(a_kb_affinity))\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "c_attr_attr = softmax(a_kb_affinity)\n",
        "#print('c_attr_attr ',c_attr_attr, torch.sum(c_attr_attr, dim=1))\n",
        "\n",
        "y_kb = torch.matmul(hp[1].t(), x_train)\n",
        "y_kb[y_kb>=0] = torch.ones_like(y_kb[y_kb>=0])\n",
        "y_kb[y_kb<0] = torch.zeros_like(y_kb[y_kb<0])\n",
        "#print('y_kb ', y_kb.size())\n",
        "\n",
        "w_eye = torch.eye(y_kb.size()[1])\n",
        "y_kb_affinity = build_c_byAtten(y_kb.t(), y_kb.t(), w_eye)\n",
        "c_y_y = softmax(y_kb_affinity)\n",
        "#print('c_y_y ',c_y_y)\n",
        "\n",
        "plt.matshow(utils.toNumpy(c_attr_attr), cmap=plt.cm.Blues)\n",
        "plt.title('Self-attention of knowledge base attribute')\n",
        "plt.show()\n",
        "\n",
        "plt.matshow(utils.toNumpy(c_y_y ), cmap=plt.cm.Blues)\n",
        "plt.title('Self-attention of knowledge base task prediction')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4-iz5yAE9BT",
        "colab_type": "text"
      },
      "source": [
        "#sparsemax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIgvj1_NFCmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analytical_soln_atten(w_kbb, e_item, e_kbb, w_atten):\n",
        "  #find analytical son for one specific novel task\n",
        "  '''\n",
        "  c - row vector \n",
        "  w_kbb - hp (list)\n",
        "  '''\n",
        "  affinity = build_c_byAtten(e_item, e_kbb, w_atten)\n",
        "\n",
        "  sparsemax = Sparsemax(dim=1)\n",
        "  c_newnew = sparsemax(affinity)\n",
        "  \n",
        "  w = torch.matmul(c_newnew, w_kbb.t())\n",
        "\n",
        "  return w.t()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy2l0mrpqNLD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "813a4e91-ba37-4d90-9ec9-c5b87205d38d"
      },
      "source": [
        "hp = [torch.eye(dm, requires_grad=True), w_kb.clone().detach().requires_grad_(True)]\n",
        "opt_hp = torch.optim.Adam(hp, lr=1e-3)\n",
        "a_kb_opt = a_kb.clone().detach().requires_grad_(False)\n",
        "\n",
        "totIter = 2000\n",
        "\n",
        "init_o_loss = outer_loss_class_atten(x_train, y_train, hp[1], a_train, a_kb_opt, hp[0])\n",
        "init_acc = acc_binary_atten(x_test, y_test, hp[1], a_test, a_kb_opt, hp[0])\n",
        "print('init outer loss {}; init acc {}'.format(init_o_loss, init_acc))\n",
        "l_lst = []\n",
        "acc_lst = []\n",
        "for tot in range(totIter):\n",
        " \n",
        "  o_loss =  outer_loss_class_atten(x_train, y_train, hp[1], a_train, a_kb_opt, hp[0])\n",
        "\n",
        "  #x_loss, y_loss, w_kbb, e_item, e_kbb, w_atten\n",
        "  batch_acc = acc_binary_atten(x_test, y_test, hp[1], a_test, a_kb_opt, hp[0])\n",
        "\n",
        "  o_loss = o_loss + 0.00001*torch.pow(torch.norm(hp[1]), 2)\n",
        "\n",
        "  opt_hp.zero_grad()\n",
        "  o_loss.backward()\n",
        "  opt_hp.step()\n",
        "\n",
        "\n",
        "  a_kb_opt, kl_loss = attention_alignment(w_train, hp[1].clone().detach(), a_train, a_kb_opt, hp[0], x_train)\n",
        "  #a = pppp\n",
        "\n",
        "  l_lst.append(utils.toNumpy(o_loss))\n",
        "  acc_lst.append(utils.toNumpy(batch_acc))\n",
        "  \n",
        "  if (tot+1) % 10 == 0:\n",
        "    print('{}/{} o_loss {}; mean test acc {} with mse loss in atten align {}'.format(tot+1, totIter, o_loss, batch_acc, kl_loss))\n",
        "    #print('{}/{} o_loss {}; mean test acc {} '.format(tot+1, totIter, o_loss, batch_acc))\n",
        "\n",
        "  #print('hp ', hp)\n",
        "\n",
        "\n",
        "o_loss = outer_loss_class_atten(x_train, y_train, hp[1], a_train, a_kb_opt, hp[0])\n",
        "batch_acc = acc_binary_atten(x_test, y_test, hp[1], a_test, a_kb_opt, hp[0])\n",
        "print('test set: outer loss {}; batch acc {}'.format(o_loss, batch_acc))\n",
        "\n",
        "\n",
        "plt.plot(l_lst, label='cross entropy loss in training data')\n",
        "plt.plot(acc_lst, label='batch accuracy in testing data')\n",
        "plt.xlabel('Iteration')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init outer loss 0.5575078129768372; init acc 0.7979999780654907\n",
            "10/2000 o_loss 0.536930501461029; mean test acc 0.8050000071525574 with mse loss in atten align 0.01586596481502056\n",
            "20/2000 o_loss 0.5235946178436279; mean test acc 0.7940000295639038 with mse loss in atten align 0.009174272418022156\n",
            "30/2000 o_loss 0.5326286554336548; mean test acc 0.7879999876022339 with mse loss in atten align 0.005260645877569914\n",
            "40/2000 o_loss 0.5468997359275818; mean test acc 0.784000039100647 with mse loss in atten align 0.0032132454216480255\n",
            "50/2000 o_loss 0.5441364645957947; mean test acc 0.7919999957084656 with mse loss in atten align 0.0021438496187329292\n",
            "60/2000 o_loss 0.5214981436729431; mean test acc 0.7989999651908875 with mse loss in atten align 0.0015524397604167461\n",
            "70/2000 o_loss 0.4990038573741913; mean test acc 0.8020000457763672 with mse loss in atten align 0.0012115187710151076\n",
            "80/2000 o_loss 0.4895666539669037; mean test acc 0.8060000538825989 with mse loss in atten align 0.001056440407410264\n",
            "90/2000 o_loss 0.488351434469223; mean test acc 0.7900000214576721 with mse loss in atten align 0.0010203568963333964\n",
            "100/2000 o_loss 0.4822544753551483; mean test acc 0.7960000038146973 with mse loss in atten align 0.0010129809379577637\n",
            "110/2000 o_loss 0.47494399547576904; mean test acc 0.8009999990463257 with mse loss in atten align 0.0010122850071638823\n",
            "120/2000 o_loss 0.4671987295150757; mean test acc 0.8029999732971191 with mse loss in atten align 0.0010122850071638823\n",
            "130/2000 o_loss 0.456773579120636; mean test acc 0.8009999990463257 with mse loss in atten align 0.0009691028972156346\n",
            "140/2000 o_loss 0.4520934820175171; mean test acc 0.8040000200271606 with mse loss in atten align 0.000948491448070854\n",
            "150/2000 o_loss 0.4459504783153534; mean test acc 0.8069999814033508 with mse loss in atten align 0.0009455237304791808\n",
            "160/2000 o_loss 0.4410015642642975; mean test acc 0.8090000152587891 with mse loss in atten align 0.0009982009651139379\n",
            "170/2000 o_loss 0.43783360719680786; mean test acc 0.8130000233650208 with mse loss in atten align 0.0009915250120684505\n",
            "180/2000 o_loss 0.4346311390399933; mean test acc 0.8140000104904175 with mse loss in atten align 0.0009913661051541567\n",
            "190/2000 o_loss 0.4308241605758667; mean test acc 0.8140000104904175 with mse loss in atten align 0.0009622526122257113\n",
            "200/2000 o_loss 0.42868682742118835; mean test acc 0.8130000233650208 with mse loss in atten align 0.0009984442731365561\n",
            "210/2000 o_loss 0.42422688007354736; mean test acc 0.8170000314712524 with mse loss in atten align 0.0009629365522414446\n",
            "220/2000 o_loss 0.4222411811351776; mean test acc 0.8180000185966492 with mse loss in atten align 0.0009952402906492352\n",
            "230/2000 o_loss 0.4184289574623108; mean test acc 0.8189999461174011 with mse loss in atten align 0.000960552366450429\n",
            "240/2000 o_loss 0.41649603843688965; mean test acc 0.8220000267028809 with mse loss in atten align 0.0009956603171303868\n",
            "250/2000 o_loss 0.4099527597427368; mean test acc 0.824999988079071 with mse loss in atten align 0.0009826384484767914\n",
            "260/2000 o_loss 0.41085880994796753; mean test acc 0.8339999914169312 with mse loss in atten align 0.0009890948422253132\n",
            "270/2000 o_loss 0.4078407883644104; mean test acc 0.8300000429153442 with mse loss in atten align 0.000981536810286343\n",
            "280/2000 o_loss 0.40460729598999023; mean test acc 0.8240000009536743 with mse loss in atten align 0.0009800329571589828\n",
            "290/2000 o_loss 0.4017018675804138; mean test acc 0.8240000009536743 with mse loss in atten align 0.0009800329571589828\n",
            "300/2000 o_loss 0.4003538191318512; mean test acc 0.824999988079071 with mse loss in atten align 0.001094448147341609\n",
            "310/2000 o_loss 0.39858633279800415; mean test acc 0.8279999494552612 with mse loss in atten align 0.0010570993181318045\n",
            "320/2000 o_loss 0.3985811173915863; mean test acc 0.8289999961853027 with mse loss in atten align 0.001055734814144671\n",
            "330/2000 o_loss 0.3961276113986969; mean test acc 0.8289999961853027 with mse loss in atten align 0.001055734814144671\n",
            "340/2000 o_loss 0.4044313132762909; mean test acc 0.8319999575614929 with mse loss in atten align 0.0011061178520321846\n",
            "350/2000 o_loss 0.40956661105155945; mean test acc 0.824999988079071 with mse loss in atten align 0.0010899873450398445\n",
            "360/2000 o_loss 0.40760111808776855; mean test acc 0.824999988079071 with mse loss in atten align 0.0010883575305342674\n",
            "370/2000 o_loss 0.4032879173755646; mean test acc 0.8260000348091125 with mse loss in atten align 0.0010883576469495893\n",
            "380/2000 o_loss 0.3995486795902252; mean test acc 0.831000030040741 with mse loss in atten align 0.0010883576469495893\n",
            "390/2000 o_loss 0.3961816728115082; mean test acc 0.831000030040741 with mse loss in atten align 0.0010883575305342674\n",
            "400/2000 o_loss 0.3931208848953247; mean test acc 0.8319999575614929 with mse loss in atten align 0.0010883575305342674\n",
            "410/2000 o_loss 0.3902038037776947; mean test acc 0.8330000042915344 with mse loss in atten align 0.001088357763364911\n",
            "420/2000 o_loss 0.38631007075309753; mean test acc 0.8359999656677246 with mse loss in atten align 0.0011699311435222626\n",
            "430/2000 o_loss 0.3837156891822815; mean test acc 0.8319999575614929 with mse loss in atten align 0.0012296369532123208\n",
            "440/2000 o_loss 0.38166266679763794; mean test acc 0.8330000042915344 with mse loss in atten align 0.001219694851897657\n",
            "450/2000 o_loss 0.3812454044818878; mean test acc 0.8319999575614929 with mse loss in atten align 0.0012118348386138678\n",
            "460/2000 o_loss 0.3804692327976227; mean test acc 0.8289999961853027 with mse loss in atten align 0.0012062950991094112\n",
            "470/2000 o_loss 0.3768824338912964; mean test acc 0.8240000009536743 with mse loss in atten align 0.0012031204532831907\n",
            "480/2000 o_loss 0.3735291361808777; mean test acc 0.8279999494552612 with mse loss in atten align 0.0012026135809719563\n",
            "490/2000 o_loss 0.3706769049167633; mean test acc 0.8289999961853027 with mse loss in atten align 0.0012026135809719563\n",
            "500/2000 o_loss 0.3679811358451843; mean test acc 0.8300000429153442 with mse loss in atten align 0.0012026134645566344\n",
            "510/2000 o_loss 0.3653313219547272; mean test acc 0.8330000042915344 with mse loss in atten align 0.0012026134645566344\n",
            "520/2000 o_loss 0.3627028465270996; mean test acc 0.8350000381469727 with mse loss in atten align 0.0012026135809719563\n",
            "530/2000 o_loss 0.36012130975723267; mean test acc 0.8399999737739563 with mse loss in atten align 0.0012026134645566344\n",
            "540/2000 o_loss 0.35758382081985474; mean test acc 0.840999960899353 with mse loss in atten align 0.0012026135809719563\n",
            "550/2000 o_loss 0.3549031913280487; mean test acc 0.843999981880188 with mse loss in atten align 0.0012026135809719563\n",
            "560/2000 o_loss 0.35224735736846924; mean test acc 0.843999981880188 with mse loss in atten align 0.0012026135809719563\n",
            "570/2000 o_loss 0.3494771420955658; mean test acc 0.847000002861023 with mse loss in atten align 0.0012026134645566344\n",
            "580/2000 o_loss 0.34724870324134827; mean test acc 0.8479999303817749 with mse loss in atten align 0.0014154784148558974\n",
            "590/2000 o_loss 0.3474363386631012; mean test acc 0.8489999771118164 with mse loss in atten align 0.001342528616078198\n",
            "600/2000 o_loss 0.3480304181575775; mean test acc 0.8520000576972961 with mse loss in atten align 0.0013388156658038497\n",
            "610/2000 o_loss 0.3440033197402954; mean test acc 0.8600000143051147 with mse loss in atten align 0.0013388156658038497\n",
            "620/2000 o_loss 0.3406502604484558; mean test acc 0.8609999418258667 with mse loss in atten align 0.0013388157822191715\n",
            "630/2000 o_loss 0.33783745765686035; mean test acc 0.859000027179718 with mse loss in atten align 0.0013388157822191715\n",
            "640/2000 o_loss 0.33559611439704895; mean test acc 0.8560000658035278 with mse loss in atten align 0.0013388157822191715\n",
            "650/2000 o_loss 0.3335779905319214; mean test acc 0.8560000658035278 with mse loss in atten align 0.0013388156658038497\n",
            "660/2000 o_loss 0.33167052268981934; mean test acc 0.8569999933242798 with mse loss in atten align 0.0013388156658038497\n",
            "670/2000 o_loss 0.32984212040901184; mean test acc 0.8569999933242798 with mse loss in atten align 0.0013388157822191715\n",
            "680/2000 o_loss 0.3280748426914215; mean test acc 0.8560000658035278 with mse loss in atten align 0.0013388156658038497\n",
            "690/2000 o_loss 0.3263757824897766; mean test acc 0.8560000658035278 with mse loss in atten align 0.0013388157822191715\n",
            "700/2000 o_loss 0.32473015785217285; mean test acc 0.8550000190734863 with mse loss in atten align 0.0013388156658038497\n",
            "710/2000 o_loss 0.32313472032546997; mean test acc 0.8550000190734863 with mse loss in atten align 0.0013388156658038497\n",
            "720/2000 o_loss 0.3215850293636322; mean test acc 0.8539999723434448 with mse loss in atten align 0.0013388157822191715\n",
            "730/2000 o_loss 0.32007265090942383; mean test acc 0.8520000576972961 with mse loss in atten align 0.0013388157822191715\n",
            "740/2000 o_loss 0.3186129033565521; mean test acc 0.8510000109672546 with mse loss in atten align 0.0013388155493885279\n",
            "750/2000 o_loss 0.31720858812332153; mean test acc 0.8510000109672546 with mse loss in atten align 0.0013388156658038497\n",
            "760/2000 o_loss 0.3158568739891052; mean test acc 0.8510000109672546 with mse loss in atten align 0.0013388156658038497\n",
            "770/2000 o_loss 0.3145504891872406; mean test acc 0.8520000576972961 with mse loss in atten align 0.0013388157822191715\n",
            "780/2000 o_loss 0.31328508257865906; mean test acc 0.8510000109672546 with mse loss in atten align 0.0013388157822191715\n",
            "790/2000 o_loss 0.3120338022708893; mean test acc 0.8520000576972961 with mse loss in atten align 0.0013388156658038497\n",
            "800/2000 o_loss 0.31078237295150757; mean test acc 0.8510000109672546 with mse loss in atten align 0.0013388156658038497\n",
            "810/2000 o_loss 0.30956751108169556; mean test acc 0.8510000109672546 with mse loss in atten align 0.0013388156658038497\n",
            "820/2000 o_loss 0.3083912432193756; mean test acc 0.8500000238418579 with mse loss in atten align 0.0013388156658038497\n",
            "830/2000 o_loss 0.30725395679473877; mean test acc 0.8500000238418579 with mse loss in atten align 0.0013388157822191715\n",
            "840/2000 o_loss 0.30616071820259094; mean test acc 0.8489999771118164 with mse loss in atten align 0.0013388157822191715\n",
            "850/2000 o_loss 0.3050139844417572; mean test acc 0.8510000109672546 with mse loss in atten align 0.0013388156658038497\n",
            "860/2000 o_loss 0.3039150536060333; mean test acc 0.8510000109672546 with mse loss in atten align 0.0013388157822191715\n",
            "870/2000 o_loss 0.302856981754303; mean test acc 0.8510000109672546 with mse loss in atten align 0.0013388156658038497\n",
            "880/2000 o_loss 0.30183905363082886; mean test acc 0.8500000238418579 with mse loss in atten align 0.0013388156658038497\n",
            "890/2000 o_loss 0.30084946751594543; mean test acc 0.8479999303817749 with mse loss in atten align 0.0013388156658038497\n",
            "900/2000 o_loss 0.2998858392238617; mean test acc 0.847000002861023 with mse loss in atten align 0.0013388155493885279\n",
            "910/2000 o_loss 0.29894453287124634; mean test acc 0.847000002861023 with mse loss in atten align 0.0013388157822191715\n",
            "920/2000 o_loss 0.2980225086212158; mean test acc 0.8460000157356262 with mse loss in atten align 0.0013388155493885279\n",
            "930/2000 o_loss 0.2971208989620209; mean test acc 0.843999981880188 with mse loss in atten align 0.0013388156658038497\n",
            "940/2000 o_loss 0.2973759174346924; mean test acc 0.843999981880188 with mse loss in atten align 0.0013935086317360401\n",
            "950/2000 o_loss 0.29981133341789246; mean test acc 0.840999960899353 with mse loss in atten align 0.0013806732604280114\n",
            "960/2000 o_loss 0.29915741086006165; mean test acc 0.8399999737739563 with mse loss in atten align 0.0013805803610011935\n",
            "970/2000 o_loss 0.29778146743774414; mean test acc 0.840999960899353 with mse loss in atten align 0.0013805801281705499\n",
            "980/2000 o_loss 0.2966140806674957; mean test acc 0.8420000076293945 with mse loss in atten align 0.0013805802445858717\n",
            "990/2000 o_loss 0.29559698700904846; mean test acc 0.8420000076293945 with mse loss in atten align 0.0013805803610011935\n",
            "1000/2000 o_loss 0.29468533396720886; mean test acc 0.8420000076293945 with mse loss in atten align 0.0013805802445858717\n",
            "1010/2000 o_loss 0.29383400082588196; mean test acc 0.843999981880188 with mse loss in atten align 0.0013805803610011935\n",
            "1020/2000 o_loss 0.29303309321403503; mean test acc 0.843999981880188 with mse loss in atten align 0.0013805802445858717\n",
            "1030/2000 o_loss 0.29017767310142517; mean test acc 0.8390000462532043 with mse loss in atten align 0.0013408915838226676\n",
            "1040/2000 o_loss 0.28862693905830383; mean test acc 0.8420000076293945 with mse loss in atten align 0.0013388157822191715\n",
            "1050/2000 o_loss 0.28770849108695984; mean test acc 0.8399999737739563 with mse loss in atten align 0.0013388156658038497\n",
            "1060/2000 o_loss 0.2868211269378662; mean test acc 0.843000054359436 with mse loss in atten align 0.0013388157822191715\n",
            "1070/2000 o_loss 0.2860550582408905; mean test acc 0.8449999690055847 with mse loss in atten align 0.0013388156658038497\n",
            "1080/2000 o_loss 0.285351037979126; mean test acc 0.8449999690055847 with mse loss in atten align 0.0013388155493885279\n",
            "1090/2000 o_loss 0.2846589684486389; mean test acc 0.8449999690055847 with mse loss in atten align 0.0013388156658038497\n",
            "1100/2000 o_loss 0.2839820086956024; mean test acc 0.8449999690055847 with mse loss in atten align 0.0013388156658038497\n",
            "1110/2000 o_loss 0.28332948684692383; mean test acc 0.843999981880188 with mse loss in atten align 0.0013388157822191715\n",
            "1120/2000 o_loss 0.2826833426952362; mean test acc 0.843000054359436 with mse loss in atten align 0.0013388156658038497\n",
            "1130/2000 o_loss 0.2820478081703186; mean test acc 0.843999981880188 with mse loss in atten align 0.0013388157822191715\n",
            "1140/2000 o_loss 0.28142276406288147; mean test acc 0.843000054359436 with mse loss in atten align 0.0013388157822191715\n",
            "1150/2000 o_loss 0.28080910444259644; mean test acc 0.8420000076293945 with mse loss in atten align 0.0013388156658038497\n",
            "1160/2000 o_loss 0.2802042067050934; mean test acc 0.8420000076293945 with mse loss in atten align 0.0013388156658038497\n",
            "1170/2000 o_loss 0.280178964138031; mean test acc 0.840999960899353 with mse loss in atten align 0.0014004682889208198\n",
            "1180/2000 o_loss 0.2778960168361664; mean test acc 0.8420000076293945 with mse loss in atten align 0.0013859786558896303\n",
            "1190/2000 o_loss 0.27675703167915344; mean test acc 0.8449999690055847 with mse loss in atten align 0.001446352107450366\n",
            "1200/2000 o_loss 0.28201231360435486; mean test acc 0.8460000157356262 with mse loss in atten align 0.001391810830682516\n",
            "1210/2000 o_loss 0.2843526601791382; mean test acc 0.843999981880188 with mse loss in atten align 0.0013851625844836235\n",
            "1220/2000 o_loss 0.2826497554779053; mean test acc 0.843999981880188 with mse loss in atten align 0.0013850496616214514\n",
            "1230/2000 o_loss 0.2805083990097046; mean test acc 0.843999981880188 with mse loss in atten align 0.0013850496616214514\n",
            "1240/2000 o_loss 0.27887314558029175; mean test acc 0.843000054359436 with mse loss in atten align 0.0013850496616214514\n",
            "1250/2000 o_loss 0.27755698561668396; mean test acc 0.840999960899353 with mse loss in atten align 0.0013850496616214514\n",
            "1260/2000 o_loss 0.27648672461509705; mean test acc 0.840999960899353 with mse loss in atten align 0.0013850496616214514\n",
            "1270/2000 o_loss 0.2755527198314667; mean test acc 0.8390000462532043 with mse loss in atten align 0.0013850496616214514\n",
            "1280/2000 o_loss 0.2747040092945099; mean test acc 0.8420000076293945 with mse loss in atten align 0.0013850497780367732\n",
            "1290/2000 o_loss 0.2739177346229553; mean test acc 0.840999960899353 with mse loss in atten align 0.0013850496616214514\n",
            "1300/2000 o_loss 0.2731899321079254; mean test acc 0.8399999737739563 with mse loss in atten align 0.001385049894452095\n",
            "1310/2000 o_loss 0.2727171778678894; mean test acc 0.840999960899353 with mse loss in atten align 0.0014724410139024258\n",
            "1320/2000 o_loss 0.2729349136352539; mean test acc 0.8319999575614929 with mse loss in atten align 0.0014463926199823618\n",
            "1330/2000 o_loss 0.2709919214248657; mean test acc 0.8370000123977661 with mse loss in atten align 0.001446206821128726\n",
            "1340/2000 o_loss 0.26952648162841797; mean test acc 0.8359999656677246 with mse loss in atten align 0.001446206821128726\n",
            "1350/2000 o_loss 0.26848581433296204; mean test acc 0.8399999737739563 with mse loss in atten align 0.001446206821128726\n",
            "1360/2000 o_loss 0.2675993740558624; mean test acc 0.8379999995231628 with mse loss in atten align 0.001446206821128726\n",
            "1370/2000 o_loss 0.26693034172058105; mean test acc 0.8379999995231628 with mse loss in atten align 0.0014462067047134042\n",
            "1380/2000 o_loss 0.2663118839263916; mean test acc 0.8379999995231628 with mse loss in atten align 0.001446206821128726\n",
            "1390/2000 o_loss 0.26568782329559326; mean test acc 0.8379999995231628 with mse loss in atten align 0.001446206821128726\n",
            "1400/2000 o_loss 0.26875749230384827; mean test acc 0.8420000076293945 with mse loss in atten align 0.0015600050101056695\n",
            "1410/2000 o_loss 0.26670005917549133; mean test acc 0.8449999690055847 with mse loss in atten align 0.0015574139542877674\n",
            "1420/2000 o_loss 0.26526278257369995; mean test acc 0.843000054359436 with mse loss in atten align 0.001557414187118411\n",
            "1430/2000 o_loss 0.2637591063976288; mean test acc 0.843999981880188 with mse loss in atten align 0.001557414187118411\n",
            "1440/2000 o_loss 0.262793630361557; mean test acc 0.8449999690055847 with mse loss in atten align 0.0015574140707030892\n",
            "1450/2000 o_loss 0.2618730366230011; mean test acc 0.847000002861023 with mse loss in atten align 0.0015574140707030892\n",
            "1460/2000 o_loss 0.2609851062297821; mean test acc 0.843999981880188 with mse loss in atten align 0.0015574139542877674\n",
            "1470/2000 o_loss 0.2601120173931122; mean test acc 0.8460000157356262 with mse loss in atten align 0.0015574139542877674\n",
            "1480/2000 o_loss 0.2593134045600891; mean test acc 0.8479999303817749 with mse loss in atten align 0.0015574139542877674\n",
            "1490/2000 o_loss 0.25853094458580017; mean test acc 0.8489999771118164 with mse loss in atten align 0.001557414187118411\n",
            "1500/2000 o_loss 0.25776931643486023; mean test acc 0.8489999771118164 with mse loss in atten align 0.0015574140707030892\n",
            "1510/2000 o_loss 0.25701940059661865; mean test acc 0.8489999771118164 with mse loss in atten align 0.0015574140707030892\n",
            "1520/2000 o_loss 0.25627315044403076; mean test acc 0.8489999771118164 with mse loss in atten align 0.001557414187118411\n",
            "1530/2000 o_loss 0.2555314004421234; mean test acc 0.8479999303817749 with mse loss in atten align 0.0015574140707030892\n",
            "1540/2000 o_loss 0.2548043727874756; mean test acc 0.847000002861023 with mse loss in atten align 0.0015574140707030892\n",
            "1550/2000 o_loss 0.2540968358516693; mean test acc 0.847000002861023 with mse loss in atten align 0.001557414187118411\n",
            "1560/2000 o_loss 0.2534032464027405; mean test acc 0.847000002861023 with mse loss in atten align 0.001557414187118411\n",
            "1570/2000 o_loss 0.2527236044406891; mean test acc 0.8479999303817749 with mse loss in atten align 0.001557414187118411\n",
            "1580/2000 o_loss 0.25205743312835693; mean test acc 0.8479999303817749 with mse loss in atten align 0.0015574140707030892\n",
            "1590/2000 o_loss 0.25140321254730225; mean test acc 0.8479999303817749 with mse loss in atten align 0.0015574140707030892\n",
            "1600/2000 o_loss 0.25076061487197876; mean test acc 0.8479999303817749 with mse loss in atten align 0.001548417960293591\n",
            "1610/2000 o_loss 0.2528882324695587; mean test acc 0.847000002861023 with mse loss in atten align 0.001501727383583784\n",
            "1620/2000 o_loss 0.25800713896751404; mean test acc 0.8489999771118164 with mse loss in atten align 0.0014941333793103695\n",
            "1630/2000 o_loss 0.25982654094696045; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1640/2000 o_loss 0.25725144147872925; mean test acc 0.8449999690055847 with mse loss in atten align 0.0014936671359464526\n",
            "1650/2000 o_loss 0.25569814443588257; mean test acc 0.8420000076293945 with mse loss in atten align 0.0014936671359464526\n",
            "1660/2000 o_loss 0.2547157108783722; mean test acc 0.8420000076293945 with mse loss in atten align 0.0014936671359464526\n",
            "1670/2000 o_loss 0.2537631392478943; mean test acc 0.840999960899353 with mse loss in atten align 0.001493666903115809\n",
            "1680/2000 o_loss 0.2528547942638397; mean test acc 0.843000054359436 with mse loss in atten align 0.0014936671359464526\n",
            "1690/2000 o_loss 0.25201651453971863; mean test acc 0.847000002861023 with mse loss in atten align 0.0014936671359464526\n",
            "1700/2000 o_loss 0.25122857093811035; mean test acc 0.8479999303817749 with mse loss in atten align 0.001493666903115809\n",
            "1710/2000 o_loss 0.25047630071640015; mean test acc 0.847000002861023 with mse loss in atten align 0.001493666903115809\n",
            "1720/2000 o_loss 0.2496650218963623; mean test acc 0.843999981880188 with mse loss in atten align 0.001493666903115809\n",
            "1730/2000 o_loss 0.2489023059606552; mean test acc 0.843999981880188 with mse loss in atten align 0.0014936671359464526\n",
            "1740/2000 o_loss 0.24815122783184052; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1750/2000 o_loss 0.24743087589740753; mean test acc 0.8460000157356262 with mse loss in atten align 0.001493666903115809\n",
            "1760/2000 o_loss 0.2467348277568817; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1770/2000 o_loss 0.24606136977672577; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1780/2000 o_loss 0.24540632963180542; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1790/2000 o_loss 0.24476641416549683; mean test acc 0.847000002861023 with mse loss in atten align 0.0014936671359464526\n",
            "1800/2000 o_loss 0.24414247274398804; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1810/2000 o_loss 0.2435167282819748; mean test acc 0.8449999690055847 with mse loss in atten align 0.0014936671359464526\n",
            "1820/2000 o_loss 0.24287673830986023; mean test acc 0.8449999690055847 with mse loss in atten align 0.0014936671359464526\n",
            "1830/2000 o_loss 0.2422724962234497; mean test acc 0.847000002861023 with mse loss in atten align 0.0014936671359464526\n",
            "1840/2000 o_loss 0.24168118834495544; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1850/2000 o_loss 0.24110369384288788; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1860/2000 o_loss 0.24054142832756042; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1870/2000 o_loss 0.23998557031154633; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1880/2000 o_loss 0.23943905532360077; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014936671359464526\n",
            "1890/2000 o_loss 0.23889994621276855; mean test acc 0.847000002861023 with mse loss in atten align 0.0014936671359464526\n",
            "1900/2000 o_loss 0.23825663328170776; mean test acc 0.847000002861023 with mse loss in atten align 0.0014936671359464526\n",
            "1910/2000 o_loss 0.23768866062164307; mean test acc 0.8479999303817749 with mse loss in atten align 0.0014936671359464526\n",
            "1920/2000 o_loss 0.23713047802448273; mean test acc 0.8489999771118164 with mse loss in atten align 0.0014936671359464526\n",
            "1930/2000 o_loss 0.23659461736679077; mean test acc 0.8500000238418579 with mse loss in atten align 0.0014936671359464526\n",
            "1940/2000 o_loss 0.23607198894023895; mean test acc 0.8500000238418579 with mse loss in atten align 0.0014936671359464526\n",
            "1950/2000 o_loss 0.23556174337863922; mean test acc 0.8500000238418579 with mse loss in atten align 0.0014936671359464526\n",
            "1960/2000 o_loss 0.23504698276519775; mean test acc 0.8479999303817749 with mse loss in atten align 0.0014936671359464526\n",
            "1970/2000 o_loss 0.2454385757446289; mean test acc 0.8390000462532043 with mse loss in atten align 0.001419872511178255\n",
            "1980/2000 o_loss 0.24270333349704742; mean test acc 0.8460000157356262 with mse loss in atten align 0.0014174104435369372\n",
            "1990/2000 o_loss 0.2399076223373413; mean test acc 0.843999981880188 with mse loss in atten align 0.0014174104435369372\n",
            "2000/2000 o_loss 0.2382323294878006; mean test acc 0.8449999690055847 with mse loss in atten align 0.0014174104435369372\n",
            "test set: outer loss 0.23641984164714813; batch acc 0.8449999690055847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg_kmkdXwdhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "8d2e2e92-190c-49c1-a893-a7edb8c7999d"
      },
      "source": [
        "w_eye = torch.eye(a_kb.size()[0])\n",
        "a_kb_affinity = build_c_byAtten(a_kb_opt, a_kb_opt, w_eye)\n",
        "#print(torch.exp(a_kb_affinity))\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "c_attr_attr = softmax(a_kb_affinity)\n",
        "#print('c_attr_attr ',c_attr_attr, torch.sum(c_attr_attr, dim=1))\n",
        "\n",
        "y_kb = torch.matmul(hp[1].t(), x_train)\n",
        "y_kb[y_kb>=0] = torch.ones_like(y_kb[y_kb>=0])\n",
        "y_kb[y_kb<0] = torch.zeros_like(y_kb[y_kb<0])\n",
        "#print('y_kb ', y_kb.size())\n",
        "\n",
        "w_eye = torch.eye(y_kb.size()[1])\n",
        "y_kb_affinity = build_c_byAtten(y_kb.t(), y_kb.t(), w_eye)\n",
        "c_y_y = softmax(y_kb_affinity)\n",
        "#print('c_y_y ',c_y_y)\n",
        "\n",
        "plt.matshow(utils.toNumpy(c_attr_attr), cmap=plt.cm.Blues)\n",
        "plt.title('Self-attention of knowledge base attribute')\n",
        "plt.show()\n",
        "\n",
        "plt.matshow(utils.toNumpy(c_y_y ), cmap=plt.cm.Blues)\n",
        "plt.title('Self-attention of knowledge base task prediction')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNLhQSlgajFY",
        "colab_type": "text"
      },
      "source": [
        "#double check accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziGaA0_feTbj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "00e1a7ae-ebff-41d6-f05e-e4ee8d51cb5e"
      },
      "source": [
        "#x_test, y_test, w_kb_opt, a_test, a_kb, hp\n",
        "\n",
        "w_pred = analytical_soln_atten(hp[1], a_test, a_kb, hp[0])\n",
        "#print(w_pred.t().size(), x_loss.size())\n",
        "logit = torch.matmul(w_pred.t(), x_test)\n",
        "pred = torch.sigmoid(logit)\n",
        "pred[pred>=0.5] = torch.ones_like(pred[pred>=0.5])\n",
        "pred[pred<0.5] = torch.zeros_like(pred[pred<0.5])\n",
        "\n",
        "compare = (pred == y_test)\n",
        "#print('compare ', compare.size(), compare)\n",
        "compare = torch.sum(compare, dim=1)\n",
        "#print('compare ', compare.size(), compare)\n",
        "#print(y_loss.size()[1])\n",
        "acc = torch.mean(compare.float())\n",
        "#print('acc ', acc)\n",
        "mean_acc = acc/y_test.size()[1]\n",
        "print(mean_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7600)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa8s-aOrawUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db881bf7-11b7-4042-962e-ab76f347b993"
      },
      "source": [
        "indx = 1\n",
        "print(pred[indx,:])\n",
        "print(y_test[indx,:])\n",
        "print(torch.sum(pred[indx,:] == y_test[indx,:]), pred[indx,:] == y_test[indx,:])\n",
        "for i in range(pred.size()[0]):\n",
        "  print(torch.sum(pred[i,:] == y_test[i,:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 1., 0., 0., 1., 1., 1., 0., 1.], grad_fn=<SliceBackward>)\n",
            "tensor([0., 1., 1., 1., 0., 1., 1., 1., 1., 1.])\n",
            "tensor(8) tensor([ True,  True,  True, False,  True,  True,  True,  True, False,  True])\n",
            "tensor(7)\n",
            "tensor(8)\n",
            "tensor(9)\n",
            "tensor(6)\n",
            "tensor(9)\n",
            "tensor(7)\n",
            "tensor(8)\n",
            "tensor(7)\n",
            "tensor(7)\n",
            "tensor(9)\n",
            "tensor(7)\n",
            "tensor(7)\n",
            "tensor(8)\n",
            "tensor(8)\n",
            "tensor(9)\n",
            "tensor(8)\n",
            "tensor(9)\n",
            "tensor(8)\n",
            "tensor(10)\n",
            "tensor(7)\n",
            "tensor(5)\n",
            "tensor(7)\n",
            "tensor(8)\n",
            "tensor(8)\n",
            "tensor(7)\n",
            "tensor(6)\n",
            "tensor(9)\n",
            "tensor(7)\n",
            "tensor(7)\n",
            "tensor(9)\n",
            "tensor(10)\n",
            "tensor(7)\n",
            "tensor(4)\n",
            "tensor(6)\n",
            "tensor(6)\n",
            "tensor(9)\n",
            "tensor(6)\n",
            "tensor(10)\n",
            "tensor(8)\n",
            "tensor(8)\n",
            "tensor(9)\n",
            "tensor(9)\n",
            "tensor(7)\n",
            "tensor(8)\n",
            "tensor(4)\n",
            "tensor(9)\n",
            "tensor(6)\n",
            "tensor(5)\n",
            "tensor(7)\n",
            "tensor(9)\n",
            "tensor(6)\n",
            "tensor(9)\n",
            "tensor(8)\n",
            "tensor(5)\n",
            "tensor(6)\n",
            "tensor(9)\n",
            "tensor(7)\n",
            "tensor(10)\n",
            "tensor(7)\n",
            "tensor(7)\n",
            "tensor(8)\n",
            "tensor(9)\n",
            "tensor(8)\n",
            "tensor(6)\n",
            "tensor(9)\n",
            "tensor(6)\n",
            "tensor(5)\n",
            "tensor(6)\n",
            "tensor(9)\n",
            "tensor(10)\n",
            "tensor(7)\n",
            "tensor(7)\n",
            "tensor(8)\n",
            "tensor(6)\n",
            "tensor(8)\n",
            "tensor(9)\n",
            "tensor(10)\n",
            "tensor(10)\n",
            "tensor(10)\n",
            "tensor(9)\n",
            "tensor(10)\n",
            "tensor(7)\n",
            "tensor(5)\n",
            "tensor(8)\n",
            "tensor(5)\n",
            "tensor(7)\n",
            "tensor(8)\n",
            "tensor(8)\n",
            "tensor(6)\n",
            "tensor(7)\n",
            "tensor(8)\n",
            "tensor(7)\n",
            "tensor(9)\n",
            "tensor(10)\n",
            "tensor(9)\n",
            "tensor(6)\n",
            "tensor(6)\n",
            "tensor(7)\n",
            "tensor(7)\n",
            "tensor(8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ujbM3MROIvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}