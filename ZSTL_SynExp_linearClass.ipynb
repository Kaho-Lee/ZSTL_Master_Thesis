{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597337068784",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import itertools\n",
    "import tqdm\n",
    "# import hypergrad as hg\n",
    "from mlmodel import *\n",
    "import utils\n",
    "import numpy as np\n",
    "from sparsemax import Sparsemax\n",
    "from argparse import ArgumentParser\n",
    "from ZSTL_model import ZSTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = FuncRecursiveNet([\n",
    "    FLinearLayer(1, False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "w_train  (100, 2049)\nw_test  (100, 2049)\nw_kb  (10, 2049)\na_train  (100, 85)\na_test  (100, 85)\na_kb  (10, 85)\nx_kb  (10, 2049)\nx_train  (10, 2049)\nx_test  (10, 2049)\ny_train  (100, 10) [ 5.  8.  6.  7.  6.  6.  6.  7.  8.  8.  8.  8.  7.  7.  5.  9.  7.  7.\n  7.  6.  4.  7.  6.  6. 10.  4.  8.  6.  6.  5.  4.  7.  7.  8.  6.  7.\n  7.  6.  6.  7.  6.  7.  7.  8.  8.  7.  7.  6.  7.  6.  4.  8.  7.  7.\n  6.  5.  5.  5.  5.  8.  7.  6.  7.  6.  5.  5.  9.  7.  7.  8.  6.  6.\n  7.  5.  8.  7.  7.  7.  8.  7.  6.  6.  7.  7.  8.  7.  6.  8.  5.  7.\n  7.  6.  7.  7.  7.  6.  6.  5.  6.  7.]\ny_test  (100, 10)\ny_kb  (10, 10)\n"
    }
   ],
   "source": [
    "d = 8\n",
    "dm = 5\n",
    "T = 100\n",
    "n = 10\n",
    "dict_k = 6\n",
    "shape_record = {}\n",
    "shape_record[0] = [(1,d)]\n",
    "\n",
    "np.random.seed(666)\n",
    "L_gt = np.random.randn(d,dict_k)\n",
    "\n",
    "D_gt = np.random.randn(dm,dict_k)\n",
    "\n",
    "s_train = np.random.uniform(0, 2, size=(dict_k, T))\n",
    "s_train[s_train>=1] = 0\n",
    "#print(s_train)\n",
    "\n",
    "s_kb = np.random.uniform(0, 2, size=(dict_k, 10))\n",
    "s_kb[s_kb>=1] = 0\n",
    "#print(s_val)\n",
    "s_test = np.random.uniform(0, 2, size=(dict_k, T))\n",
    "s_test[s_test>=1] = 0\n",
    "#print(s_test)\n",
    "\n",
    "w_train = L_gt @ s_train\n",
    "w_kb = L_gt @ s_kb\n",
    "w_test = L_gt @ s_test\n",
    "\n",
    "a_train = D_gt @ s_train\n",
    "a_kb = D_gt @ s_kb\n",
    "a_test = D_gt @ s_test\n",
    "\n",
    "\n",
    "\n",
    "ones = np.atleast_2d(np.ones(n))\n",
    "\n",
    "x_kb = np.random.normal(0, 1.0, size=(d-1,n))\n",
    "x_kb = np.vstack((ones, x_kb))\n",
    "\n",
    "x_train = np.random.normal(0, 1.0, size=(d-1,n))\n",
    "x_train = np.vstack((ones, x_train))\n",
    "\n",
    "\n",
    "x_test = np.random.normal(0, 1.0, size=(d-1,n))\n",
    "x_test = np.vstack((ones, x_test))\n",
    "\n",
    "y_train = utils.sigmoid(w_train.T @ x_train)\n",
    "y_train[y_train >=0.5] = 1\n",
    "y_train[y_train <0.5] = 0\n",
    "\n",
    "y_test = utils.sigmoid(w_test.T @x_test)\n",
    "y_test[y_test >=0.5] = 1\n",
    "y_test[y_test <0.5] = 0\n",
    "\n",
    "\n",
    "y_kb = utils.sigmoid(w_kb.T @ x_kb)\n",
    "y_kb[y_kb >=0.5] = 1\n",
    "y_kb[y_kb <0.5] = 0\n",
    "\n",
    "w_train = w_train.T\n",
    "w_test = w_test.T\n",
    "w_kb = w_kb.T\n",
    "a_train = a_train.T\n",
    "a_test = a_test.T\n",
    "a_kb = a_kb.T\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "x_kb = x_kb.T\n",
    "\n",
    "print('w_train ', w_train.shape)\n",
    "print('w_test ', w_test.shape)\n",
    "print('w_kb ', w_kb.shape)\n",
    "print('a_train ', a_train.shape)\n",
    "print('a_test ', a_test.shape)\n",
    "print('a_kb ', a_kb.shape)\n",
    "print('x_kb ', x_kb.shape)\n",
    "print('x_train ', x_train.shape)\n",
    "print('x_test ', x_test.shape)\n",
    "print('y_train ', y_train.shape, np.sum(y_train, axis=1))\n",
    "print('y_test ', y_test.shape)\n",
    "print('y_kb ', y_kb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_data = utils.Dataset([(a_kb[t,:], [w_kb[t,:]], x_kb, np.expand_dims(y_kb[t, :], axis=2)) for t in range(a_kb.shape[0])])\n",
    "\n",
    "train_data = utils.Dataset([(a_train[t,:], [w_train[t,:]], x_train, np.expand_dims(y_train[t, :], axis=2)) for t in range(a_train.shape[0])])\n",
    "\n",
    "test_data = utils.Dataset([(a_test[t,:], [w_test[t,:]], x_test, np.expand_dims(y_test[t, :], axis=2)) for t in range(a_test.shape[0])])\n",
    "\n",
    "kb_loader = DataLoader(kb_data, batch_size=a_kb.shape[0], shuffle=False)\n",
    "train_loader = DataLoader(train_data, batch_size=a_train.shape[0], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=a_test.shape[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([10, 1, 85]) torch.Size([10, 1, 2049]) torch.Size([10, 10, 2049]) torch.Size([10, 10, 1])\n"
    }
   ],
   "source": [
    "support_a, support_w, support_x, support_y = next(iter(kb_loader))\n",
    "support_a, support_w, support_x, support_y = support_a.float(), support_w.float(), support_x.float(), support_y.float()\n",
    "print(support_a.size(), support_w.size(), support_x.size(), support_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "85\n"
    }
   ],
   "source": [
    "support_a, support_w, support_x, support_y = next(iter(kb_loader))\n",
    "support_a, support_w, support_x, support_y = support_a.float(), support_w.float(), support_x.float(), support_y.float()\n",
    "support_a = support_a.squeeze().t()\n",
    "support_w = support_w.squeeze().t()\n",
    "dm = support_a.size()[0]\n",
    "print(dm)\n",
    "\n",
    "param_dict = {}\n",
    "param_dict['rho'] = 0.01\n",
    "param_dict['mu'] = 0.01\n",
    "param_dict['loss'] = 'binary class'\n",
    "param_dict['outer lr'] = 1e-4\n",
    "param_dict['align lr'] = 1e-4\n",
    "param_dict['dm'] = dm\n",
    "param_dict['d'] = d\n",
    "param_dict['model_shape'] = shape_record\n",
    "param_dict['atten_activation'] = 'Sparsemax'\n",
    "\n",
    "\n",
    "ZSTL_model = ZSTL(support_w, support_a, support_x, net, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "init mean test metric 0.745;\n1/2000 o_loss 6.485660483633518; mean test loss 0.743 with mse loss in atten align 0.09784694761037827\n10/2000 o_loss 6.119658792700269; mean test loss 0.7469999999999999 with mse loss in atten align 0.06670576333999634\n20/2000 o_loss 5.972738562315936; mean test loss 0.75 with mse loss in atten align 0.04486630856990814\n30/2000 o_loss 5.831549898583908; mean test loss 0.7509999999999999 with mse loss in atten align 0.028592895716428757\n40/2000 o_loss 5.672798697296137; mean test loss 0.7440000000000002 with mse loss in atten align 0.01727953739464283\n50/2000 o_loss 5.381901179417037; mean test loss 0.7300000000000001 with mse loss in atten align 0.010422677733004093\n60/2000 o_loss 4.598868195444957; mean test loss 0.7410000000000001 with mse loss in atten align 0.006303907837718725\n70/2000 o_loss 3.985861252806277; mean test loss 0.7390000000000003 with mse loss in atten align 0.0038475533947348595\n80/2000 o_loss 3.794411244437245; mean test loss 0.7610000000000002 with mse loss in atten align 0.0023994185030460358\n90/2000 o_loss 3.7486473969435177; mean test loss 0.7680000000000003 with mse loss in atten align 0.0016147695714607835\n100/2000 o_loss 3.4243104934257556; mean test loss 0.7310000000000001 with mse loss in atten align 0.001260800869204104\n110/2000 o_loss 3.5556037698522185; mean test loss 0.703 with mse loss in atten align 0.001126130810007453\n120/2000 o_loss 3.542070442953475; mean test loss 0.6880000000000002 with mse loss in atten align 0.0011555184610188007\n130/2000 o_loss 3.413409382078763; mean test loss 0.6820000000000002 with mse loss in atten align 0.0011796124745160341\n140/2000 o_loss 3.212342109226916; mean test loss 0.6709999999999999 with mse loss in atten align 0.0011891849571838975\n150/2000 o_loss 3.093333067742818; mean test loss 0.6739999999999999 with mse loss in atten align 0.0012128140078857541\n160/2000 o_loss 2.9977545483452266; mean test loss 0.6739999999999999 with mse loss in atten align 0.0012408099137246609\n170/2000 o_loss 2.925524822284952; mean test loss 0.6820000000000002 with mse loss in atten align 0.0012553420383483171\n180/2000 o_loss 2.771937387629734; mean test loss 0.6830000000000002 with mse loss in atten align 0.0012752378825098276\n190/2000 o_loss 2.66042483255113; mean test loss 0.6829999999999999 with mse loss in atten align 0.0013075360329821706\n200/2000 o_loss 2.5335904537402705; mean test loss 0.6750000000000002 with mse loss in atten align 0.001335725886747241\n210/2000 o_loss 2.4475155008003213; mean test loss 0.6780000000000002 with mse loss in atten align 0.0013578619109466672\n220/2000 o_loss 2.368889029027815; mean test loss 0.6779999999999999 with mse loss in atten align 0.0013727969489991665\n230/2000 o_loss 2.2916284576333554; mean test loss 0.677 with mse loss in atten align 0.0013697221875190735\n240/2000 o_loss 2.2385425988463634; mean test loss 0.6810000000000003 with mse loss in atten align 0.0013388888910412788\n250/2000 o_loss 2.191566298091078; mean test loss 0.6779999999999999 with mse loss in atten align 0.00128170195966959\n260/2000 o_loss 2.191776166805696; mean test loss 0.682 with mse loss in atten align 0.0012173228897154331\n270/2000 o_loss 2.2029129975908583; mean test loss 0.6809999999999999 with mse loss in atten align 0.0011678285663947463\n280/2000 o_loss 2.170162805353507; mean test loss 0.6759999999999999 with mse loss in atten align 0.0011387673439458013\n290/2000 o_loss 2.1174770372102647; mean test loss 0.6749999999999998 with mse loss in atten align 0.0011415911139920354\n300/2000 o_loss 2.1165764109407927; mean test loss 0.6789999999999999 with mse loss in atten align 0.001165221445262432\n310/2000 o_loss 2.1589479106882843; mean test loss 0.684 with mse loss in atten align 0.0011861444218084216\n320/2000 o_loss 2.191452027923897; mean test loss 0.6859999999999999 with mse loss in atten align 0.001197831821627915\n330/2000 o_loss 2.1575262009218568; mean test loss 0.687 with mse loss in atten align 0.001199611579068005\n340/2000 o_loss 2.0864053119806885; mean test loss 0.6799999999999998 with mse loss in atten align 0.0011977142421528697\n350/2000 o_loss 2.054618785666953; mean test loss 0.6829999999999998 with mse loss in atten align 0.0011987551115453243\n360/2000 o_loss 2.0343122956401567; mean test loss 0.6839999999999997 with mse loss in atten align 0.0012003022711724043\n370/2000 o_loss 2.0226349207557357; mean test loss 0.6869999999999998 with mse loss in atten align 0.0012008149642497301\n380/2000 o_loss 2.0114557581347983; mean test loss 0.6869999999999998 with mse loss in atten align 0.0012010892387479544\n390/2000 o_loss 1.9884854114780954; mean test loss 0.6869999999999998 with mse loss in atten align 0.001200828468427062\n400/2000 o_loss 1.9825691958329572; mean test loss 0.687 with mse loss in atten align 0.0012007745681330562\n410/2000 o_loss 1.9610776438334596; mean test loss 0.682 with mse loss in atten align 0.0012037280248478055\n420/2000 o_loss 1.9452669103462994; mean test loss 0.6829999999999999 with mse loss in atten align 0.0012138535967096686\n430/2000 o_loss 1.9092842076848047; mean test loss 0.6839999999999999 with mse loss in atten align 0.0012314890045672655\n440/2000 o_loss 1.891333365987116; mean test loss 0.6859999999999999 with mse loss in atten align 0.001248414278961718\n450/2000 o_loss 1.880485078943202; mean test loss 0.684 with mse loss in atten align 0.0012453196104615927\n460/2000 o_loss 1.859450868174772; mean test loss 0.681 with mse loss in atten align 0.0012375867227092385\n470/2000 o_loss 1.826862415267592; mean test loss 0.6829999999999999 with mse loss in atten align 0.0012463793391361833\n480/2000 o_loss 1.797112593164734; mean test loss 0.6819999999999999 with mse loss in atten align 0.0012525516794994473\n490/2000 o_loss 1.7774769556548837; mean test loss 0.6869999999999998 with mse loss in atten align 0.0012529625091701746\n500/2000 o_loss 1.7653607200967054; mean test loss 0.6920000000000001 with mse loss in atten align 0.0012503360630944371\n510/2000 o_loss 1.7546461063025072; mean test loss 0.693 with mse loss in atten align 0.0012475070543587208\n520/2000 o_loss 1.7464099348452964; mean test loss 0.69 with mse loss in atten align 0.0012452759547159076\n530/2000 o_loss 1.7410124852960098; mean test loss 0.69 with mse loss in atten align 0.0012434314703568816\n540/2000 o_loss 1.7362732166010846; mean test loss 0.6859999999999999 with mse loss in atten align 0.0012416348326951265\n550/2000 o_loss 1.7326440861667924; mean test loss 0.6859999999999999 with mse loss in atten align 0.0012386755552142859\n560/2000 o_loss 1.7301944913648821; mean test loss 0.6859999999999999 with mse loss in atten align 0.0012341130059212446\n570/2000 o_loss 1.7284605069864756; mean test loss 0.6859999999999999 with mse loss in atten align 0.001229132292792201\n580/2000 o_loss 1.7267580438807544; mean test loss 0.6879999999999998 with mse loss in atten align 0.0012257762718945742\n590/2000 o_loss 1.723407232756963; mean test loss 0.6889999999999997 with mse loss in atten align 0.001225578016601503\n600/2000 o_loss 1.7195942383755392; mean test loss 0.6909999999999998 with mse loss in atten align 0.0012285774573683739\n610/2000 o_loss 1.7156068602063896; mean test loss 0.689 with mse loss in atten align 0.0012315583880990744\n620/2000 o_loss 1.7093515650133486; mean test loss 0.689 with mse loss in atten align 0.0012321766698732972\n630/2000 o_loss 1.7016678102420337; mean test loss 0.6869999999999998 with mse loss in atten align 0.001230563153512776\n640/2000 o_loss 1.6949723301591804; mean test loss 0.6859999999999999 with mse loss in atten align 0.0012278860667720437\n650/2000 o_loss 1.689991869142723; mean test loss 0.685 with mse loss in atten align 0.0012254483299329877\n660/2000 o_loss 1.6864266743951521; mean test loss 0.6859999999999999 with mse loss in atten align 0.001223876839503646\n670/2000 o_loss 1.6833460830253353; mean test loss 0.687 with mse loss in atten align 0.0012226474937051535\n680/2000 o_loss 1.680253962400202; mean test loss 0.687 with mse loss in atten align 0.0012208494590595365\n690/2000 o_loss 1.6771337140542892; mean test loss 0.687 with mse loss in atten align 0.0012181783095002174\n700/2000 o_loss 1.6742270504408951; mean test loss 0.688 with mse loss in atten align 0.0012148607056587934\n710/2000 o_loss 1.6720804669352218; mean test loss 0.6890000000000001 with mse loss in atten align 0.0012109058443456888\n720/2000 o_loss 1.671595057011582; mean test loss 0.6859999999999999 with mse loss in atten align 0.001205519656650722\n730/2000 o_loss 1.6720135198364006; mean test loss 0.6859999999999998 with mse loss in atten align 0.0011982759460806847\n740/2000 o_loss 1.657975519429965; mean test loss 0.6859999999999999 with mse loss in atten align 0.0011912374757230282\n750/2000 o_loss 1.6479583244175773; mean test loss 0.685 with mse loss in atten align 0.0011864391854032874\n760/2000 o_loss 1.6362070105687962; mean test loss 0.682 with mse loss in atten align 0.0011830878211185336\n770/2000 o_loss 1.6298055796161135; mean test loss 0.6819999999999999 with mse loss in atten align 0.0011815463658422232\n780/2000 o_loss 1.6248738084513024; mean test loss 0.6819999999999999 with mse loss in atten align 0.0011816093465313315\n790/2000 o_loss 1.620132939672949; mean test loss 0.6819999999999999 with mse loss in atten align 0.0011824187822639942\n800/2000 o_loss 1.6169353290816617; mean test loss 0.682 with mse loss in atten align 0.00118471949826926\n810/2000 o_loss 1.6193153689826068; mean test loss 0.677 with mse loss in atten align 0.0011870816815644503\n820/2000 o_loss 1.624284944202136; mean test loss 0.68 with mse loss in atten align 0.0011899825185537338\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9e9f18953c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mZSTL_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thesis_code/ZSTL_Master_Thesis/ZSTL_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, test_loader, max_iter)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mtest_l_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mtrain_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mtrain_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mtrain_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thesis_code/ZSTL_Master_Thesis/utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ZSTL_model.train(train_loader, test_loader, max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}