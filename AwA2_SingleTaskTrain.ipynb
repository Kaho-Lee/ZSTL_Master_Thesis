{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597328855566",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import timeit\n",
    "from mlmodel import *\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_feature = '../ZSTL_Data/Animals_with_Attributes2/Features/ResNet101/AwA2-features.txt'\n",
    "path_labels = '../ZSTL_Data/Animals_with_Attributes2/Features/ResNet101/AwA2-labels.txt'\n",
    "path_attributes = '../ZSTL_Data/Animals_with_Attributes2/predicate-matrix-continuous.txt'\n",
    "path_destination = '../ZSTL_Data/Animals_with_Attributes2/splitedTask/'\n",
    "data_feature = pd.read_csv(path_feature, sep=\" \", header=None)\n",
    "data_labels = pd.read_csv(path_labels, sep=\" \", header=None)\n",
    "data_labels.columns = [\"label\"]\n",
    "\n",
    "num_task = 50\n",
    "num_data = 100\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "label         0         1         2         3         4         5  \\\n0          1  0.127028  3.236108  0.935148  0.144205  1.114897  1.502288   \n1          1  0.000000  2.466911  0.026454  0.075211  1.159094  3.066645   \n2          1  0.383341  1.011904  0.000000  0.054472  0.343532  0.917366   \n3          1  0.117190  1.225786  0.001932  0.000000  3.135732  0.061605   \n4          1  0.274902  0.337544  0.084937  0.000000  1.788061  0.143165   \n...      ...       ...       ...       ...       ...       ...       ...   \n37317     38  0.498370  1.883775  0.000000  0.212900  0.195262  0.201184   \n37318     38  0.043884  0.309244  0.012275  0.173839  0.893198  0.183430   \n37319     38  0.016755  1.105690  0.103399  0.384196  0.469869  0.512281   \n37320     38  0.121401  1.050093  0.006921  0.545237  0.884461  0.738782   \n37321     38  0.243461  1.255479  0.048471  0.660457  0.147349  1.431781   \n\n              6         7         8  ...      2038      2039      2040  \\\n0      0.410044  0.053410  0.000000  ...  0.000000  0.008841  0.059838   \n1      0.191157  0.049365  0.017394  ...  0.000000  0.312142  0.447039   \n2      0.044860  0.100728  0.266673  ...  0.000000  0.112120  0.000000   \n3      0.974178  0.463193  0.057579  ...  0.000000  0.168017  0.460633   \n4      0.250313  0.000000  0.134671  ...  0.000000  0.106673  0.040709   \n...         ...       ...       ...  ...       ...       ...       ...   \n37317  1.139161  0.072897  0.040727  ...  0.000000  0.000000  1.554183   \n37318  0.092317  0.000000  0.589606  ...  0.111526  0.000000  0.989355   \n37319  0.173142  0.197311  0.164240  ...  0.000000  0.024520  0.987232   \n37320  0.308057  0.086900  0.256168  ...  0.013474  0.001264  0.677624   \n37321  0.139688  0.174798  0.250953  ...  0.034443  0.319134  2.102535   \n\n           2041      2042      2043      2044      2045      2046      2047  \n0      0.018889  0.024724  0.256931  0.071584  0.672237  0.032265  0.407616  \n1      0.212022  0.000000  0.226731  0.278318  1.125202  0.006843  0.256230  \n2      0.000000  1.184899  0.000000  0.000000  2.252654  0.029523  0.033138  \n3      0.000000  0.008595  0.001324  0.000000  0.725105  0.101483  0.000000  \n4      0.000000  0.000000  0.010016  0.010241  0.755545  0.000000  0.060499  \n...         ...       ...       ...       ...       ...       ...       ...  \n37317  0.946372  0.146409  0.412591  0.250193  0.013291  0.018395  0.040261  \n37318  0.306995  0.000000  0.728876  0.330187  0.400795  0.032162  0.272096  \n37319  0.133453  0.096120  0.299018  0.135867  0.188487  0.000819  0.353541  \n37320  0.313628  0.086169  0.792456  0.576316  0.050573  0.000000  0.022577  \n37321  0.472797  0.061420  1.461895  0.498844  0.123365  0.015723  0.062191  \n\n[37322 rows x 2049 columns]\n"
    }
   ],
   "source": [
    "data = pd.concat([data_labels, data_feature], axis=1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(37322, 2048)\ndata_x_compress  (37322, 2048) [[1.27028410e-01 3.23610830e+00 9.35147520e-01 ... 6.72237160e-01\n  3.22649500e-02 4.07616200e-01]\n [0.00000000e+00 2.46691060e+00 2.64539600e-02 ... 1.12520206e+00\n  6.84306000e-03 2.56229610e-01]\n [3.83341340e-01 1.01190424e+00 0.00000000e+00 ... 2.25265384e+00\n  2.95226900e-02 3.31381500e-02]\n ...\n [1.67553000e-02 1.10568988e+00 1.03398520e-01 ... 1.88486860e-01\n  8.19240000e-04 3.53541340e-01]\n [1.21401340e-01 1.05009258e+00 6.92100000e-03 ... 5.05729500e-02\n  0.00000000e+00 2.25771200e-02]\n [2.43460600e-01 1.25547945e+00 4.84714400e-02 ... 1.23365400e-01\n  1.57233700e-02 6.21905400e-02]]\nTime: 0.004224443999994776\n       label         0         1         2         3         4         5  \\\n0        1.0  0.127028  3.236108  0.935148  0.144205  1.114897  1.502288   \n1        1.0  0.000000  2.466911  0.026454  0.075211  1.159094  3.066645   \n2        1.0  0.383341  1.011904  0.000000  0.054472  0.343532  0.917366   \n3        1.0  0.117190  1.225786  0.001932  0.000000  3.135732  0.061605   \n4        1.0  0.274902  0.337544  0.084937  0.000000  1.788061  0.143165   \n...      ...       ...       ...       ...       ...       ...       ...   \n37317   38.0  0.498370  1.883775  0.000000  0.212900  0.195262  0.201184   \n37318   38.0  0.043884  0.309244  0.012275  0.173839  0.893198  0.183430   \n37319   38.0  0.016755  1.105690  0.103399  0.384196  0.469869  0.512281   \n37320   38.0  0.121401  1.050093  0.006921  0.545237  0.884461  0.738782   \n37321   38.0  0.243461  1.255479  0.048471  0.660457  0.147349  1.431781   \n\n              6         7         8  ...      2038      2039      2040  \\\n0      0.410044  0.053410  0.000000  ...  0.000000  0.008841  0.059838   \n1      0.191157  0.049365  0.017394  ...  0.000000  0.312142  0.447039   \n2      0.044860  0.100728  0.266673  ...  0.000000  0.112120  0.000000   \n3      0.974178  0.463193  0.057579  ...  0.000000  0.168017  0.460633   \n4      0.250313  0.000000  0.134671  ...  0.000000  0.106673  0.040709   \n...         ...       ...       ...  ...       ...       ...       ...   \n37317  1.139161  0.072897  0.040727  ...  0.000000  0.000000  1.554183   \n37318  0.092317  0.000000  0.589606  ...  0.111526  0.000000  0.989355   \n37319  0.173142  0.197311  0.164240  ...  0.000000  0.024520  0.987232   \n37320  0.308057  0.086900  0.256168  ...  0.013474  0.001264  0.677624   \n37321  0.139688  0.174798  0.250953  ...  0.034443  0.319134  2.102535   \n\n           2041      2042      2043      2044      2045      2046      2047  \n0      0.018889  0.024724  0.256931  0.071584  0.672237  0.032265  0.407616  \n1      0.212022  0.000000  0.226731  0.278318  1.125202  0.006843  0.256230  \n2      0.000000  1.184899  0.000000  0.000000  2.252654  0.029523  0.033138  \n3      0.000000  0.008595  0.001324  0.000000  0.725105  0.101483  0.000000  \n4      0.000000  0.000000  0.010016  0.010241  0.755545  0.000000  0.060499  \n...         ...       ...       ...       ...       ...       ...       ...  \n37317  0.946372  0.146409  0.412591  0.250193  0.013291  0.018395  0.040261  \n37318  0.306995  0.000000  0.728876  0.330187  0.400795  0.032162  0.272096  \n37319  0.133453  0.096120  0.299018  0.135867  0.188487  0.000819  0.353541  \n37320  0.313628  0.086169  0.792456  0.576316  0.050573  0.000000  0.022577  \n37321  0.472797  0.061420  1.461895  0.498844  0.123365  0.015723  0.062191  \n\n[37322 rows x 2049 columns]\n"
    }
   ],
   "source": [
    "def generate_compressed_data(data, num_task, num_data):\n",
    "    # print('row')\n",
    "    data_numpy = data.to_numpy()\n",
    "    data_x = data_numpy[:,1:]\n",
    "    data_y = np.atleast_2d(data_numpy[:,0])\n",
    "    print(data_x.shape)\n",
    "    start = timeit.default_timer()\n",
    "    # pca = PCA(n_components=1024)\n",
    "    # data_x_compress = pca.fit_transform(data_x)\n",
    "    data_x_compress = data_x\n",
    "    print('data_x_compress ',data_x_compress.shape, data_x_compress)\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time:', stop - start)\n",
    "\n",
    "    data_compress_np = np.hstack((data_y.T, data_x_compress))\n",
    "    data_compressed =  pd.DataFrame(data_compress_np,\n",
    "                   columns=['label'] +[i for i in range(data_x_compress.shape[1])])\n",
    "    print(data_compressed)\n",
    "    return data_compressed\n",
    "\n",
    "        \n",
    "\n",
    "data_compressed = generate_compressed_data(data, num_task, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 107.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\ntot task  50\n"
    }
   ],
   "source": [
    "def generate_split(splits, num_data):\n",
    "    indx = [ x for x in range(num_data)]\n",
    "    #print(temp)\n",
    "    train_indx = list(np.random.choice(indx, size=splits['train'], replace=False))\n",
    "    temp = [x for x in indx if x not in train_indx]\n",
    "    #print(len(train_indx))\n",
    "    # val_indx = list(np.random.choice(temp, size=55, replace=False))\n",
    "    test_indx = temp\n",
    "    #print(len(test_indx))\n",
    "    return train_indx, test_indx\n",
    "\n",
    "def task_data_split(data, num_task, num_data, splits, random_state=1):\n",
    "    task_train_byID = {}\n",
    "    task_test_byID = {}\n",
    "    task_val_byID = {}\n",
    "  \n",
    "    for i in range(num_task):   \n",
    "        task_data = data.loc[data['label'].eq(i+1)]\n",
    "        \n",
    "        sampled_task_data = task_data.sample(n=num_data, random_state=i)\n",
    "        #print(sampled_task_data.shape, sampled_task_data)\n",
    "        sampled_data = data.sample(n=num_data, random_state=i)\n",
    "\n",
    "        train_indx, test_indx = generate_split(splits, num_data)\n",
    "\n",
    "        task_train_byID[i+1] = np.vstack((sampled_task_data.iloc[train_indx].to_numpy(), \\\n",
    "            sampled_data.iloc[train_indx].to_numpy()))\n",
    "        task_train_byID[i+1] = np.hstack( (task_train_byID[i+1], np.ones((len(train_indx)*2, 1))) )\n",
    "        \n",
    "        temp = task_train_byID[i+1][:, 0]==(i+1)\n",
    "        task_train_byID[i+1][:, 0][temp==True] = 1.\n",
    "        task_train_byID[i+1][:, 0][temp==False] = 0.\n",
    "        print(task_train_byID[i+1].shape, np.sum(task_train_byID[i+1][:,0]))\n",
    "\n",
    "\n",
    "        task_test_byID[i+1] = np.vstack((sampled_task_data.iloc[test_indx].to_numpy(), \\\n",
    "            sampled_data.iloc[test_indx].to_numpy()))\n",
    "        task_test_byID[i+1] = np.hstack( (task_test_byID[i+1], np.ones((len(test_indx)*2, 1))) )\n",
    "\n",
    "        temp = task_test_byID[i+1][:, 0]==(i+1)\n",
    "        task_test_byID[i+1][:, 0][temp==True] = 1.\n",
    "        task_test_byID[i+1][:, 0][temp==False] = 0.\n",
    "        print(task_test_byID[i+1].shape, np.sum(task_test_byID[i+1][:,0]))\n",
    "\n",
    "    print('tot task ', len(list(task_train_byID.keys())))\n",
    "    return task_train_byID, task_test_byID\n",
    "\n",
    "\n",
    "splits = {}\n",
    "splits['train'] = 100\n",
    "splits['test'] = 0\n",
    "task_train_byID, task_test_byID = task_data_split(data_compressed, num_task, num_data, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = FuncRecursiveNet([\n",
    "    FLinearLayer(1, False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test the coef in logistic regression\n",
    "\n",
    "def singleTaskTrain(task_train, task_test):\n",
    "    X = task_train[:, 1:]\n",
    "    y = task_train[:, 0]\n",
    "\n",
    "    clf = LogisticRegression(fit_intercept = False, max_iter=100, C=1.0, random_state=0).fit(X, y)\n",
    "    pred_y = clf.predict(X)\n",
    "\n",
    "    # X_test = task_test[:, 1:]\n",
    "    # y_test = task_test[:, 0]\n",
    "    # pred_y_test = clf.predict(X_test)\n",
    "\n",
    "    param = clf.coef_\n",
    "    # print('pred_y_test ', np.sum(pred_y_test == y_test)/y_test.shape[0], param.shape)\n",
    "\n",
    "    #bias = clf.intercept_\n",
    "    #return np.hstack((param, np.atleast_2d(bias)))\n",
    "    return param, 1\n",
    "\n",
    "weight = singleTaskTrain(task_train_byID[1], task_test_byID[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[torch.Size([1, 2049])]\n{0: [(1, 2049)]}\n"
    }
   ],
   "source": [
    "init_param = net.initialize_weights(utils.toTensor(task_train_byID[1][:, 1:]))\n",
    "p_lst = [p.size() for p in init_param]\n",
    "print(p_lst)\n",
    "shape_record = {}\n",
    "for i , p in enumerate(p_lst):\n",
    "    shape_record[i] = [tuple(p)]\n",
    "print(shape_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# net_pred_y_test = net(reshape_w, xtest)\n",
    "# print('net_pred_y_test ', net_pred_y_test.shape)\n",
    "# net_pred_y_test = torch.sigmoid(net_pred_y_test )\n",
    "# net_pred_y_test[net_pred_y_test >= 0.5] = 1.\n",
    "# net_pred_y_test[net_pred_y_test < 0.5] = 0.\n",
    "# print((utils.toNumpy(net_pred_y_test) == np.atleast_2d(pred_y_test).T).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n0     -1.00  -1.00  -1.00  -1.00  12.34   0.00   0...\n1     39.25   1.39   0.00  74.14   3.75   0.00   0...\n2     83.40  64.79   0.00   0.00   1.25   0.00   0...\n3     19.38   0.00   0.00  87.81   7.50   0.00   0...\n4     69.58  73.33   0.00   6.39   0.00   0.00   0...\n5     19.38  50.09  29.44   8.98  38.19   0.00   0...\n6     44.90  42.91   4.44  69.41  35.94   0.00   0...\n7     43.54  15.88   5.00  54.16  26.82   3.12   2...\n8     12.92   4.38  67.08   7.50  25.60   0.00   0...\n9     56.21  23.51  12.22  32.69  38.13   0.00   0...\n10    87.99  85.35   0.00   0.00   0.00   0.00   0...\n11    39.05   0.00   0.00  51.33  34.91   0.00   0...\n12    42.47  30.12   3.75  20.00   2.50  72.91   5...\n13     4.77   0.00   0.00  18.61  81.49   0.00   0...\n14    40.88  19.44   0.00  31.33   2.50  20.42   2...\n15    10.24   6.25   0.00  91.20  11.81   0.00   3...\n16    36.04   6.77   0.00  55.21  34.48   0.00   0...\n17    24.01   5.92  31.10   8.75  59.43   0.00   0...\n18     2.50   3.75   0.00  15.23  83.97   0.00   0...\n19    63.37   1.79   7.14  45.51  17.01   8.48   3...\n20    43.99  27.68  12.21  50.80  36.31   5.91   3...\n21     0.00   1.56   0.00  51.25  11.17  48.92  40...\n22    32.36  89.09   0.00   8.41  19.38   0.00   0...\n23    81.96  29.31   1.56  35.00  44.50   0.00   0...\n24    47.51   9.93   0.00  69.10  13.05   7.14   0...\n25    41.38  39.71   0.00  62.76  37.38  17.50   2...\n26    10.56  13.19   0.00  64.51  72.67   0.00   3...\n27     9.75   7.50   7.50  19.90  63.96   0.00   0...\n28    26.49  64.32   0.00  47.12  39.71   0.00   0...\n29    91.55   1.39   0.00  54.76  27.64   0.00   0...\n30     6.11  11.87   0.00  32.21   0.00  24.70   1...\n31    43.74  23.96   0.00  58.07  53.94   0.00   0...\n32    32.63  10.00   0.00  64.79  22.63   2.50   0...\n33    50.13  33.78   0.00  46.23  40.42   0.00   0...\n34    30.57  14.11   0.00  62.48  30.34   0.00   0...\n35    46.81   0.00   0.00  44.86  16.25   0.00   0...\n36    45.37   0.00   0.00  61.05   9.90   0.00   0...\n37    85.04  85.04   0.00   0.00   0.00   0.00   0...\n38    76.85  72.33   0.00   5.00   4.38   0.00   0...\n39     0.00  20.34   0.00  75.85   5.92   0.00   0...\n40    16.13   9.44   0.00  38.39   2.50  37.08   5...\n41    21.52  25.04   0.00  35.13  26.62   0.00   6...\n42     1.88   1.25   0.00  43.91   0.00  13.12   0...\n43    18.37  55.35   0.00  32.53  49.72   0.00   0...\n44    10.00  95.62   2.50   3.12  12.50   0.00   0...\n45    10.13  41.37   0.00  47.27   3.75   8.00   0...\n46    18.84   4.82   0.00  67.59  44.27   0.00   0...\n47    63.57  43.10   0.00  17.29  54.51   0.00   0...\n48    55.31  55.46   0.00  58.48  15.50   1.49   0...\n49    10.22  21.53  27.73   0.33  60.82   0.00   0...\nattr_mat  (50, 85) [[-1.   -1.   -1.   ...  2.35  9.7   8.38]\n [39.25  1.39  0.   ... 58.64 20.14 11.39]\n [83.4  64.79  0.   ... 15.77 13.41 15.42]\n ...\n [63.57 43.1   0.   ... 35.95 28.26  5.  ]\n [55.31 55.46  0.   ...  5.04 18.89 72.99]\n [10.22 21.53 27.73 ...  3.96 14.05 37.98]]\nattr_mean  (85,) [35.353  26.8552  4.2134 38.7088 26.7472  5.4258  1.6162  3.4638 17.408\n 13.7532  7.527  47.9434 17.579  24.761  42.6342 25.5634 27.3922 24.3846\n  9.9202  6.6932 14.0662 13.4276 31.3452 16.8672  7.384  46.6096 31.0334\n 26.2148 15.9748  7.2286 10.4296 21.5932  4.3376 22.3498  2.8336  4.8648\n 15.8432  4.616  49.8198 43.1296 19.9802 37.7474 13.0554 26.9938  8.978\n 55.8386 38.9788 21.1302 15.9926 12.2854 35.2384 17.1956 24.8958  4.6066\n 32.6254  5.3666 21.906  19.9656 21.6444  9.1574  2.839  14.419  45.24\n 46.9496 11.3378  9.9162  3.5946 14.1796 19.1218 22.7342 22.952  14.2398\n 12.3536 11.113  47.7196 15.4424 11.177   7.6474 26.7582 29.2078 33.6064\n 31.0194 25.8326 20.8992 25.6106]\nsub  (50, 85) [[35.353  26.8552  4.2134 ... 25.8326 20.8992 25.6106]\n [35.353  26.8552  4.2134 ... 25.8326 20.8992 25.6106]\n [35.353  26.8552  4.2134 ... 25.8326 20.8992 25.6106]\n ...\n [35.353  26.8552  4.2134 ... 25.8326 20.8992 25.6106]\n [35.353  26.8552  4.2134 ... 25.8326 20.8992 25.6106]\n [35.353  26.8552  4.2134 ... 25.8326 20.8992 25.6106]]\nattr_std  (85,) [25.78326855 27.6032904  11.566389   25.6046854  22.71734659 13.68025513\n  5.83216114  9.48177091 18.94898689 20.98975068 22.54484884 27.34491774\n 23.08905137 21.4124635  33.46576508 28.19931294 22.67803319 21.0269474\n 24.75392934 16.45096623 25.59956249 12.07203273 28.66201041 21.76546605\n 15.75149745 22.97892286 13.13154577 26.43406857 18.96176112 12.914046\n 24.29786015 23.50304792 16.19597685 17.31643664 13.62507024 12.9375505\n 28.84225709 13.11231543 24.59238097 23.46726895 19.62697567 25.54466369\n 15.22871514 17.78577532 16.95907769 25.18255086 21.59364774 17.3170192\n 20.35840002 18.64288006 22.96745962 23.37839457 27.0514552  16.02369703\n 26.47740963  9.11133922 16.55216034 26.0711311  25.38265622 11.4468432\n  9.76550833 24.25145045 21.33646428 20.92279159 21.78987933 13.36149661\n  5.51455935 15.57556162 18.02559399 23.03163217 21.54441301 24.46585698\n 15.07316805 26.76493641 22.47148789 27.86737042 21.22906095 14.44687092\n 24.08915388 18.77568931 18.50139225 21.23117862 17.22951268 10.56036133\n 26.24974586]\nz score (50, 85) [[25.78326855 27.6032904  11.566389   ... 17.22951268 10.56036133\n  26.24974586]\n [25.78326855 27.6032904  11.566389   ... 17.22951268 10.56036133\n  26.24974586]\n [25.78326855 27.6032904  11.566389   ... 17.22951268 10.56036133\n  26.24974586]\n ...\n [25.78326855 27.6032904  11.566389   ... 17.22951268 10.56036133\n  26.24974586]\n [25.78326855 27.6032904  11.566389   ... 17.22951268 10.56036133\n  26.24974586]\n [25.78326855 27.6032904  11.566389   ... 17.22951268 10.56036133\n  26.24974586]]\nffff  [[-1.40994537 -1.00912607 -0.45073705 ... -1.36292886 -1.06049402\n  -0.65641016]\n [ 0.15114453 -0.92254219 -0.36427964 ...  1.90413975 -0.07189148\n  -0.54174239]\n [ 1.86349531  1.37428544 -0.36427964 ... -0.58403277 -0.70918028\n  -0.38821709]\n ...\n [ 1.09439189  0.58850955 -0.36427964 ...  0.58721336  0.6970216\n  -0.78517332]\n [ 0.77402909  1.03628225 -0.36427964 ... -1.2068014  -0.19025864\n   1.804947  ]\n [-0.97477944 -0.19291903  2.03318426 ... -1.26948454 -0.64857629\n   0.4712198 ]]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n"
    }
   ],
   "source": [
    "def gen_attr(path_attributes):\n",
    "    data_attributes = pd.read_csv(path_attributes, sep=\"\\n\", header=None)\n",
    "    print(data_attributes)\n",
    "    lst = []\n",
    "    task_attr_byID = {}\n",
    "    i = 1\n",
    "    for r in data_attributes.iterrows():\n",
    "        # print(len(r), )\n",
    "        s = r[1].to_numpy()\n",
    "        # print(len(s[0].split()), s[0].split())\n",
    "        print(len(s[0].split()))\n",
    "        task_attr_byID[i] = np.array([float(a) for a in s[0].split()])\n",
    "        print(task_attr_byID[i].shape)\n",
    "        i += 1\n",
    "    return task_attr_byID\n",
    "\n",
    "def gen_attr_zScore(path_attributes):\n",
    "    data_attributes = pd.read_csv(path_attributes, sep=\"\\n\", header=None)\n",
    "    print(data_attributes)\n",
    "    lst = []\n",
    "    task_attr_byID = {}\n",
    "    i = 1\n",
    "    for r in data_attributes.iterrows():\n",
    "        #print(len(r), )\n",
    "        s = r[1].to_numpy()\n",
    "        # print(len(s[0].split()), s[0].split())\n",
    "        #print(r[0], len(s[0].split()))\n",
    "        lst.append(np.atleast_2d(np.array([float(a) for a in s[0].split()])))\n",
    "        # print(task_attr_byID[i].shape)\n",
    "        # i += 1\n",
    "    attr_mat = np.concatenate(lst, axis=0)\n",
    "    print('attr_mat ', attr_mat.shape, attr_mat)\n",
    "    attr_mean = np.mean(attr_mat, axis=0)\n",
    "    print('attr_mean ', attr_mean.shape, attr_mean)\n",
    "\n",
    "\n",
    "    attr_sub_mean = attr_mat - attr_mean\n",
    "    print('sub ', attr_sub_mean.shape, attr_mat - attr_sub_mean)\n",
    "\n",
    "    attr_std = np.std(attr_mat, axis=0)\n",
    "    print('attr_std ', attr_std.shape, attr_std)\n",
    "\n",
    "    attr_z = attr_sub_mean/attr_std\n",
    "    print('z score', attr_z.shape, attr_sub_mean/attr_z)\n",
    "\n",
    "    print('ffff ',attr_z)\n",
    "\n",
    "    for t in range(attr_z.shape[0]):\n",
    "        task_attr_byID[t+1] = attr_z[t,:]\n",
    "    \n",
    "    print(list(task_attr_byID.keys()))\n",
    "    return task_attr_byID\n",
    "\n",
    "task_attr_byID = gen_attr_zScore(path_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50\n50\nmean acc  1.0\n"
    }
   ],
   "source": [
    "def ZSTL_train_test_val(num_task, task_train_byID, task_test_byID, task_attr_byID, destination):\n",
    "    task_train_data = {}\n",
    "    task_test_data = {}\n",
    "    task_val_data = {}\n",
    "    acc = 0\n",
    "    for t in range(num_task):\n",
    "        weight, a = singleTaskTrain(task_train_byID[t+1], task_test_byID[t+1])\n",
    "        acc += a\n",
    "        cur_task_train = (task_attr_byID[t+1], weight, task_train_byID[t+1][:,1:], np.atleast_2d(task_train_byID[t+1][:,0]).T)\n",
    "        cur_task_test = (task_attr_byID[t+1], weight, task_test_byID[t+1][:,1:], np.atleast_2d(task_test_byID[t+1][:,0]).T)\n",
    "\n",
    "        task_train_data[t+1] = cur_task_train\n",
    "        task_test_data[t+1] = cur_task_test\n",
    "\n",
    "    print(len(task_train_data))\n",
    "    print(len(task_test_data))\n",
    "    \n",
    "    with open(destination+'task_train_data_standard.pickle', 'wb') as handle:\n",
    "        pickle.dump(task_train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(destination+'task_test_data_standard.pickle', 'wb') as handle:\n",
    "        pickle.dump(task_test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('mean acc ', acc/num_task)\n",
    "\n",
    "\n",
    "# a = {'hello': 'world'}\n",
    "\n",
    "# with open('filename.pickle', 'wb') as handle:\n",
    "#     pickle.dump([a], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('filename.pickle', 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "\n",
    "# print(a == b[0])\n",
    "ZSTL_train_test_val(num_task, task_train_byID, task_test_byID, task_attr_byID, path_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}