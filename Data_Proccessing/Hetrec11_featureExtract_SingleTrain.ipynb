{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598647971025",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import timeit\n",
    "from mlmodel import *\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "import utils\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import json\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[31martists.dat\u001b[m\u001b[m                       \u001b[31muser_artists.dat\u001b[m\u001b[m\n\u001b[34mextracted_feature\u001b[m\u001b[m                 \u001b[31muser_friends.dat\u001b[m\u001b[m\n\u001b[31mreadme.txt\u001b[m\u001b[m                        \u001b[31muser_taggedartists-timestamps.dat\u001b[m\u001b[m\n\u001b[31mtags.dat\u001b[m\u001b[m                          \u001b[31muser_taggedartists.dat\u001b[m\u001b[m\n"
    }
   ],
   "source": [
    "!ls ../ZSTL_Data/hetrec2011-lastfm-2k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "userID  artistID  weight\n0           2        51   13883\n1           2        52   11690\n2           2        53   11351\n3           2        54   10300\n4           2        55    8983\n...       ...       ...     ...\n92829    2100     18726     337\n92830    2100     18727     297\n92831    2100     18728     281\n92832    2100     18729     280\n92833    2100     18730     263\n\n[92834 rows x 3 columns] 17632\n        userID  artistID  tagID      timestamp\n0            2        52     13  1238536800000\n1            2        52     15  1238536800000\n2            2        52     18  1238536800000\n3            2        52     21  1238536800000\n4            2        52     41  1238536800000\n...        ...       ...    ...            ...\n186474    2100     16437      4  1277935200000\n186475    2100     16437    292  1272664800000\n186476    2100     16437   2087  1277935200000\n186477    2100     16437   2801  1272664800000\n186478    2100     16437   3335  1277935200000\n\n[186479 rows x 4 columns] 1892 12523 9749\n       userID  friendID\n0           2       275\n1           2       428\n2           2       515\n3           2       761\n4           2       831\n...       ...       ...\n25429    2099      1801\n25430    2099      2006\n25431    2099      2016\n25432    2100       586\n25433    2100       607\n\n[25434 rows x 2 columns] 1892 1892\n          id               name                                           url  \\\n0          1       MALICE MIZER         http://www.last.fm/music/MALICE+MIZER   \n1          2    Diary of Dreams      http://www.last.fm/music/Diary+of+Dreams   \n2          3  Carpathian Forest    http://www.last.fm/music/Carpathian+Forest   \n3          4       Moi dix Mois         http://www.last.fm/music/Moi+dix+Mois   \n4          5        Bella Morte          http://www.last.fm/music/Bella+Morte   \n...      ...                ...                                           ...   \n17627  18741     Diamanda GalÃ¡s  http://www.last.fm/music/Diamanda+Gal%C3%A1s   \n17628  18742             Aya RL               http://www.last.fm/music/Aya+RL   \n17629  18743        Coptic Rain          http://www.last.fm/music/Coptic+Rain   \n17630  18744       Oz Alchemist         http://www.last.fm/music/Oz+Alchemist   \n17631  18745   Grzegorz Tomczak     http://www.last.fm/music/Grzegorz+Tomczak   \n\n                                              pictureURL  \n0        http://userserve-ak.last.fm/serve/252/10808.jpg  \n1      http://userserve-ak.last.fm/serve/252/3052066.jpg  \n2      http://userserve-ak.last.fm/serve/252/40222717...  \n3      http://userserve-ak.last.fm/serve/252/54697835...  \n4      http://userserve-ak.last.fm/serve/252/14789013...  \n...                                                  ...  \n17627  http://userserve-ak.last.fm/serve/252/16352971...  \n17628   http://userserve-ak.last.fm/serve/252/207445.jpg  \n17629   http://userserve-ak.last.fm/serve/252/344868.jpg  \n17630  http://userserve-ak.last.fm/serve/252/29297695...  \n17631  http://userserve-ak.last.fm/serve/252/59486303...  \n\n[17632 rows x 4 columns] 17632\nEmpty DataFrame\nColumns: [id, name, url, pictureURL]\nIndex: []\n"
    }
   ],
   "source": [
    "path_user_artist = '../ZSTL_Data/hetrec2011-lastfm-2k/user_artists.dat'\n",
    "path_artist = '../ZSTL_Data/hetrec2011-lastfm-2k/artists.dat'\n",
    "path_user_artist_tag = '../ZSTL_Data/hetrec2011-lastfm-2k/user_taggedartists-timestamps.dat'\n",
    "path_user_friends = '../ZSTL_Data/hetrec2011-lastfm-2k/user_friends.dat'\n",
    "\n",
    "df_user_artist = pd.read_csv(path_user_artist, sep=\"\\t\")\n",
    "print(df_user_artist, len(df_user_artist.artistID.unique()))\n",
    "df_user_artist_tag = pd.read_csv(path_user_artist_tag, sep=\"\\t\")\n",
    "print(df_user_artist_tag, len(df_user_artist_tag.userID.unique()),\\\n",
    "    len(df_user_artist_tag.artistID.unique()),len(df_user_artist_tag.tagID.unique()))\n",
    "\n",
    "df_user_friends = pd.read_csv(path_user_friends, sep=\"\\t\")\n",
    "print(df_user_friends, len(df_user_friends.userID.unique()), len(df_user_friends.friendID.unique()))\n",
    "\n",
    "df_artist = pd.read_csv(path_artist, sep=\"\\t\")\n",
    "print(df_artist, len(df_artist.id.unique()))\n",
    "print(df_artist.loc[df_artist.id.eq(14103)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1892 17632 9749\n"
    }
   ],
   "source": [
    "num_user = len(df_user_artist.userID.unique())\n",
    "num_artist = len(df_artist.id.unique())\n",
    "num_tag = len(df_user_artist_tag.tagID.unique())\n",
    "print(num_user, num_artist, num_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y shape  (1892, 17632)\nsparsity  0.9972171848800758 92834.0\n[]\ni, j  1892 17632\n"
    }
   ],
   "source": [
    "def genUserItem_table(df_user_artist, num_user, num_artist):\n",
    "    artistID_to_X_row = {}\n",
    "    userID_to_Y_row = {}\n",
    "\n",
    "    y = np.zeros((num_user, num_artist))\n",
    "    print('y shape ', y.shape)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for r in df_user_artist.iterrows():\n",
    "        #print(r[0])\n",
    "        #print(r[1].to_numpy())\n",
    "        user_artist_count = r[1].to_numpy()\n",
    "        #print(user_artist_count)\n",
    "        if user_artist_count[0] not in userID_to_Y_row.keys():\n",
    "            userID_to_Y_row[user_artist_count[0]] = i\n",
    "            cur_i = i\n",
    "            i += 1\n",
    "        else:\n",
    "            cur_i = userID_to_Y_row[user_artist_count[0]]\n",
    "\n",
    "        if user_artist_count[1] not in artistID_to_X_row.keys():\n",
    "            artistID_to_X_row[user_artist_count[1]] = j\n",
    "            cur_j = j\n",
    "            j += 1\n",
    "        else:\n",
    "            cur_j = artistID_to_X_row[user_artist_count[1]]\n",
    "        \n",
    "        #print(cur_i, cur_j)\n",
    "        y[cur_i, cur_j] = 1\n",
    "\n",
    "    sparse = 1 - np.sum(y)/(num_user * num_artist)\n",
    "    print('sparsity ', sparse, np.sum(y))\n",
    "    print(y[y>1])\n",
    "    print('i, j ', i ,j)\n",
    "    return y, userID_to_Y_row, artistID_to_X_row\n",
    "\n",
    "y, userID_to_Y_row, artistID_to_X_row = genUserItem_table(df_user_artist, num_user, num_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "len  17632\na shape  (1892, 1892)\n"
    }
   ],
   "source": [
    "def gen_X_n_Attr(df_user_artist_tag, df_user_friends, userID_to_Y_row, artistID_to_X_row, num_user, num_artist, num_tag):\n",
    "    X_tagRecord_byID = {}\n",
    "    friend_to_a_col = {}\n",
    "    tag_to_x_col = {}\n",
    "    xcol = 0\n",
    "    acol = 0\n",
    "    num_tag_artist = len(df_user_artist_tag.artistID.unique())\n",
    "\n",
    "    a = np.zeros((num_user, num_user))\n",
    "    print('a shape ', a.shape)\n",
    "    x = np.zeros((num_artist, num_tag))\n",
    "    not_shown_artist = []\n",
    "\n",
    "    #gen attr\n",
    "    for r in df_user_friends.iterrows():\n",
    "        #print('friend relation ', r[1].to_numpy())\n",
    "        user_friend = r[1].to_numpy()\n",
    "        cur_user = user_friend[0]\n",
    "        friend = user_friend[1]\n",
    "        if friend not in friend_to_a_col:\n",
    "            friend_to_a_col[friend] = acol\n",
    "            cur_acol = acol\n",
    "            acol += 1          \n",
    "        else:\n",
    "            cur_acol = friend_to_a_col[friend]\n",
    "\n",
    "        a[userID_to_Y_row[cur_user], cur_acol] = 1\n",
    "        \n",
    "    #a = ppp\n",
    "\n",
    "    for r in df_user_artist_tag.iterrows():\n",
    "        user_artist_tag = r[1].to_numpy()\n",
    "\n",
    "        cur_user = user_artist_tag[0]\n",
    "        cur_artist = user_artist_tag[1]\n",
    "        cur_tag = user_artist_tag[2]\n",
    "\n",
    "        if cur_user not in X_tagRecord_byID:\n",
    "            X_tagRecord_byID[cur_user] = []\n",
    "            X_tagRecord_byID[cur_user].append((cur_artist, cur_tag))\n",
    "        else:\n",
    "            X_tagRecord_byID[cur_user].append((cur_artist, cur_tag))\n",
    "\n",
    "        if cur_tag not in tag_to_x_col:\n",
    "            tag_to_x_col[cur_tag] = xcol\n",
    "            xcol += 1\n",
    "\n",
    "        if cur_artist in artistID_to_X_row:\n",
    "            x[artistID_to_X_row[cur_artist], tag_to_x_col[cur_tag]] += 1\n",
    "\n",
    "    #print('not shown artist ', len(not_shown_artist), not_shown_artist)\n",
    "    return a, X_tagRecord_byID, tag_to_x_col, x\n",
    "\n",
    "print('len ', len(artistID_to_X_row))\n",
    "a, X_tagRecord_byID, tag_to_x_col, x = gen_X_n_Attr(df_user_artist_tag, df_user_friends, userID_to_Y_row, artistID_to_X_row, num_user, num_artist, num_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "avg_tag  98.56183932346723\na_meanFriend  13.44291754756871\nx  184941.0 10.48894056261343\n31\n"
    }
   ],
   "source": [
    "num_tag = 0\n",
    "for k in X_tagRecord_byID.keys():\n",
    "    num_tag += len(X_tagRecord_byID[k])\n",
    "\n",
    "avg_tag = num_tag/len(list(X_tagRecord_byID.keys()))\n",
    "print('avg_tag ', avg_tag)\n",
    "\n",
    "a_meanFriend = np.mean(np.sum(a, axis=1))\n",
    "print('a_meanFriend ', a_meanFriend)\n",
    "\n",
    "print('x ', np.sum(x), np.mean(np.sum(x, axis=1)) )\n",
    "print(np.sum(np.sum(x, axis=0)==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sum y  50.0 1.0 49.06659619450317 50.0\n"
    }
   ],
   "source": [
    "sum_y = np.sum(y, axis=1)\n",
    "print('sum y ', np.max(sum_y), np.min(sum_y), np.mean(sum_y), np.median(sum_y))\n",
    "# for i in sum_y:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "def genCompressedData(userID_to_Y_row, artistID_to_X_row, X_tagRecord_byID, y, a, destination, compressed_size=100):\n",
    "    compressd_task_byID = {}\n",
    "    i = 0\n",
    "    for t_id in userID_to_Y_row.keys():\n",
    "        #print('t ', t_id, userID_to_Y_row[t_id])\n",
    "        \n",
    "        cur_indx = userID_to_Y_row[t_id]\n",
    "        cur_y = y[cur_indx, :]\n",
    "        #print('y ', cur_y)\n",
    "        pos_indx = np.where(cur_y == 1)\n",
    "        neg_indx = np.where(cur_y == 0)\n",
    "        #print('pos ', len(pos_indx[0]), pos_indx[0])\n",
    "        #print('neg ', len(neg_indx[0]), neg_indx[0])\n",
    "        neg_indx_selected = np.random.choice(neg_indx[0], size=100-len(pos_indx[0]), replace=False)\n",
    "        #print('neg select ', len(neg_indx_selected), neg_indx_selected)\n",
    "        data_indx_selected = np.concatenate([pos_indx[0], neg_indx_selected])\n",
    "        #print('indx selected ', len(indx_selected), indx_selected )\n",
    "        cur_tag_record = X_tagRecord_byID[t_id]\n",
    "\n",
    "        # task_y = cur_y[data_indx_selected]\n",
    "        # task_x = X_tagRecord_byID[t_id]\n",
    "       \n",
    "        #print('task_y ', task_y.shape)\n",
    "        #print('task_x ', task_x.shape)\n",
    "\n",
    "        # clf = LogisticRegression(fit_intercept = False, max_iter=1000,random_state=0).fit(task_x, task_y)\n",
    "        # pred_y = clf.predict(task_x)\n",
    "        # #print('pred_y ', pred_y.shape, 'task_y ', task_y.shape)\n",
    "        # print('acc ', np.sum(pred_y==task_y)/task_y.shape[0])\n",
    "\n",
    "        # param = clf.coef_\n",
    "\n",
    "        #print('param ', param.shape)\n",
    "\n",
    "        #compressd_task_byID[str(t_id)] = (a[cur_indx, :], param, task_x, np.atleast_2d(task_y).T)\n",
    "        compressd_task_byID[t_id] = (cur_indx, data_indx_selected, cur_tag_record)\n",
    "\n",
    "\n",
    "        # i+= 1\n",
    "        # if i == 3:\n",
    "        #     a = pppp\n",
    "\n",
    "    # with open(destination+'sampled_task_0826.pickle', 'wb') as handle:\n",
    "    #     pickle.dump(compressd_task_byID, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return 0 \n",
    "\n",
    "destination = '../ZSTL_Data/hetrec2011-lastfm-2k/extracted_feature/'\n",
    "genCompressedData(userID_to_Y_row, artistID_to_X_row, X_tagRecord_byID, y, a, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "a  [[1. 1. 1. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "def storeTestData( y, a, userID_to_Y_row, artistID_to_X_row, tag_to_x_col, destination):\n",
    "    totData = {}\n",
    "    totData['y'] = y\n",
    "    totData['a'] = a\n",
    "    print('a ', a)\n",
    "    totData['userID_to_Y_row'] = userID_to_Y_row\n",
    "    totData['artistID_to_X_row'] = artistID_to_X_row\n",
    "    totData['tag_to_x_col'] = tag_to_x_col\n",
    "    totData['num_artist'] = len(list(artistID_to_X_row.keys()))\n",
    "    totData['num_tag'] = len(list(tag_to_x_col.keys()))\n",
    "\n",
    "    with open(destination+'detailed_data_0826.pickle', 'wb') as handle:\n",
    "        pickle.dump(totData, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return 0\n",
    "storeTestData( y, a, userID_to_Y_row, artistID_to_X_row, tag_to_x_col, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../ZSTL_Data/hetrec2011-lastfm-2k/extracted_feature/'\n",
    "compressed_data = 'sampled_task_0826.pickle'\n",
    "detailed_data = 'detailed_data_0826.pickle'\n",
    "\n",
    "with open(path_data+compressed_data, 'rb') as f:\n",
    "    compressed_dataset = pickle.load(f)\n",
    "\n",
    "with open(path_data+detailed_data, 'rb') as f:\n",
    "    detailed_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_hetrec(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, compressed_data, detailed_data, preset_x = None, train_task=False, phase='Normal', learner='LR'):\n",
    "        'Initialization'\n",
    "        self.compressed_data = compressed_data\n",
    "\n",
    "        self.num_artist = detailed_data['num_artist']\n",
    "        #print('num_artist ', self.num_artist, len(list(detailed_data['artistID_to_X_row'].keys())))\n",
    "        self.num_tag = detailed_data['num_tag']\n",
    "        #print('num_tag', self.num_tag)\n",
    "        self.userID_to_Y_row = detailed_data['userID_to_Y_row']\n",
    "        self.artistID_to_X_row = detailed_data['artistID_to_X_row']\n",
    "        self.tag_to_x_col = detailed_data['tag_to_x_col']\n",
    "        #print('tag_to_x_col ',len(list(self.tag_to_x_col.keys())))\n",
    "        #print\n",
    "\n",
    "        \n",
    "        self.y = detailed_data['y']\n",
    "        self.a = detailed_data['a']\n",
    "        #print('a ', self.a)\n",
    "        self.pahse = phase\n",
    "        self.param = {}\n",
    "        print('learner ', learner)\n",
    "        if preset_x is None:\n",
    "            print('gen x', learner)\n",
    "            self.x = np.zeros((self.num_artist, self.num_tag))\n",
    "            if learner=='LR':\n",
    "                self.genX()\n",
    "            elif learner=='NN':\n",
    "                print('x nn')\n",
    "                self.genX(ones_col=False)\n",
    "        else:\n",
    "            self.x = preset_x\n",
    "\n",
    "        if train_task:\n",
    "            print('gen w, wait ', learner)\n",
    "            if learner=='LR':\n",
    "                self.genW_LR()\n",
    "            elif learner=='NN':\n",
    "                self.genW_NN()\n",
    "\n",
    "            print('gen w done')\n",
    "        else:\n",
    "            print('not gen w')\n",
    "            for k in self.userID_to_Y_row.keys():\n",
    "                self.param[k] = np.atleast_2d(np.array(0))\n",
    "\n",
    "  def genX(self, ones_col=True):\n",
    "        tag_len = 0\n",
    "        not_inLst = set()\n",
    "        for data in self.compressed_data:\n",
    "            k = data[0]\n",
    "            cur_tag_record = data[1][-1]\n",
    "            tag_len += len(cur_tag_record)\n",
    "            #print('cur_tag_record ', cur_tag_record)\n",
    "            for item in cur_tag_record:\n",
    "                #print(item)\n",
    "                artist = item[0]\n",
    "                tag = item[1]\n",
    "                if artist in self.artistID_to_X_row.keys():\n",
    "                    self.x[self.artistID_to_X_row[artist], self.tag_to_x_col[tag]] += 1\n",
    "                else:\n",
    "                    not_inLst.add(artist)\n",
    "            \n",
    "        #print('x shape ', self.x.shape)\n",
    "        if ones_col==True:\n",
    "            print('concat ones')\n",
    "            ones = np.ones((self.x.shape[0], 1))\n",
    "            self.x = np.concatenate([ones, self.x], axis=1)\n",
    "        print('x shape ', self.x.shape)\n",
    "\n",
    "        \n",
    "        # print('avg listen ', np.sum(self.x))\n",
    "        # print('avg tag ', tag_len/len(self.compressed_data), tag_len)\n",
    "        # print('num not in ', len(not_inLst))\n",
    "\n",
    "  def genW_LR(self,):\n",
    "        \n",
    "        for data in self.compressed_data:\n",
    "            k = data[0]\n",
    "            cur_compressed_data = data[1]\n",
    "            cur_y = self.y[cur_compressed_data[0],:]\n",
    "\n",
    "            task_y = cur_y[cur_compressed_data[1]]\n",
    "            task_x = [np.expand_dims(self.x[i,:], axis=0) for i in cur_compressed_data[1]]\n",
    "            task_x = np.concatenate(task_x, axis=0)\n",
    "\n",
    "            clf = LogisticRegression(fit_intercept = False, max_iter=1000,random_state=0).fit(task_x, task_y)\n",
    "            pred_y = clf.predict(task_x)\n",
    "\n",
    "            print('acc ', np.sum(pred_y==task_y)/task_y.shape[0])\n",
    "            self.param[k] = clf.coef_\n",
    "        #print(self.param[k].shape)\n",
    "\n",
    "  def genW_NN(self,):\n",
    "        for data in self.compressed_data:\n",
    "            k = data[0]\n",
    "            cur_compressed_data = data[1]\n",
    "            cur_y = self.y[cur_compressed_data[0],:]\n",
    "\n",
    "            task_y = np.expand_dims(cur_y[cur_compressed_data[1]], axis=0).T\n",
    "            task_x = [np.expand_dims(self.x[i,:], axis=0) for i in cur_compressed_data[1]]\n",
    "            task_x = np.concatenate(task_x, axis=0)\n",
    "            #print('num i/o ',task_x.size()[1], task_y.size()[1])\n",
    "            model = utils.CSR_model(task_x.shape[1], task_y.shape[1])\n",
    "\n",
    "            #print('task y', task_y.shape, 'task x',task_x.shape)\n",
    "            self.param[k] = utils.CSR_train(task_x, task_y, 200, 1e-4, model)\n",
    "            \n",
    "            #a = ppppp\n",
    "            # print('acc ', np.sum(pred_y==task_y)/task_y.shape[0])\n",
    "            # self.param[k] = clf.coef_\n",
    "        #print(self.param[k].shape)\n",
    "        self.shape_record = {}\n",
    "        for i, w in enumerate(self.param[k]):\n",
    "            self.shape_record[i] = [w.shape]\n",
    "        print('shape record ', self.shape_record)\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.compressed_data)\n",
    "\n",
    "  def vectorize(self, weights):\n",
    "        #weights with original model parameter shape\n",
    "        flatted_param = []\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            #print(w)\n",
    "            if len(w) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                flatted = w.flatten()\n",
    "                flatted_param = flatted_param + list(flatted)\n",
    "        \n",
    "        return flatted_param\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        k = self.compressed_data[index][0]\n",
    "        item = self.compressed_data[index][1]\n",
    "        data_indx = item[0]\n",
    "        # Load data and get label\n",
    "        #print('data_indx ', data_indx)\n",
    "        a = np.array(self.a[data_indx, :])\n",
    "        a = np.expand_dims(a, axis=0)\n",
    "      \n",
    "        w = self.param[k]\n",
    "        w = np.expand_dims(self.vectorize(w), axis=0)\n",
    "\n",
    "        if self.pahse == 'Normal':\n",
    "            selected_data = item[1]\n",
    "            x = [np.expand_dims(self.x[i,:], axis=0) for i in selected_data]\n",
    "            x = np.concatenate(x, axis=0)\n",
    "            cur_y = self.y[data_indx, :]\n",
    "            y = np.expand_dims(cur_y[selected_data], axis=0).T\n",
    "            return a, w, x, y\n",
    "        elif self.pahse == 'mAP':\n",
    "            y = np.expand_dims(self.y[data_indx, :], axis=1)\n",
    "            return a, w, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_size  1513 test_size  379\n1513\n379\nlearner  NN\ngen x NN\nx nn\nx shape  (17632, 9749)\nnot gen w\npreset x  (17632, 9749)\n100 1413\nlearner  NN\ngen w, wait  NN\n====> Epoch: 99 Average loss: 0.3587; ACC 0.8500000238418579\n====> Epoch: 199 Average loss: 0.2215; ACC 0.8600000143051147\n\n\n====> Epoch: 99 Average loss: 0.3757; ACC 0.9200000166893005\n====> Epoch: 199 Average loss: 0.1834; ACC 0.949999988079071\n\n\n====> Epoch: 99 Average loss: 0.3379; ACC 0.9300000071525574\n====> Epoch: 199 Average loss: 0.1788; ACC 0.9399999976158142\n\n\n====> Epoch: 99 Average loss: 0.2602; ACC 1.0\n====> Epoch: 199 Average loss: 0.0915; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.2931; ACC 1.0\n====> Epoch: 199 Average loss: 0.1034; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.2632; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.0975; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3050; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1319; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3428; ACC 0.9399999976158142\n====> Epoch: 199 Average loss: 0.1583; ACC 0.9399999976158142\n\n\n====> Epoch: 99 Average loss: 0.3455; ACC 0.9100000262260437\n====> Epoch: 199 Average loss: 0.2001; ACC 0.9300000071525574\n\n\n====> Epoch: 99 Average loss: 0.3261; ACC 0.9599999785423279\n====> Epoch: 199 Average loss: 0.1534; ACC 0.9599999785423279\n\n\n====> Epoch: 99 Average loss: 0.2985; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1049; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.2925; ACC 0.9399999976158142\n====> Epoch: 199 Average loss: 0.1469; ACC 0.9399999976158142\n\n\n====> Epoch: 99 Average loss: 0.2902; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1194; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3303; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1527; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.2981; ACC 0.9599999785423279\n====> Epoch: 199 Average loss: 0.1383; ACC 0.9599999785423279\n\n\n====> Epoch: 99 Average loss: 0.3348; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1502; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.3376; ACC 0.9599999785423279\n====> Epoch: 199 Average loss: 0.1461; ACC 0.9599999785423279\n\n\n====> Epoch: 99 Average loss: 0.3185; ACC 0.9599999785423279\n====> Epoch: 199 Average loss: 0.1451; ACC 0.9599999785423279\n\n\n====> Epoch: 99 Average loss: 0.3211; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1341; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3071; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1102; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.3329; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1115; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3379; ACC 0.9399999976158142\n====> Epoch: 199 Average loss: 0.1724; ACC 0.9399999976158142\n\n\n====> Epoch: 99 Average loss: 0.2739; ACC 0.9599999785423279\n====> Epoch: 199 Average loss: 0.1163; ACC 0.9599999785423279\n\n\n====> Epoch: 99 Average loss: 0.2927; ACC 1.0\n====> Epoch: 199 Average loss: 0.0833; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.3034; ACC 0.9300000071525574\n====> Epoch: 199 Average loss: 0.1464; ACC 0.9399999976158142\n\n\n====> Epoch: 99 Average loss: 0.2551; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1075; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3295; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1237; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3083; ACC 0.8999999761581421\n====> Epoch: 199 Average loss: 0.2050; ACC 0.9200000166893005\n\n\n====> Epoch: 99 Average loss: 0.3340; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1389; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.4030; ACC 0.8199999928474426\n====> Epoch: 199 Average loss: 0.2721; ACC 0.8500000238418579\n\n\n====> Epoch: 99 Average loss: 0.3750; ACC 0.9100000262260437\n====> Epoch: 199 Average loss: 0.2137; ACC 0.9200000166893005\n\n\n====> Epoch: 99 Average loss: 0.3024; ACC 0.9599999785423279\n====> Epoch: 199 Average loss: 0.1374; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.3065; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.0974; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.4050; ACC 0.8799999952316284\n====> Epoch: 199 Average loss: 0.2486; ACC 0.8799999952316284\n\n\n====> Epoch: 99 Average loss: 0.2904; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1290; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.2712; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1021; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.4164; ACC 0.8700000047683716\n====> Epoch: 199 Average loss: 0.2578; ACC 0.8700000047683716\n\n\n====> Epoch: 99 Average loss: 0.3321; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1344; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3583; ACC 0.8600000143051147\n====> Epoch: 199 Average loss: 0.2156; ACC 0.8700000047683716\n\n\n====> Epoch: 99 Average loss: 0.3969; ACC 0.8799999952316284\n====> Epoch: 199 Average loss: 0.2336; ACC 0.8799999952316284\n\n\n====> Epoch: 99 Average loss: 0.3194; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1324; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3168; ACC 0.949999988079071\n====> Epoch: 199 Average loss: 0.1341; ACC 0.949999988079071\n\n\n====> Epoch: 99 Average loss: 0.2811; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.0911; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.3310; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1226; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.2863; ACC 0.9300000071525574\n====> Epoch: 199 Average loss: 0.1682; ACC 0.9399999976158142\n\n\n====> Epoch: 99 Average loss: 0.2563; ACC 1.0\n====> Epoch: 199 Average loss: 0.0703; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.2977; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.0885; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.2795; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1181; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3495; ACC 0.8700000047683716\n====> Epoch: 199 Average loss: 0.2071; ACC 0.8999999761581421\n\n\n====> Epoch: 99 Average loss: 0.4125; ACC 0.8999999761581421\n====> Epoch: 199 Average loss: 0.2424; ACC 0.8999999761581421\n\n\n====> Epoch: 99 Average loss: 0.3028; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1164; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3339; ACC 0.949999988079071\n====> Epoch: 199 Average loss: 0.1651; ACC 0.949999988079071\n\n\n====> Epoch: 99 Average loss: 0.3126; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1097; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3164; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1105; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.3509; ACC 0.9399999976158142\n====> Epoch: 199 Average loss: 0.1772; ACC 0.9399999976158142\n\n\n====> Epoch: 99 Average loss: 0.3352; ACC 0.8799999952316284\n====> Epoch: 199 Average loss: 0.2330; ACC 0.8899999856948853\n\n\n====> Epoch: 99 Average loss: 0.2433; ACC 1.0\n====> Epoch: 199 Average loss: 0.0642; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.3142; ACC 0.9599999785423279\n====> Epoch: 199 Average loss: 0.1353; ACC 0.9599999785423279\n\n\n====> Epoch: 99 Average loss: 0.4012; ACC 0.8399999737739563\n====> Epoch: 199 Average loss: 0.2791; ACC 0.8399999737739563\n\n\n====> Epoch: 99 Average loss: 0.3056; ACC 1.0\n====> Epoch: 199 Average loss: 0.0995; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.3440; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1273; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3277; ACC 0.949999988079071\n====> Epoch: 199 Average loss: 0.1490; ACC 0.949999988079071\n\n\n====> Epoch: 99 Average loss: 0.3381; ACC 0.9399999976158142\n====> Epoch: 199 Average loss: 0.1631; ACC 0.9399999976158142\n\n\n====> Epoch: 99 Average loss: 0.2754; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1014; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.2619; ACC 1.0\n====> Epoch: 199 Average loss: 0.0761; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.2891; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.0986; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3392; ACC 0.949999988079071\n====> Epoch: 199 Average loss: 0.1598; ACC 0.949999988079071\n\n\n====> Epoch: 99 Average loss: 0.2716; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.0692; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.2737; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1073; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3214; ACC 0.9599999785423279\n====> Epoch: 199 Average loss: 0.1499; ACC 0.9599999785423279\n\n\n====> Epoch: 99 Average loss: 0.3179; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1180; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3178; ACC 0.9399999976158142\n====> Epoch: 199 Average loss: 0.1460; ACC 0.949999988079071\n\n\n====> Epoch: 99 Average loss: 0.3695; ACC 0.8899999856948853\n====> Epoch: 199 Average loss: 0.2183; ACC 0.8999999761581421\n\n\n====> Epoch: 99 Average loss: 0.3450; ACC 0.9399999976158142\n====> Epoch: 199 Average loss: 0.1749; ACC 0.9399999976158142\n\n\n====> Epoch: 99 Average loss: 0.3020; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1008; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3292; ACC 0.949999988079071\n====> Epoch: 199 Average loss: 0.1468; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.3391; ACC 0.9399999976158142\n====> Epoch: 199 Average loss: 0.1658; ACC 0.949999988079071\n\n\n====> Epoch: 99 Average loss: 0.3754; ACC 0.9100000262260437\n====> Epoch: 199 Average loss: 0.2168; ACC 0.9100000262260437\n\n\n====> Epoch: 99 Average loss: 0.3195; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1423; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.3762; ACC 0.8700000047683716\n====> Epoch: 199 Average loss: 0.2510; ACC 0.8799999952316284\n\n\n====> Epoch: 99 Average loss: 0.4181; ACC 0.8199999928474426\n====> Epoch: 199 Average loss: 0.2855; ACC 0.8199999928474426\n\n\n====> Epoch: 99 Average loss: 0.3237; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1355; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3008; ACC 0.9300000071525574\n====> Epoch: 199 Average loss: 0.1258; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.3282; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1002; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.3036; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1264; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3826; ACC 0.9300000071525574\n====> Epoch: 199 Average loss: 0.1759; ACC 0.9599999785423279\n\n\n====> Epoch: 99 Average loss: 0.3006; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1023; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.3056; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.0993; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3279; ACC 1.0\n====> Epoch: 199 Average loss: 0.0828; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.3416; ACC 0.9399999976158142\n====> Epoch: 199 Average loss: 0.1593; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.3252; ACC 0.8899999856948853\n====> Epoch: 199 Average loss: 0.1809; ACC 0.8999999761581421\n\n\n====> Epoch: 99 Average loss: 0.2896; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.0847; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.4237; ACC 0.8799999952316284\n====> Epoch: 199 Average loss: 0.2637; ACC 0.8799999952316284\n\n\n====> Epoch: 99 Average loss: 0.3561; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1503; ACC 0.9700000286102295\n\n\n====> Epoch: 99 Average loss: 0.3174; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1317; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.3262; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.1172; ACC 0.9900000095367432\n\n\n====> Epoch: 99 Average loss: 0.2730; ACC 0.9900000095367432\n====> Epoch: 199 Average loss: 0.0872; ACC 1.0\n\n\n====> Epoch: 99 Average loss: 0.3394; ACC 0.9599999785423279\n====> Epoch: 199 Average loss: 0.1463; ACC 0.9599999785423279\n\n\n====> Epoch: 99 Average loss: 0.3115; ACC 0.9800000190734863\n====> Epoch: 199 Average loss: 0.1156; ACC 0.9800000190734863\n\n\n====> Epoch: 99 Average loss: 0.2839; ACC 0.9700000286102295\n====> Epoch: 199 Average loss: 0.1176; ACC 0.9700000286102295\n\n\nshape record  {0: (512, 9749), 1: (512,), 2: (256, 512), 3: (256,), 4: (1, 256), 5: (1,)}\ngen w done\nlearner  LR\nnot gen w\nlearner  LR\nnot gen w\ntorch.Size([100, 1, 1892]) torch.Size([100, 1, 5123585]) torch.Size([100, 100, 9749]) torch.Size([100, 100, 1])\ntorch.Size([200, 1, 1892]) torch.Size([200, 1, 1]) torch.Size([200, 100, 9749]) torch.Size([200, 100, 1])\ntorch.Size([379, 1, 1892]) torch.Size([379, 1, 1]) torch.Size([379, 100, 9749]) torch.Size([379, 100, 1])\nlearner  LR\nnot gen w\ntorch.Size([379, 1, 1892]) torch.Size([379, 1, 1]) torch.Size([379, 17632, 1]) (17632, 9749)\n"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_batch_size = 200\n",
    "task_id = list(compressed_dataset.keys())\n",
    "tot_len = len(task_id)\n",
    "train_size = int(0.8*tot_len)\n",
    "test_size = tot_len  - train_size\n",
    "print('train_size ', train_size, 'test_size ', test_size)\n",
    "\n",
    "support_size = 100\n",
    "\n",
    "train_indx_full = list(np.random.choice(task_id, size=train_size, replace=False))\n",
    "print(len(train_indx_full))\n",
    "test_indx = [x for x in task_id if x not in train_indx_full]\n",
    "print(len(test_indx))\n",
    "\n",
    "train_data = Dataset_hetrec([(d, compressed_dataset[d]) for d in train_indx_full], detailed_dataset, train_task=False, learner='NN')\n",
    "preset_x =  train_data.x\n",
    "print('preset x ',preset_x.shape)\n",
    "\n",
    "support_indx = list(np.random.choice(train_indx_full, size=support_size, replace=False))\n",
    "train_indx = [x for x in train_indx_full if x not in support_indx]\n",
    "print(len(support_indx), len(train_indx))\n",
    "\n",
    "support_data = Dataset_hetrec([(d, compressed_dataset[d]) for d in support_indx], detailed_dataset, preset_x =preset_x, train_task=True, learner='NN')\n",
    "\n",
    "train_data = Dataset_hetrec([(d, compressed_dataset[d]) for d in train_indx], detailed_dataset,preset_x =preset_x, train_task=False)\n",
    "test_data = Dataset_hetrec([(d, compressed_dataset[d]) for d in test_indx], detailed_dataset, preset_x = preset_x, train_task=False)\n",
    "\n",
    "support_loader = DataLoader(support_data, batch_size=support_size, shuffle=False)\n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=test_size, shuffle=True)\n",
    "\n",
    "support_a, support_w, support_x, support_y = next(iter(support_loader))\n",
    "print(support_a.shape, support_w.shape, support_x.shape, support_y.shape)\n",
    "\n",
    "train_a, train_w, train_x, train_y = next(iter(train_loader))\n",
    "train_a, train_w, train_x, train_y = train_a.float(), train_w.float(), train_x.float(), train_y.float()\n",
    "print(train_a.shape, train_w.shape, train_x.shape, train_y.shape)\n",
    "#print(train_a)\n",
    "\n",
    "test_a, test_w, test_x, test_y = next(iter(test_loader))\n",
    "test_a, test_w, test_x, test_y = test_a.float(), test_w.float(), test_x.float(), test_y.float()\n",
    "print(test_a.shape, test_w.shape, test_x.shape, test_y.shape)\n",
    "#a = ppp\n",
    "\n",
    "\n",
    "test_data_full = Dataset_hetrec([(d, compressed_dataset[d]) for d in test_indx], detailed_dataset, preset_x =preset_x, phase='mAP', train_task=False)\n",
    "test_data_full_loader = DataLoader(test_data_full, batch_size=test_size, shuffle=True)\n",
    "\n",
    "test_batch = next(iter(test_data_full_loader))\n",
    "test_a, test_w, test_y = test_batch[0].float(), test_batch[1].float(), \\\n",
    "    test_batch[2].float()\n",
    "\n",
    "test_x = preset_x\n",
    "print(test_a.shape, test_w.shape, test_y.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_size  1513 test_size  379\ngen x\nx shape  (17632, 9750)\ngen w, wait \ngen w done\ntorch.Size([150, 1, 1892]) torch.Size([150, 1, 9750]) torch.Size([150, 100, 9750]) torch.Size([150, 100, 1])\ngen w, wait \ngen w done\ntorch.Size([379, 1, 1892]) torch.Size([379, 1, 9750]) torch.Size([379, 100, 9750]) torch.Size([379, 100, 1])\ngen w, wait \ngen w done\ntorch.Size([379, 1, 1892]) torch.Size([379, 1, 9750]) torch.Size([379, 17632, 1]) torch.Size([17632, 9750])\n"
    }
   ],
   "source": [
    "#for comparison, only train and test\n",
    "\n",
    "train_batch_size = 150\n",
    "task_id = list(compressed_dataset.keys())\n",
    "tot_len = len(task_id)\n",
    "train_size = int(0.8*tot_len)\n",
    "test_size = tot_len  - train_size\n",
    "print('train_size ', train_size, 'test_size ', test_size)\n",
    "\n",
    "train_indx = list(np.random.choice(task_id, size=train_size, replace=False))\n",
    "print(len(train_indx))\n",
    "test_indx = [x for x in task_id if x not in train_indx]\n",
    "print(len(test_indx))\n",
    "\n",
    "train_data = Dataset_hetrec([(d, compressed_dataset[d]) for d in train_indx], detailed_dataset)\n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "train_a, train_w, train_x, train_y = next(iter(train_loader))\n",
    "train_a, train_w, train_x, train_y = train_a.float(), train_w.float(), train_x.float(), train_y.float()\n",
    "print(train_a.shape, train_w.shape, train_x.shape, train_y.shape)\n",
    "\n",
    "preset_x = utils.toTensor(train_data.x)\n",
    "\n",
    "test_data = Dataset_hetrec([(d, compressed_dataset[d]) for d in test_indx], detailed_dataset, preset_x = preset_x)\n",
    "test_loader = DataLoader(test_data, batch_size=test_size, shuffle=True)\n",
    "#print(train_a)\n",
    "\n",
    "test_a, test_w, test_x, test_y = next(iter(test_loader))\n",
    "test_a, test_w, test_x, test_y = test_a.float(), test_w.float(), test_x.float(), test_y.float()\n",
    "print(test_a.shape, test_w.shape, test_x.shape, test_y.shape)\n",
    "\n",
    "#a = ppp\n",
    "\n",
    "\n",
    "\n",
    "test_data_full = Dataset_hetrec([(d, compressed_dataset[d]) for d in test_indx], detailed_dataset, preset_x =preset_x, phase='mAP')\n",
    "test_data_full_loader = DataLoader(test_data_full, batch_size=test_size, shuffle=True)\n",
    "\n",
    "test_batch = next(iter(test_data_full_loader))\n",
    "test_a, test_w, test_y = test_batch[0].float(), test_batch[1].float(), \\\n",
    "    test_batch[2].float()\n",
    "\n",
    "test_x = preset_x\n",
    "print(test_a.shape, test_w.shape, test_y.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def genSplits_hectrec(compressed_data, detailed_data, train_size, test_size, support_size, train_batch_size=100):\n",
    "\n",
    "    task_id = list(compressed_data.keys())\n",
    "    tot_len = len(task_id)\n",
    "\n",
    "    support_indx = list(np.random.choice(task_id, size=support_size, replace=False))\n",
    "    print(len(support_indx))\n",
    "    temp = [x for x in task_id if x not in support_indx]\n",
    "    train_indx = list(np.random.choice(temp, size=train_size, replace=False))\n",
    "    temp = [x for x in temp if x not in train_indx]\n",
    "    print(len(train_indx))\n",
    "    test_indx = temp\n",
    "    print(len(test_indx))\n",
    "\n",
    "    support_data = utils.Dataset_hetrec([compressed_data[d] for d in support_indx], detailed_data)\n",
    "    train_data = utils.Dataset_hetrec([compressed_data[d] for d in train_indx], detailed_data)\n",
    "    test_data = utils.Dataset_hetrec([compressed_data[d] for d in test_indx], detailed_data)\n",
    "\n",
    "    support_loader = DataLoader(support_data, batch_size=support_size, shuffle=False)\n",
    "    train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=test_size, shuffle=True)\n",
    "\n",
    "    return support_loader, train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "path_data = '../ZSTL_Data/hetrec2011-lastfm-2k/extracted_feature/'\n",
    "compressed_data = 'sampled_task.pickle'\n",
    "detailed_data = 'detailed_data.pickle'\n",
    "\n",
    "with open(path_data+compressed_data, 'rb') as f:\n",
    "    compressed_dataset = pickle.load(f)\n",
    "\n",
    "with open(path_data+detailed_data, 'rb') as f:\n",
    "    detailed_dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tot_len  1892 (0, array([[-1.72784869,  0.50704983,  0.30139832, ...,  0.        ,\n         0.        ,  0.        ]]), array([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n           9,    10,    11,    12,    13,    14,    15,    16,    17,\n          18,    19,    20,    21,    22,    23,    24,    25,    26,\n          27,    28,    29,    30,    31,    32,    33,    34,    35,\n          36,    37,    38,    39,    40,    41,    42,    43,    44,\n          45,    46,    47,    48,    49, 10556, 14767,  1654,  9267,\n        7242,  3435,  5862,  8099,  3307,  3973,  9003,  7807, 17251,\n       16162, 14519,  1073,  4214,  1393, 12544,  9680, 10355,  3158,\n        2921, 12854,  3927,  6533, 13634, 10059,  3989,  7949, 14261,\n        4394,  6692, 15157,  2093,  2457,  3482, 16200, 15700,  8771,\n       15223, 17363,  7165,  7816,   926, 16911,   574,   409,  5483,\n        7275]))\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'genSplits' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-94657687895a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_len\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msupport_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msupport_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenSplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressed_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'genSplits' is not defined"
     ]
    }
   ],
   "source": [
    "total_len = len(compressed_dataset)\n",
    "print('tot_len ', total_len, compressed_dataset[2])\n",
    "support_size = 150\n",
    "test_size = int(total_len*0.2)\n",
    "train_size = int(total_len - support_size - test_size)\n",
    "support_loader, train_loader, test_loader = genSplits(compressed_dataset, detailed_dataset, train_size, test_size, support_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(support_loader))\n",
    "support_a, support_w, support_x, support_y = next(iter(support_loader))\n",
    "support_a, support_w, support_x, support_y = support_a.float(), support_w.float(), support_x.float(), support_y.float()\n",
    "print(support_a.shape, support_w.shape, support_x.shape, support_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-de4472034240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}