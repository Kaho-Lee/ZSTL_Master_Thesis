{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599220579833",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import timeit\n",
    "from mlmodel import *\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_feature = '../ZSTL_Data/Animals_with_Attributes2/Features/ResNet101/AwA2-features.txt'\n",
    "path_labels = '../ZSTL_Data/Animals_with_Attributes2/Features/ResNet101/AwA2-labels.txt'\n",
    "path_attributes = '../ZSTL_Data/Animals_with_Attributes2/predicate-matrix-binary.txt'\n",
    "path_destination = '../ZSTL_Data/Animals_with_Attributes2/splitedTask/'\n",
    "data_feature = pd.read_csv(path_feature, sep=\" \", header=None)\n",
    "data_labels = pd.read_csv(path_labels, sep=\" \", header=None)\n",
    "data_labels.columns = [\"label\"]\n",
    "\n",
    "num_task = 50\n",
    "num_data = 100\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "label         0         1         2         3         4         5  \\\n0          1  0.127028  3.236108  0.935148  0.144205  1.114897  1.502288   \n1          1  0.000000  2.466911  0.026454  0.075211  1.159094  3.066645   \n2          1  0.383341  1.011904  0.000000  0.054472  0.343532  0.917366   \n3          1  0.117190  1.225786  0.001932  0.000000  3.135732  0.061605   \n4          1  0.274902  0.337544  0.084937  0.000000  1.788061  0.143165   \n...      ...       ...       ...       ...       ...       ...       ...   \n37317     38  0.498370  1.883775  0.000000  0.212900  0.195262  0.201184   \n37318     38  0.043884  0.309244  0.012275  0.173839  0.893198  0.183430   \n37319     38  0.016755  1.105690  0.103399  0.384196  0.469869  0.512281   \n37320     38  0.121401  1.050093  0.006921  0.545237  0.884461  0.738782   \n37321     38  0.243461  1.255479  0.048471  0.660457  0.147349  1.431781   \n\n              6         7         8  ...      2038      2039      2040  \\\n0      0.410044  0.053410  0.000000  ...  0.000000  0.008841  0.059838   \n1      0.191157  0.049365  0.017394  ...  0.000000  0.312142  0.447039   \n2      0.044860  0.100728  0.266673  ...  0.000000  0.112120  0.000000   \n3      0.974178  0.463193  0.057579  ...  0.000000  0.168017  0.460633   \n4      0.250313  0.000000  0.134671  ...  0.000000  0.106673  0.040709   \n...         ...       ...       ...  ...       ...       ...       ...   \n37317  1.139161  0.072897  0.040727  ...  0.000000  0.000000  1.554183   \n37318  0.092317  0.000000  0.589606  ...  0.111526  0.000000  0.989355   \n37319  0.173142  0.197311  0.164240  ...  0.000000  0.024520  0.987232   \n37320  0.308057  0.086900  0.256168  ...  0.013474  0.001264  0.677624   \n37321  0.139688  0.174798  0.250953  ...  0.034443  0.319134  2.102535   \n\n           2041      2042      2043      2044      2045      2046      2047  \n0      0.018889  0.024724  0.256931  0.071584  0.672237  0.032265  0.407616  \n1      0.212022  0.000000  0.226731  0.278318  1.125202  0.006843  0.256230  \n2      0.000000  1.184899  0.000000  0.000000  2.252654  0.029523  0.033138  \n3      0.000000  0.008595  0.001324  0.000000  0.725105  0.101483  0.000000  \n4      0.000000  0.000000  0.010016  0.010241  0.755545  0.000000  0.060499  \n...         ...       ...       ...       ...       ...       ...       ...  \n37317  0.946372  0.146409  0.412591  0.250193  0.013291  0.018395  0.040261  \n37318  0.306995  0.000000  0.728876  0.330187  0.400795  0.032162  0.272096  \n37319  0.133453  0.096120  0.299018  0.135867  0.188487  0.000819  0.353541  \n37320  0.313628  0.086169  0.792456  0.576316  0.050573  0.000000  0.022577  \n37321  0.472797  0.061420  1.461895  0.498844  0.123365  0.015723  0.062191  \n\n[37322 rows x 2049 columns]\n"
    }
   ],
   "source": [
    "data = pd.concat([data_labels, data_feature], axis=1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(37322, 2048)\ndata_x_compress  (37322, 2048) [[1.27028410e-01 3.23610830e+00 9.35147520e-01 ... 6.72237160e-01\n  3.22649500e-02 4.07616200e-01]\n [0.00000000e+00 2.46691060e+00 2.64539600e-02 ... 1.12520206e+00\n  6.84306000e-03 2.56229610e-01]\n [3.83341340e-01 1.01190424e+00 0.00000000e+00 ... 2.25265384e+00\n  2.95226900e-02 3.31381500e-02]\n ...\n [1.67553000e-02 1.10568988e+00 1.03398520e-01 ... 1.88486860e-01\n  8.19240000e-04 3.53541340e-01]\n [1.21401340e-01 1.05009258e+00 6.92100000e-03 ... 5.05729500e-02\n  0.00000000e+00 2.25771200e-02]\n [2.43460600e-01 1.25547945e+00 4.84714400e-02 ... 1.23365400e-01\n  1.57233700e-02 6.21905400e-02]]\nTime: 0.003762139999992087\n       label         0         1         2         3         4         5  \\\n0        1.0  0.127028  3.236108  0.935148  0.144205  1.114897  1.502288   \n1        1.0  0.000000  2.466911  0.026454  0.075211  1.159094  3.066645   \n2        1.0  0.383341  1.011904  0.000000  0.054472  0.343532  0.917366   \n3        1.0  0.117190  1.225786  0.001932  0.000000  3.135732  0.061605   \n4        1.0  0.274902  0.337544  0.084937  0.000000  1.788061  0.143165   \n...      ...       ...       ...       ...       ...       ...       ...   \n37317   38.0  0.498370  1.883775  0.000000  0.212900  0.195262  0.201184   \n37318   38.0  0.043884  0.309244  0.012275  0.173839  0.893198  0.183430   \n37319   38.0  0.016755  1.105690  0.103399  0.384196  0.469869  0.512281   \n37320   38.0  0.121401  1.050093  0.006921  0.545237  0.884461  0.738782   \n37321   38.0  0.243461  1.255479  0.048471  0.660457  0.147349  1.431781   \n\n              6         7         8  ...      2038      2039      2040  \\\n0      0.410044  0.053410  0.000000  ...  0.000000  0.008841  0.059838   \n1      0.191157  0.049365  0.017394  ...  0.000000  0.312142  0.447039   \n2      0.044860  0.100728  0.266673  ...  0.000000  0.112120  0.000000   \n3      0.974178  0.463193  0.057579  ...  0.000000  0.168017  0.460633   \n4      0.250313  0.000000  0.134671  ...  0.000000  0.106673  0.040709   \n...         ...       ...       ...  ...       ...       ...       ...   \n37317  1.139161  0.072897  0.040727  ...  0.000000  0.000000  1.554183   \n37318  0.092317  0.000000  0.589606  ...  0.111526  0.000000  0.989355   \n37319  0.173142  0.197311  0.164240  ...  0.000000  0.024520  0.987232   \n37320  0.308057  0.086900  0.256168  ...  0.013474  0.001264  0.677624   \n37321  0.139688  0.174798  0.250953  ...  0.034443  0.319134  2.102535   \n\n           2041      2042      2043      2044      2045      2046      2047  \n0      0.018889  0.024724  0.256931  0.071584  0.672237  0.032265  0.407616  \n1      0.212022  0.000000  0.226731  0.278318  1.125202  0.006843  0.256230  \n2      0.000000  1.184899  0.000000  0.000000  2.252654  0.029523  0.033138  \n3      0.000000  0.008595  0.001324  0.000000  0.725105  0.101483  0.000000  \n4      0.000000  0.000000  0.010016  0.010241  0.755545  0.000000  0.060499  \n...         ...       ...       ...       ...       ...       ...       ...  \n37317  0.946372  0.146409  0.412591  0.250193  0.013291  0.018395  0.040261  \n37318  0.306995  0.000000  0.728876  0.330187  0.400795  0.032162  0.272096  \n37319  0.133453  0.096120  0.299018  0.135867  0.188487  0.000819  0.353541  \n37320  0.313628  0.086169  0.792456  0.576316  0.050573  0.000000  0.022577  \n37321  0.472797  0.061420  1.461895  0.498844  0.123365  0.015723  0.062191  \n\n[37322 rows x 2049 columns]\n"
    }
   ],
   "source": [
    "def generate_compressed_data(data, num_task, num_data):\n",
    "    '''\n",
    "    dataframe with labels (col 0) and feature (col 1:)\n",
    "    '''\n",
    "    # print('row')\n",
    "    data_numpy = data.to_numpy()\n",
    "    data_x = data_numpy[:,1:]\n",
    "    data_y = np.atleast_2d(data_numpy[:,0])\n",
    "    print(data_x.shape)\n",
    "    start = timeit.default_timer()\n",
    "    # pca = PCA(n_components=1024)\n",
    "    # data_x_compress = pca.fit_transform(data_x)\n",
    "    data_x_compress = data_x\n",
    "    print('data_x_compress ',data_x_compress.shape, data_x_compress)\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time:', stop - start)\n",
    "\n",
    "    data_compress_np = np.hstack((data_y.T, data_x_compress))\n",
    "    data_compressed =  pd.DataFrame(data_compress_np,\n",
    "                   columns=['label'] +[i for i in range(data_x_compress.shape[1])])\n",
    "    print(data_compressed)\n",
    "    return data_compressed\n",
    "\n",
    "data_compressed = generate_compressed_data(data, num_task, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 107.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 103.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\n(200, 2050) 102.0\n(0, 2050) 0.0\n(200, 2050) 100.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 104.0\n(0, 2050) 0.0\n(200, 2050) 101.0\n(0, 2050) 0.0\ntot task  50\n"
    }
   ],
   "source": [
    "def generate_split(splits, num_data):\n",
    "    indx = [ x for x in range(num_data)]\n",
    "    #print(temp)\n",
    "    train_indx = list(np.random.choice(indx, size=splits['train'], replace=False))\n",
    "    temp = [x for x in indx if x not in train_indx]\n",
    "    #print(len(train_indx))\n",
    "    # val_indx = list(np.random.choice(temp, size=55, replace=False))\n",
    "    test_indx = temp\n",
    "    #print(len(test_indx))\n",
    "    return train_indx, test_indx\n",
    "\n",
    "def task_data_split(data, num_task, num_data, splits, random_state=1):\n",
    "    task_train_byID = {}\n",
    "    task_test_byID = {}\n",
    "    task_val_byID = {}\n",
    "  \n",
    "    for i in range(num_task):   \n",
    "        task_data = data.loc[data['label'].eq(i+1)]\n",
    "        \n",
    "        sampled_task_data = task_data.sample(n=num_data, random_state=i)\n",
    "        #print(sampled_task_data.shape, sampled_task_data)\n",
    "        sampled_data = data.sample(n=num_data, random_state=i)\n",
    "\n",
    "        train_indx, test_indx = generate_split(splits, num_data)\n",
    "\n",
    "        task_train_byID[i+1] = np.vstack((sampled_task_data.iloc[train_indx].to_numpy(), \\\n",
    "            sampled_data.iloc[train_indx].to_numpy()))\n",
    "        task_train_byID[i+1] = np.hstack( (task_train_byID[i+1], np.ones((len(train_indx)*2, 1))) )\n",
    "        \n",
    "        temp = task_train_byID[i+1][:, 0]==(i+1)\n",
    "        task_train_byID[i+1][:, 0][temp==True] = 1.\n",
    "        task_train_byID[i+1][:, 0][temp==False] = 0.\n",
    "        print(task_train_byID[i+1].shape, np.sum(task_train_byID[i+1][:,0]))\n",
    "\n",
    "\n",
    "        task_test_byID[i+1] = np.vstack((sampled_task_data.iloc[test_indx].to_numpy(), \\\n",
    "            sampled_data.iloc[test_indx].to_numpy()))\n",
    "        task_test_byID[i+1] = np.hstack( (task_test_byID[i+1], np.ones((len(test_indx)*2, 1))) )\n",
    "\n",
    "        temp = task_test_byID[i+1][:, 0]==(i+1)\n",
    "        task_test_byID[i+1][:, 0][temp==True] = 1.\n",
    "        task_test_byID[i+1][:, 0][temp==False] = 0.\n",
    "        print(task_test_byID[i+1].shape, np.sum(task_test_byID[i+1][:,0]))\n",
    "\n",
    "    print('tot task ', len(list(task_train_byID.keys())))\n",
    "    return task_train_byID, task_test_byID\n",
    "\n",
    "\n",
    "splits = {}\n",
    "splits['train'] = 100\n",
    "splits['test'] = 0\n",
    "task_train_byID, task_test_byID = task_data_split(data_compressed, num_task, num_data, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = FuncRecursiveNet([\n",
    "    FLinearLayer(1, False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "pred_y_test  1.0 (1, 2049)\n"
    }
   ],
   "source": [
    "#test the coef in logistic regression\n",
    "\n",
    "def singleTaskTrain(task_train, task_test):\n",
    "    X = task_train[:, 1:]\n",
    "    y = task_train[:, 0]\n",
    "\n",
    "    clf = LogisticRegression(fit_intercept = False, max_iter=100, C=1.0, random_state=0).fit(X, y)\n",
    "    pred_y = clf.predict(X)\n",
    "\n",
    "    #X_test = task_test[:, 1:]\n",
    "    # y_test = task_test[:, 0]\n",
    "    pred_y_test = clf.predict(X)\n",
    "\n",
    "    param = clf.coef_\n",
    "    print('pred_y_test ', np.sum(pred_y_test == y)/y.shape[0], param.shape)\n",
    "\n",
    "    #bias = clf.intercept_\n",
    "    #return np.hstack((param, np.atleast_2d(bias)))\n",
    "    return param, 1\n",
    "\n",
    "weight = singleTaskTrain(task_train_byID[1], task_test_byID[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[torch.Size([1, 2049])]\n{0: [(1, 2049)]}\n"
    }
   ],
   "source": [
    "init_param = net.initialize_weights(utils.toTensor(task_train_byID[1][:, 1:]))\n",
    "p_lst = [p.size() for p in init_param]\n",
    "print(p_lst)\n",
    "shape_record = {}\n",
    "for i , p in enumerate(p_lst):\n",
    "    shape_record[i] = [tuple(p)]\n",
    "print(shape_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# net_pred_y_test = net(reshape_w, xtest)\n",
    "# print('net_pred_y_test ', net_pred_y_test.shape)\n",
    "# net_pred_y_test = torch.sigmoid(net_pred_y_test )\n",
    "# net_pred_y_test[net_pred_y_test >= 0.5] = 1.\n",
    "# net_pred_y_test[net_pred_y_test < 0.5] = 0.\n",
    "# print((utils.toNumpy(net_pred_y_test) == np.atleast_2d(pred_y_test).T).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n0   0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 ...\n1   1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 ...\n2   1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 ...\n3   0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 ...\n4   1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 ...\n5   0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 ...\n6   1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 ...\n7   1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 ...\n8   0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 ...\n9   1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 1 ...\n10  1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 1 1 ...\n11  1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 ...\n12  1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 ...\n13  0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 ...\n14  1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 ...\n15  0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 ...\n16  1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 ...\n17  1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 ...\n18  0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 ...\n19  1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 ...\n20  1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 ...\n21  0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 ...\n22  1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 ...\n23  1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 ...\n24  1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 ...\n25  1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 ...\n26  0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 ...\n27  0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 ...\n28  1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 ...\n29  1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 ...\n30  0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 ...\n31  1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 ...\n32  1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 ...\n33  1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 ...\n34  1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 ...\n35  1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 1 ...\n36  1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 ...\n37  1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 ...\n38  1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 ...\n39  0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 ...\n40  0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 ...\n41  1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 ...\n42  0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 ...\n43  0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 ...\n44  0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 ...\n45  0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 ...\n46  0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 ...\n47  1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 ...\n48  1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 ...\n49  0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 ...\nattr_mat  (50, 85) [[0. 0. 0. ... 0. 0. 0.]\n [1. 0. 0. ... 1. 0. 0.]\n [1. 1. 0. ... 0. 0. 0.]\n ...\n [1. 1. 0. ... 1. 1. 0.]\n [1. 1. 0. ... 0. 0. 1.]\n [0. 1. 1. ... 0. 0. 1.]]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n"
    }
   ],
   "source": [
    "def gen_attr(path_attributes):\n",
    "    data_attributes = pd.read_csv(path_attributes, sep=\"\\n\", header=None)\n",
    "    print(data_attributes)\n",
    "    lst = []\n",
    "    task_attr_byID = {}\n",
    "    i = 1\n",
    "    for r in data_attributes.iterrows():\n",
    "        # print(len(r), )\n",
    "        s = r[1].to_numpy()\n",
    "        # print(len(s[0].split()), s[0].split())\n",
    "        print(len(s[0].split()))\n",
    "        task_attr_byID[i] = np.array([float(a) for a in s[0].split()])\n",
    "        print(task_attr_byID[i].shape)\n",
    "        i += 1\n",
    "    return task_attr_byID\n",
    "\n",
    "def gen_attr_zScore(path_attributes):\n",
    "    data_attributes = pd.read_csv(path_attributes, sep=\"\\n\", header=None)\n",
    "    print(data_attributes)\n",
    "    lst = []\n",
    "    task_attr_byID = {}\n",
    "    i = 1\n",
    "    for r in data_attributes.iterrows():\n",
    "        #print(len(r), )\n",
    "        s = r[1].to_numpy()\n",
    "        # print(len(s[0].split()), s[0].split())\n",
    "        #print(r[0], len(s[0].split()))\n",
    "        lst.append(np.atleast_2d(np.array([float(a) for a in s[0].split()])))\n",
    "        # print(task_attr_byID[i].shape)\n",
    "        # i += 1\n",
    "    attr_mat = np.concatenate(lst, axis=0)\n",
    "    print('attr_mat ', attr_mat.shape, attr_mat)\n",
    "    # attr_mean = np.mean(attr_mat, axis=0)\n",
    "    # print('attr_mean ', attr_mean.shape, attr_mean)\n",
    "\n",
    "\n",
    "    # attr_sub_mean = attr_mat - attr_mean\n",
    "    # print('sub ', attr_sub_mean.shape, attr_mat - attr_sub_mean)\n",
    "\n",
    "    # attr_std = np.std(attr_mat, axis=0)\n",
    "    # print('attr_std ', attr_std.shape, attr_std)\n",
    "\n",
    "    # attr_z = attr_sub_mean/attr_std\n",
    "    # print('z score', attr_z.shape, attr_sub_mean/attr_z)\n",
    "\n",
    "    # print('ffff ',attr_z)\n",
    "\n",
    "    for t in range(attr_mat.shape[0]):\n",
    "        task_attr_byID[t+1] = attr_mat[t,:]\n",
    "    \n",
    "    print(list(task_attr_byID.keys()))\n",
    "    return task_attr_byID\n",
    "\n",
    "task_attr_byID = gen_attr_zScore(path_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "pred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\npred_y_test  1.0 (1, 2049)\n50\n50\nmean acc  1.0\n"
    }
   ],
   "source": [
    "def ZSTL_train_test_val(num_task, task_train_byID, task_test_byID, task_attr_byID, destination):\n",
    "    task_train_data = {}\n",
    "    task_test_data = {}\n",
    "    task_val_data = {}\n",
    "    acc = 0\n",
    "    for t in range(num_task):\n",
    "        weight, a = singleTaskTrain(task_train_byID[t+1], task_test_byID[t+1])\n",
    "        acc += a\n",
    "        cur_task_train = (task_attr_byID[t+1], weight, task_train_byID[t+1][:,1:], np.atleast_2d(task_train_byID[t+1][:,0]).T)\n",
    "        cur_task_test = (task_attr_byID[t+1], weight, task_test_byID[t+1][:,1:], np.atleast_2d(task_test_byID[t+1][:,0]).T)\n",
    "\n",
    "        task_train_data[t+1] = cur_task_train\n",
    "        task_test_data[t+1] = cur_task_test\n",
    "\n",
    "    print(len(task_train_data))\n",
    "    print(len(task_test_data))\n",
    "    \n",
    "    with open(destination+'task_train_data_binary.pickle', 'wb') as handle:\n",
    "        pickle.dump(task_train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(destination+'task_test_data_binary.pickle', 'wb') as handle:\n",
    "        pickle.dump(task_test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('mean acc ', acc/num_task)\n",
    "\n",
    "\n",
    "\n",
    "ZSTL_train_test_val(num_task, task_train_byID, task_test_byID, task_attr_byID, path_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}